<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="209">Il metodo proposto ha un guadagno del 29% rispetto al metodo di riferimento.</sample>
    <sample id="210">Shuheng Liu è il nome della relatrice.</sample>
    <sample id="211">Sì, i risultati e i set di dati nell'articolo possono essere utilizzati come parametri di riferimento per la semplificazione automatica del testo in futuro.</sample>
    <sample id="212">Due modelli più piccoli:
- T5 (11B)
- Flan-T5 (11B)</sample>
    <sample id="213">OFA (One For All) è il modello utilizzato come base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="215">This talk discusses the dependency structure of coordination in English. Different linguistic theories and corpora adopt various dependency structures for coordination, which can be asymmetric (e.g., Bouquet/Stanford and Chain/Moscow approaches, where one conjunct heads the entire coordinate structure) or symmetric (e.g., Conjunction-headed/Prague and Multi-headed/London approaches, where the conjunction or all conjuncts act as heads).

The speaker presents a novel argument in favor of symmetric coordination structures based on the principle of Dependency Length Minimization (DLM). DLM suggests that word order tends to minimize dependency lengths. In English, direct objects are typically placed close to their verbs, while adjuncts can be further away. However, if a direct object is long, it can be moved after an adjunct to shorten its overall dependency length, making the sentence more natural.

The researchers analyzed coordination statistics from an enhanced version of the Penn Treebank. They confirmed previous observations that left conjuncts tend to be shorter, and this tendency increases with the length difference between conjuncts. Crucially, they found that this preference for shorter left conjuncts *only* occurs when the governor (the word that the coordinate structure depends on) is on the left or absent (e.g., "I saw Bart and Lisa"). When the governor is on the right (e.g., "Ted and Ned laughed"), this length-based ordering effect disappears.

This finding supports symmetric dependency structures for coordination. Asymmetric structures, which posit one conjunct as the head, create different dependency lengths for the conjuncts. However, symmetric structures, where all conjuncts (or the conjunction itself) are treated equally, are more compatible with the observed lack of a length-based preference when the governor is on the right. The paper provides a full argument and invites further discussion at the poster session.</sample>
    <sample id="217">In questo lavoro, abbiamo esplorato la generalizzazione composizionale per la generazione di dialoghi controllabili multi-attributo e abbiamo scoperto che i modelli esistenti mancano di capacità di generalizzazione. Abbiamo proposto il DCG, un modello di generazione di dialoghi controllabili districati che apprende concetti di attributi da valori visti e utilizza una perdita di districamento per districare diverse combinazioni di attributi. Il modello DCG si basa sul framework DialoGPT con un modulo prompt composizionale e impiega due tipi di prompt: orientati agli attributi e orientati al compito. I prompt orientati agli attributi utilizzano combinazioni di valori di attributi controllabili per guidare il modello a concentrarsi su informazioni specifiche nel dialogo, mentre i prompt orientati al compito utilizzano feature globali indipendenti dall'istanza per migliorare la qualità del compito di generazione delle risposte del dialogo. Abbiamo anche introdotto un framework di valutazione unificato senza riferimento, MAE, per diverse granularità di attributi. Attraverso esperimenti su due benchmark, abbiamo dimostrato l'efficacia del nostro metodo e della nostra metrica di valutazione. I risultati hanno mostrato che il DCG supera tutte le altre baseline sia in termini di controllabilità dell'attributo che di qualità del testo, dimostrando la capacità del nostro metodo di generalizzare da attributi visti a combinazioni non viste. Il nostro MAE proposto mostra una correlazione più elevata con i giudizi umani per la valutazione su CDG, confermando ulteriormente la sua efficacia.</sample>
    <sample id="218">Gli autori dell'articolo sono del team di Google Translate.</sample>
    <sample id="219">La presentazione illustra una pipeline multistadio per l'identificazione di segnali finanziari nei rapporti finanziari annuali (Form 10-K). I ricercatori hanno osservato due caratteristiche principali di questi documenti: l'80% dei token sono uguali tra i rapporti di una stessa azienda, e il contenuto è dipendente dall'anno, con i rapporti di anni adiacenti più simili tra loro.

Basandosi su queste osservazioni, è stato introdotto un "task di evidenziazione" (highlighting task) per prevedere parole importanti nei rapporti finanziari, considerati segnali finanziari. Il task consiste nel confrontare e contrastare i contesti di una data coppia di frasi, una dal rapporto corrente (target) e una dal rapporto dell'anno precedente (reference). L'obiettivo è identificare la "ragione fondamentale" (rationale) delle relazioni tra queste coppie.

La pipeline proposta si articola in tre fasi principali: segmentazione del documento, riconoscimento delle relazioni e fasi di evidenziazione. La fase di riconoscimento delle relazioni classifica le coppie di segmenti in tre tipi: "insignificanti" (con alta prossimità sintattica e semantica), "riviste" (simili sintatticamente ma con significati diversi, come un aumento vs. una diminuzione) e "disparate" (con significato esclusivo, come nuove politiche).

Per la fase di evidenziazione, viene utilizzato un approccio di "fine-tuning a due stadi": un "fine-tuning out-of-domain" su un dataset di inferenza linguistica naturale (e-SNLIc) e un "fine-tuning in-domain" sulle coppie riviste con pseudo-etichette. Quest'ultimo stadio usa una combinazione di cross-entropy loss e Kullback-Leibler divergence per mitigare i problemi di qualità delle pseudo-etichette.

I risultati mostrano che il modello "domain-adaptive highlighting" proposto ha superato tutti gli altri modelli sul dataset FIN(ancial)AL, mantenendo al contempo la capacità di generalizzazione su e-SNLIc. È stato inoltre osservato che il metodo può beneficiare di relazioni non viste, come le coppie disparate, che non sono state utilizzate durante l'addestramento.

I futuri lavori includono lo sviluppo di modelli linguistici finanziari più efficaci, l'integrazione di funzionalità di razionalizzazione bidirezionale, l'esplorazione di approcci end-to-end e l'analisi di dati multimodali come grafici e tabelle.</sample>
    <sample id="220">Stony Brook University e Human Language Analysis Beings.</sample>
    <sample id="221">Nell'articolo sono state analizzate le coppie linguistiche tedesco-inglese.</sample>
    <sample id="222">Il documento riguarda l'adattamento e l'annotazione nei sistemi di question answering a dominio aperto (QA), con particolare attenzione alle sfide poste dalla generalizzazione fuori dal dominio. I sistemi QA a dominio aperto tipicamente recuperano passaggi rilevanti da un corpus di documenti (es. Wikipedia) e poi utilizzano un modello lettore per generare una risposta. Tuttavia, l'applicazione di modelli addestrati su dati generici a nuovi domini, come quello biomedico, presenta delle sfide. Quando un corpus è espanso per includere documenti da un nuovo dominio, il modello può confondersi e fornire risposte errate, non riuscendo a discriminare tra i nuovi passaggi.

Lo studio propone tre contributi principali:
1. Indaga diversi interventi sui dati per abilitare la generalizzazione fuori dal dominio, esplorando metodi few-shot e zero-shot.
2. Analizza la compatibilità del modello sorgente con il dominio target, classificando i dataset target in base a tipi di "spostamento" (shift) concettuale, covariato, completo o assente.
3. Determina la relazione tra il tipo di spostamento del dataset e l'efficacia dei vari interventi sui dati.

Per gli interventi few-shot, vengono utilizzati pochi esempi dal dominio target per istruire modelli linguistici di grandi dimensioni (LLM) a generare ulteriori dati. Questo approccio migliora le prestazioni del recuperatore e del lettore. Per gli interventi zero-shot, vengono controllate le interazioni tra domanda, risposta e contesto. Si osserva che il formato delle domande non influisce significativamente sulle prestazioni, mentre una distribuzione uniforme dei tipi di risposta funziona meglio. Inoltre, i metodi di recupero tradizionali come BM25 si dimostrano più robusti ai cambiamenti di distribuzione del contesto rispetto ai recuperatori appresi.

I risultati mostrano che l'efficacia degli interventi sui dati dipende dal tipo di spostamento del dataset. Tutti i dataset target beneficiano degli adattamenti few-shot. I dataset con spostamento concettuale e covariato rispondono bene agli adattamenti zero-shot. In assenza di spostamento, le prestazioni non cambiano significativamente, indicando che il modello sorgente è già compatibile con il dominio target.</sample>
    <sample id="223">Il relatore è Shangbin Feng.</sample>
    <sample id="224">Sono stati studiati i modelli long-mBART e mBART.</sample>
    <sample id="225">53 attività per l'addestramento e 9 attività per il test.</sample>
    <sample id="226">Tre autori sono coinvolti nell'articolo.</sample>
    <sample id="227">Questo video presenta il Pangu Framework, una soluzione unificata per la comprensione del linguaggio radicato. Questo quadro affronta le sfide legate alla mancanza di radicamento durante il pre-addestramento nei modelli linguistici, che spesso porta a piani o programmi non grammaticali o non validi. Il Pangu Framework consente ai modelli linguistici di concentrarsi sulla discriminazione anziché sulla generazione. In questo modo, i modelli linguistici non hanno bisogno di gestire la validità e la grammatica del piano target da soli. Il framework è stato istanziato sulla risposta alle domande basate sulla conoscenza, uno scenario tipico con un ambiente eterogeneo per la comprensione del linguaggio radicato. Gli esperimenti sono stati condotti con modelli linguistici di diverse nature, tra cui BERT, T5 e Codex, utilizzando sia il fine-tuning che l'apprendimento in-context. I risultati hanno dimostrato una performance eccezionale su tutte le impostazioni, inclusa una robusta efficienza campionaria. In particolare, il Pangu Framework ha mostrato forti capacità di generalizzazione sotto impostazioni non-IID (indipendenti e distribuite identicamente), superando i modelli autoregressivi che tendono a sovradattarsi alle strutture viste durante l'addestramento. La conclusione principale del lavoro è che la generazione diretta di piani o programmi potrebbe non essere il modo ottimale per utilizzare i modelli linguistici per la comprensione del linguaggio radicato; invece, la discriminazione potrebbe essere una strategia molto migliore.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui set di dati AG News, MIND, SST2 ed Enron Spam.</sample>
    <sample id="229">Il documento presenta il problema della rilevazione di affermazioni non dimostrabili e del suggerimento di miglioramenti per la scrittura argomentativa. La revisione del testo è un aspetto cruciale della scrittura argomentativa, influenzando la persuasione. Per affrontarlo, il documento introduce due nuovi compiti: il rilevamento delle affermazioni subottimali e il suggerimento di miglioramenti delle affermazioni. Il primo compito si concentra sulla determinazione se un'affermazione necessiti di revisione o se sia formulata in modo ottimale. Il secondo compito mira a suggerire tipi di problemi di qualità che dovrebbero essere migliorati quando si rivede l'affermazione. Il documento esplora come la qualità del testo argomentativo possa essere modellata utilizzando schemi di revisione impliciti da piattaforme di dibattito online, come Kialo. Questa metodologia si basa sull'apprendimento dai comportamenti di revisione umana anziché sulla definizione esplicita di affermazioni "buone" o "cattive". L'autrice mostra come le storie di revisione con versioni finali considerate "ottimali" (in verde) e le precedenti versioni "subottimali" (in rosso) possano essere utilizzate per questo scopo. Il documento evidenzia anche le sfide nel lavorare con dati basati sulla revisione, tra cui la rappresentatività e l'affidabilità, la complessità del modello e l'architettura, la contestualità (esperienza nell'argomento, conoscenza del dominio, affermazioni genitrici e altro contesto) e i pregiudizi tematici e degli utenti. Per affrontare queste sfide, il documento presenta un'analisi dettagliata dei punti di forza e di debolezza di varie strategie e un confronto sistematico degli approcci per i compiti introdotti. In sintesi, il documento sottolinea che i dati basati sulla revisione possono essere utilizzati efficacemente, la modellazione della distanza tra le versioni delle affermazioni è utile per il rilevamento delle affermazioni subottimali e l'impatto delle informazioni contestuali dipende dal compito e dal problema di qualità.</sample>
    <sample id="231">NACHOS è un set di dati open-source da 1,1 miliardi di parole di dati eterogenei acquisiti da diversi domini, nature e stili medici.</sample>
    <sample id="232">Il relatore si chiama David Vilar.</sample>
    <sample id="233">Un relatore presenta un lavoro congiunto con altri due colleghi sul tema "Attention as a Guide for Simultaneous Speech Translation". Inizia definendo la Simultaneous Speech Translation (SimulST) come il processo di traduzione del linguaggio parlato in tempo reale, facilitando la comunicazione interlinguistica.

Il relatore evidenzia i problemi dei modelli SimulST attuali, che includono: architetture specifiche con moduli aggiuntivi da ottimizzare, procedure di training lunghe e complesse, e la necessità di addestrare e mantenere diversi modelli per diversi regimi di latenza.

La soluzione proposta è "EDAtt" (Encoder-Decoder Attention), che si basa sull'uso di modelli ST offline esistenti, l'impiego di un unico modello per ogni regime di latenza e lo sfruttamento del meccanismo di attenzione tra input audio e output testuale. Viene spiegato il funzionamento di EDAtt, che decide se emettere o meno una traduzione parziale basandosi sulla concentrazione dell'attenzione. Se l'attenzione non è concentrata sugli ultimi 'lambda' frame del parlato (ovvero, la sua somma è inferiore a una soglia 'alpha'), il modello attende ulteriori frame. Altrimenti, la traduzione viene emessa.

I risultati mostrano che EDAtt supera le strategie popolari applicate ai modelli offline (wait-k, LA, CAAT) in termini di prestazioni di traduzione e latenza. Inoltre, EDAtt si rivela la strategia più veloce se si considera il tempo effettivo trascorso, o il tempo di calcolo consapevole.

Infine, il relatore invita a leggere il loro paper per scoprire più risultati e menziona che il codice e i modelli sono stati rilasciati open source per facilitare la riproducibilità.</sample>
    <sample id="234">La strategia del prompting ha un impatto notevole sulla qualità della traduzione. La maggior parte delle frasi mostra una differenza di oltre 1 punto BLEURT, e in casi estremi la differenza può arrivare fino a 40 punti BLEURT.</sample>
    <sample id="235">Gli autori sono affiliati alla Carnegie Mellon University, a Técnico Lisboa, a BAIR e a Unbabel.</sample>
    <sample id="236">Le istruzioni scritte da esperti non sono fornite nel contenuto.</sample>
    <sample id="237">Per testare i modelli sull'utilizzo di informazioni provenienti da più fonti, gli autori propongono un dataset diagnostico e un compito di risoluzione del coreference.</sample>
    <sample id="238">Ciao, mi chiamo Yebowen Hu della University of Central Florida. In questo video, presenterò un nuovo set di dati di benchmark per la summarizzazione delle riunioni.

Vi è mai capitato di trovarvi in una riunione cercando disperatamente di annotare tutti i punti chiave? Nel nostro mondo frenetico, le riunioni si tengono ogni giorno per diversi scopi, il che ha creato un'esigenza emergente di diversi set di dati per sviluppare tecnologie di summarizzazione per diversi domini delle riunioni. Per creare questo set di dati, abbiamo affrontato due sfide principali. In primo luogo, la scarsità di riassunti di alta qualità delle riunioni. In secondo luogo, la difficoltà nell'identificare fonti affidabili per le riunioni pubbliche. Alla fine, siamo riusciti a creare un repository di riunioni del consiglio comunale: MeetingBank. Tutti i dati che abbiamo raccolto saranno rilasciati in questo set di dati, inclusi i trascritti delle riunioni, il riassunto di riferimento e tutti gli URL che contengono una varietà di risorse utili.

Per prima cosa, vi guiderò attraverso il processo di raccolta dati. Come punto di partenza, abbiamo utilizzato l'API Speechmatics per convertire i dati audio in trascritti. Poi abbiamo aperto il sito web della riunione. Questo esempio proviene dal consiglio comunale di Boston. Possiamo prima identificare il tipo e la data della riunione dalle informazioni fornite sotto il video. Queste informazioni verranno convertite in un ID riunione che rappresenta in modo univoco quella particolare riunione. Abbiamo utilizzato questo ID articolo selezionato per individuare i riassunti di riferimento corrispondenti dai verbali della riunione. Poi abbiamo utilizzato lo stesso ID articolo per ottenere i segmenti di riunione corrispondenti, inclusi l'ora di inizio e di fine. Come passo finale, abbiamo allineato i timestamp per ottenere il trascritto del segmento e lo abbiamo abbinato al riassunto estratto in precedenza.

Il nostro set di dati include un numero totale di 1366 riunioni del consiglio comunale e quasi 7000 istanze. Nelle nostre statistiche sui dati, da sinistra a destra, abbiamo contato il numero di riunioni, la durata della riunione, il numero di token per riunione, il numero di relatori per riunione per ogni città e il periodo annuale delle riunioni raccolte nel nostro set di dati. Forniamo anche il numero di istanze di summarizzazione raccolte per ogni città, così come il numero medio di frasi e token sia nel testo sorgente che nel testo del riassunto. Sono riportate le statistiche medie sia a livello di riunione che a livello di segmento per tutte le città.

Per l'analisi dei dati, abbiamo misurato il livello di estrazione nei riassunti delle riunioni con due misure comuni: copertura e densità. Il punteggio di copertura misura la percentuale di parole del riassunto che appaiono nel documento sorgente. Un punteggio di densità valuta quanto un riassunto può essere caratterizzato come un insieme di frammenti estrattivi. Come mostrato in questa figura, il punteggio di copertura della maggior parte dei riassunti delle riunioni del consiglio comunale è compreso tra 0,7 e 0,9, il che indica che i riassunti includono spesso punti salienti estrattivi rispetto all'astrazione. L'analisi del punteggio di densità rivela che Seattle e Boston hanno i punteggi più alti, mentre Denver ha il punteggio più basso. Questo può indicare un alto livello di editing effettuato sui verbali delle riunioni.

Per la valutazione del modello, abbiamo valutato sistemi di summarizzazione di alto livello sul set di test di MeetingBank. I nostri summarizer estrattivi includevano Oracle, Lead, LexRank e TexRank. Abbiamo utilizzato cinque dei summarizer astrattivi neurali più performanti per il nostro test. Questi includono Bart-Large, Pegasus, Longformer, DialogLM e HMNet. Abbiamo valutato Bart-Large con e senza fine-tuning sul nostro set di dati e abbiamo confrontato i risultati con altri modelli. Abbiamo sfruttato le capacità avanzate del modello linguistico di grandi dimensioni GPT-3. In particolare, abbiamo condotto una summarizzazione a zero-shot con Davinci-003 sul nostro set di test. Confrontando questi 10 diversi sistemi, abbiamo diverse osservazioni interessanti. In primo luogo, per il sistema estrattivo, Extr-Oracle ha prodotto un punteggio ROUGE-2 elevato del 46,6%, il che indica che il contenuto dei riassunti di riferimento proviene principalmente dai trascritti sorgente e che un sistema di summarizzazione estrattiva potrebbe essere promettente. Sorprendentemente, il DialogLM, progettato per riassunti di dialoghi lunghi, ha prodotto il punteggio ROUGE-2 più alto tra i modelli astrattivi. Infine, GPT-3 non ha funzionato bene secondo le metriche automatiche. Ma abbiamo un'interessante scoperta nella valutazione umana successiva. Per ottenere una valutazione complessiva, abbiamo condotto esperimenti non solo con le metriche tradizionali, ma abbiamo anche utilizzato nuove metriche come BertScore e MoverScore. Inoltre, abbiamo anche utilizzato metriche basate sulle risposte alle domande.

Alla fine, abbiamo effettuato una valutazione umana, in cui abbiamo chiesto a tre annotatori umani di valutare la qualità dei riassunti generati da ciascun sistema in base a cinque criteri: informatività, fattualità, fluidità, coerenza e ridondanza. Abbiamo selezionato casualmente 200 istanze e ciascuna include trascritti di segmenti sorgente e riassunti generati da sette modelli. Abbiamo reclutato tre valutatori esperti negli Stati Uniti per eseguire una scala Likert a 5 punti. A causa dei vincoli di tempo, voglio presentare una scoperta interessante: GPT-3 ha ottenuto il punteggio complessivo più alto. Ha mostrato prestazioni eccezionali in termini di fluidità e coerenza. Tuttavia, i suoi risultati sono meno impressionanti in termini di informatività e fattualità. Questa scoperta suggerisce che le soluzioni di summarizzazione delle riunioni dovrebbero continuare a concentrarsi sulla cattura dei punti di discussione principali e che dovrebbe essere sviluppato un nuovo metodo di metriche di valutazione automatica per allinearsi meglio alle preferenze umane.

In conclusione, il nostro principale contributo è la creazione di MeetingBank. Questo è un set di dati di benchmark costruito segmentando le riunioni del consiglio comunale e abbinando questi segmenti a riassunti scritti da esperti. Questo set di dati potrebbe essere un prezioso banco di prova per i ricercatori che progettano summarizer di riunioni avanzati. Fornisce inoltre spunti interessanti nel processo decisionale dei consigli comunali. Alla fine di questo video, incoraggio tutti voi a utilizzare questa risorsa. Sentitevi liberi di scaricarla e giocarci. Non vedo l'ora di ulteriori discussioni con voi online. Grazie.</sample>
    <sample id="239">00:00
Ciao a tutti, il mio nome è David Villar e vi presenterò una breve panoramica del documento "Promoting PaLM for Translation: Assessing Strategies and Performance".
00:07
Questo è un lavoro congiunto con i miei colleghi di Google Translate.
00:12
PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato lo scorso anno, nel 2022.
00:19
È stato addestrato su una vasta collezione di testi, comprendente 780 miliardi di token.
00:26
Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di attività di elaborazione del linguaggio naturale.
00:30
In questo lavoro, presentiamo il primo studio sistematico del prompting per i modelli linguistici di grandi dimensioni per la traduzione automatica.
00:38
Valutiamo la capacità di traduzione di tali modelli, utilizzando le migliori pratiche della comunità di traduzione automatica.
00:46
Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico.
00:54
E confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni nella valutazione WMT.
01:00
Abbiamo utilizzato metriche di traduzione automatica neurali all'avanguardia e, inoltre, abbiamo mostrato i risultati di valutazione umana basata su esperti.
01:08
Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.
01:13
Il prompting ha una grande influenza sulle prestazioni dei modelli linguistici di grandi dimensioni per la traduzione, come possiamo vedere in un semplice esperimento in cui abbiamo utilizzato un prompting a singolo esempio e fornito due prompt diversi per ogni frase.
01:31
La maggior parte delle frasi (516 su 1000) mostra una differenza di oltre un punto BLEURT.
01:38
E questa differenza può arrivare, in casi estremi, fino a 40 punti BLEURT.
01:43
Quindi è importante selezionare una buona strategia di prompting.
01:48
Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque esempi, in cui ci limitiamo a contrassegnare ogni frase che forniamo al sistema con la lingua in cui si trova.
02:02
Quindi, in questo esempio qui, in cui eseguiamo la traduzione dal tedesco all'inglese, le frasi sorgente sono contrassegnate con "German:" e le traduzioni in inglese con "English:".
02:18
Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di prompting a più esempi.
02:26
È cruciale per il prompting a zero e un esempio, ma quando arriviamo, come nel nostro caso, al prompting a cinque esempi, non c'è quasi nessuna differenza nella forma effettiva del prompting.
02:37
Sono gli esempi che hanno il peso maggiore.
02:40
Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase sorgente.
02:50
Quindi, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo.
03:03
I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, e i risultati mostrano prestazioni migliori quando si utilizzano i dati di sviluppo.
03:14
Tuttavia, i sistemi all'avanguardia specializzati hanno un vantaggio sostanziale rispetto alle traduzioni PaLM.
03:22
Ma PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate.
03:32
Le intuizioni che abbiamo ricavato dalla valutazione umana, che abbiamo eseguito utilizzando il framework MQM, sono che la fluidità di PaLM è paragonabile a quella dei sistemi all'avanguardia.
03:43
Ma la differenza principale deriva dall'accuratezza. Quindi, in particolare, l'errore più comune sono gli errori di omissione.
03:52
Quindi, sembra che PaLM scelga di produrre una traduzione più fluida, a volte eliminando parti della frase sorgente che vengono omesse nella traduzione.
04:09
Tuttavia, la categoria "Style/Awkward" per PaLM è inferiore rispetto ai sistemi all'avanguardia, il che è un segnale aggiuntivo che PaLM produce un output davvero fluido, ma con ancora alcuni problemi di accuratezza.
04:26
Questo è tutto per questa breve panoramica.
04:31
Per maggiori dettagli, vi invito a consultare la presentazione completa del documento.
04:36
Grazie mille.</sample>
    <sample id="240">00:00
Ciao, sono Dawei, uno studente di dottorato alla Saarland University in Germania. In questo video, vorrei presentare il nostro recente lavoro: "Più debole di quanto pensi: uno sguardo critico all'apprendimento supervisionato debole". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.
00:21
Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento supervisionato debole. Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati usando sorgenti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono imprecise. Se addestriamo direttamente le reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento supervisionato debole, gli algoritmi di addestramento sono proposti per addestrare robustamente le reti neurali sotto tale rumore dell'etichetta, in modo che i modelli addestrati generalizzino comunque bene.
01:14
In lavori recenti in WSL (che sta per apprendimento supervisionato debole), un'affermazione comune è che le persone dicono di addestrare i modelli solo su dati supervisionati debolmente e di raggiungere alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un trucco, cioè che le persone presumono che ci sia un set di validazione pulito aggiuntivo e disponibile per la selezione del modello. Abbiamo esaminato questo problema, poiché ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato debole, ma, come un elefante nella stanza, questa necessità viene spesso trascurata.
01:58
I suddetti dubbi ci hanno portato a porre tre domande di ricerca. Primo, sono necessari dati di validazione puliti per il WSL? O possiamo forse usare un set di validazione rumoroso? Secondo, se i dati puliti sono necessari, o se i dati puliti sono obbligatori affinché il WSL funzioni, quanti campioni puliti ci servono? Infine, dovremmo usare i campioni puliti solo per la validazione o ci sono modi migliori per utilizzarli?
02:30
Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti. Primo, abbiamo scoperto che, sorprendentemente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo di prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.
03:14
La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per raggiungere alte prestazioni. Ma non è la fine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi raggiungerà prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che sono direttamente applicati sui dati puliti, e gli approcci WSL, che usano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL.
04:07
Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo il fine-tuning continuo sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello di base, denominato FTW, inizialmente ha prestazioni inferiori rispetto a metodi WSL più complicati come COSine. Tuttavia, se consentiamo il fine-tuning continuo sui campioni puliti, allora FTW ha prestazioni altrettanto buone come altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco.
04:51
Per riassumere, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro guadagno di prestazioni e la praticità sono fortemente sovrastimati. Le nostre raccomandazioni concrete per lavori futuri sono le seguenti: Primo, riportare i criteri di selezione del modello, ad esempio, riportare se la selezione del modello è stata eseguita su campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con baseline di apprendimento con pochi colpi per entrambi i lavori sui campioni puliti. Terzo, il fine-tuning continuo è una baseline semplice ma robusta che dovrebbe essere considerata nei lavori futuri in WSL. Infine, abbiamo open-sourced il nostro codice. Potete trovarlo tramite il codice QR su questa slide. Non esitate a controllarlo. Grazie e buon congresso.</sample>
    <sample id="241">La maggior parte degli approcci attuali per il rilevamento automatico della disinformazione sono valutati in modo irrealistico, utilizzando set di dati di benchmark retrospettivi che non sono rappresentativi del mondo reale e possono contenere evidenze "leaked". Inoltre, non sono incentrati sull'uomo, poiché non riescono a considerare il ruolo dei moderatori di contenuti umani o dei verificatori di fatti.

Questo documento propone una metodologia di valutazione per i sistemi di rilevamento della disinformazione "human-in-the-loop" (HiTL), che tiene conto di queste carenze. I sistemi HiTL sono end-to-end, vanno dai tweet grezzi agli output azionabili, e integrano il feedback umano in varie fasi del processo.

Viene presentata un'implementazione concreta del framework HiTL per il rilevamento della disinformazione sui trattamenti COVID-19 su Twitter. Il sistema è costituito da due componenti principali: la rilevazione precoce di affermazioni fuorvianti e la verifica delle violazioni delle politiche. La rilevazione precoce di affermazioni fuorvianti identifica le affermazioni che propongono trattamenti COVID-19 non approvati prima che siano smentite nei notiziari, utilizzando il filtro delle parole chiave e un modello T5 per l'estrazione delle affermazioni. La verifica delle violazioni delle politiche determina la posizione di un tweet rispetto a un'affermazione disinformata e lo segnala per la revisione umana se supporta il trattamento non approvato.

La valutazione del sistema mostra che può rilevare in modo efficace le affermazioni fuorvianti precocemente. Il sistema ha una precisione del 65% nel rilevamento delle violazioni delle politiche e può rilevare 124,2 violazioni delle politiche per ora umana di lavoro.

Questo framework propone un approccio più realistico per valutare i sistemi di rilevamento della disinformazione, considerando il ruolo degli esseri umani e il contesto del mondo reale. Si spera che possa motivare lo sviluppo di sistemi HiTL più utili e fornire uno standard concreto per la valutazione futura.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono la valutazione comparativa umana e la valutazione tramite scala Likert.</sample>
    <sample id="243">5 autori sono coinvolti.</sample>
    <sample id="244">La conoscenza di base necessaria è che "i giudici decidono i casi nei tribunali".</sample>
    <sample id="245">La presentazione introduce un metodo in due fasi per la selezione di lavoratori Amazon Mechanical Turk (MTurk) affidabili per i task di valutazione. Questo è motivato dalla scarsa comprensione delle migliori pratiche di reclutamento su MTurk e dai potenziali problemi con le metriche automatiche. Il processo prevede un task di qualificazione iniziale, che valuta la capacità dei partecipanti di valutare correttamente più dimensioni di testo. I lavoratori vengono categorizzati in Oro, Argento, Bronzo o Blocco, con solo le categorie Oro e Argento che passano alla fase successiva. Dopo il task di qualificazione, viene assegnato un task di resistenza per valutare la capacità dei lavoratori di gestire un carico di lavoro elevato. Questo task prevede 10 task di intelligenza umana (HIT), ognuno con un documento sorgente e quattro riassunti candidati da valutare per la salienza. I lavoratori che superano questa fase dimostrano un elevato accordo inter-annotatore (IAA) rispetto agli esperti. I risultati indicano che il 13% dei 200 partecipanti iniziali si è qualificato nel primo task e il 6% ha superato il task di resistenza, mostrando un IAA superiore a quello degli esperti. La fase finale, un task basato su riferimenti, testa le prestazioni generali su un task di annotazione reale utilizzando 30 HIT. I lavoratori del pipeline che hanno completato gli HIT hanno raggiunto un punteggio Cohen's Kappa di 0.68. Rispetto ai lavoratori MTurk di base (filtrati con MACE) e ai lavoratori CloudResearch, il metodo del pipeline ha mostrato un'efficienza simile o migliore in termini di qualità delle annotazioni a un costo inferiore, evitando gli sprechi di risorse sulle annotazioni scartate. Sebbene il pipeline non garantisca la correttezza, i modelli GPT si sono correlati bene con i giudizi degli esperti. Le limitazioni includono il focus sulla riassunzione in inglese e il fatto che le domande progettate non sono "soluzioni panacea".</sample>
    <sample id="246">Sì. È disponibile su GitHub all'indirizzo mpoemsl/kitmus.</sample>
    <sample id="247">In questo video, Jiho Kim del KAIST AI presenta "FactKG: Fact Verification via Reasoning on Knowledge Graphs". Il relatore inizia evidenziando la mancanza di dataset di verifica dei fatti basati su grafi di conoscenza (KG) che utilizzano affermazioni in linguaggio naturale, nonostante i KG siano fonti di conoscenza affidabili e pratiche. Per affrontare questo problema, il relatore introduce FactKG, un nuovo dataset progettato per la verifica dei fatti basata su KG.

FactKG utilizza il KG DBpedia e include affermazioni sia in stile scritto che colloquiale per una maggiore applicabilità pratica. I tipi di ragionamento coperti includono one-hop, congiunzione, esistenza, multi-hop e negazione. Per creare le affermazioni, sono stati utilizzati metodi di parafrasi, come il modello di trasferimento di stile colloquiale di Kim et al. (2021) e modelli di presupposizione.

Il dataset è composto da 108.674 affermazioni, con una distribuzione di 34.462 affermazioni scritte e 74.212 affermazioni colloquiali (5000 da un modello di parafrasi e 9424 da modelli di presupposizione).

Le esperimenti di base sono stati condotti utilizzando modelli "Claim Only" (BERT, BlueBERT, Flan-T5) che non utilizzano prove da grafi, e un modello "With Evidence" (GEAR) che incorpora prove da grafi. I risultati mostrano che il modello GEAR, che utilizza prove da grafi, supera tutti gli altri modelli di base, indicando il valore dell'evidenza grafica nella verifica dei fatti basata su KG.

Il relatore conclude riassumendo i punti chiave e incoraggiando la comunità a utilizzare FactKG per una migliore comprensione e applicazione dei KG. Il dataset è disponibile su GitHub.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati tra i gruppi demografici. Invece, lo studio ha scelto di annotare nuovamente i dati per ottenere annotazioni per ogni istanza da diverse persone con diverse demografie.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo prefissi/suffissi avverbiali, avverbi prefissi lunghi, clausole aggiunte e citazioni.</sample>
    <sample id="250">Una valutazione dimensionale significa valutare la qualità di un dialogo su **più aspetti specifici** (dimensioni) piuttosto che con un'unica valutazione complessiva. Questo permette di capire meglio i punti di forza e debolezza di un modello.</sample>
    <sample id="251">Gli autori dell'articolo sono affiliati con l'Università di Scienza e Tecnologia della Cina, Microsoft Research Asia, Beijing Jiaotong University, Sony AI e Microsoft STC Asia.</sample>
    <sample id="252">Questo documento presenta U-CREAT (Unsupervised Case Retrieval using Events extrAcTion), un metodo per il recupero di casi precedenti nel dominio legale. Tradizionalmente, avvocati e giudici si affidano alla loro esperienza per citare precedenti rilevanti, ma l'aumento dei casi rende difficile citare i precedenti più datati. Il recupero di casi precedenti (Prior Case Retrieval - PCR) risolve questo problema recuperando automaticamente i documenti legali precedenti che dovrebbero essere citati in un documento legale corrente, basandosi sulla pertinenza fattuale e precedente.

Gli autori presentano due contributi principali: il dataset IL-PCR (Indian Legal - Prior Case Retrieval) e la pipeline U-CREAT. L'IL-PCR è un nuovo benchmark per il PCR nel sistema legale indiano, contenente 7070 casi legali con una media di 6,775 citazioni per documento di query. È più esteso dell'esistente dataset COLIEE'21 (Canadian Legal Documents).

La pipeline U-CREAT utilizza tecniche di apprendimento non supervisionato e introduce un approccio basato sugli eventi per il compito PCR. Questo approccio comporta l'estrazione di eventi dai documenti legali (query e candidati) attraverso la parsing delle dipendenze per identificare predicati (verbi) e i loro argomenti corrispondenti, formando triplette soggetto-verbo-oggetto. Queste triplette sono considerate eventi. Viene quindi calcolata una matrice di interazione tra gli eventi della query e quelli candidati per identificare gli eventi comuni.

Gli esperimenti hanno confrontato modelli basati sul conteggio (BM25), modelli basati su trasformatori (BERT, DistilBERT, Legal Transformer) e modelli basati sugli eventi. Sorprendentemente, i modelli basati su trasformatori, inclusi quelli specificamente addestrati su testi legali indiani, hanno mostrato prestazioni inferiori rispetto ai metodi baseline come BM25. I modelli basati sugli eventi, in particolare "Event Filtered Docs" (che considera solo le frasi con eventi corrispondenti), hanno superato significativamente i baseline e i trasformatori, raggiungendo un F1-score del 25,3% nell'IL-PCR e del 12,6% nel COLIEE'21. Questo suggerisce che gli eventi catturano meglio la pertinenza fattuale e precedente nel dominio legale. U-CREAT è non supervisionato, richiede un tempo di inferenza inferiore ed è generalizzabile tra diversi sistemi legali senza un'accordatura specifica del corpus.</sample>
    <sample id="253">In questo lavoro, abbiamo presentato DisorBERT, un modello di adattamento a doppio dominio per rilevare i segni di disturbi mentali nei social media. Abbiamo utilizzato Reddit e i dati sulla salute mentale per specializzare il modello pre-addestrato BERT. Un modello linguistico pre-addestrato è stato dapprima adattato per comprendere il linguaggio dei social media e successivamente, utilizzando il masking guidato, specializzato nel dominio dei disturbi mentali per focalizzare il modello su parole importanti durante il processo di addestramento. Abbiamo valutato DisorBERT utilizzando i set di dati eRisk, che contengono post di social media annotati che indicano disturbi mentali specifici come anoressia, depressione e autolesionismo. I risultati mostrano che DisorBERT ha raggiunto un buon equilibrio tra precisione e recall, con una migliore performance rispetto ai modelli di base come MentalBERT, che è stato addestrato con una maggiore quantità di dati e un maggiore consumo di risorse computazionali.

Per illustrare il comportamento di DisorBERT, abbiamo analizzato le parole predette dal modello utilizzando esempi dell'Inventario della Depressione di Beck (BDI). Le parole predette da DisorBERT mostrano un orientamento più negativo o psicologico, riflettendo la sua specializzazione nel dominio dei disturbi mentali. Inoltre, abbiamo visualizzato le sequenze più importanti del testo di un utente depresso, identificando parole rilevanti come "ansioso" e "medicazione", che sono fortemente associate alla depressione.

In futuro, intendiamo esplorare l'applicazione di diverse risorse lessicali più specializzate per i compiti target, nonché l'uso di dati clinici per addestrare modelli linguistici più specializzati, migliorando così la precisione e l'affidabilità del nostro sistema di rilevamento dei disturbi mentali.</sample>
    <sample id="254">In questa presentazione, Qi Sun dell'Università di Scienza e Tecnologia di Nanchino presenta la ricerca sulle relazioni distanti a livello di documento attraverso il de-noising delle etichette basato sull'incertezza. Il metodo DocRE (Document-level Relation Extraction) mira a estrarre le relazioni tra entità in un documento.

I metodi precedenti per il DocRE si basavano su corpora annotati da esseri umani, che sono dispendiosi in termini di tempo e risorse. Studi recenti hanno utilizzato dati con supervisione distante (DS), etichettati automaticamente da basi di conoscenza, per pre-addestrare i modelli DocRE e migliorare le prestazioni. Tuttavia, i dati DS contengono etichette rumorose, il che rappresenta una sfida.

Per affrontare questo problema, viene proposto un quadro che utilizza il de-noising delle etichette guidato dall'incertezza per migliorare la qualità delle etichette DS. Il quadro prevede l'addestramento di un modello DocRE pre-de-noising con dati DS e dati annotati da esseri umani per generare etichette pseudo. Data la presenza di etichette pseudo-false, viene introdotta la stima dell'incertezza (UE) a livello di istanza per determinare l'affidabilità delle previsioni del modello. Per le relazioni sovrapposte, i metodi UE precedenti non sono efficaci. Per questo, il processo di stima viene modificato per ottenere un punteggio di incertezza a livello di istanza per ciascuna etichetta pseudo positiva.

Il team ha osservato che le distribuzioni del punteggio di incertezza variano tra le diverse classi di relazione, con le classi frequenti che tendono ad avere un'incertezza media inferiore rispetto alle classi con coda lunga. Per filtrare le etichette pseudo con elevata incertezza, sono state proposte soglie di incertezza dinamiche basate sulla classe. Inoltre, è stata sviluppata una strategia iterativa di ri-etichettatura multi-fase per sfruttare appieno i dati DS e migliorare ulteriormente le prestazioni del modello DocRE.

Gli esperimenti sui dataset pubblici DocRED e Re-DocRED hanno dimostrato che il quadro proposto supera le baseline esistenti. Le ampie sperimentazioni hanno dimostrato un notevole miglioramento delle prestazioni dei modelli DocRE addestrati sui dati DS de-noising.</sample>
    <sample id="255">La forma del prompting è cruciale per il prompting a zero e a un colpo, ma non ha quasi nessuna influenza per il prompting a più colpi.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo open-domain.</sample>
    <sample id="258">Questo video presenta un nuovo lavoro di ricerca sulle Large Language Models (LLM) utilizzate per valutare la qualità dei testi in Natural Language Processing (NLP).  La ricerca si intitola "Can Large Language Models Be an Alternative to Human Evaluations?".  L'idea principale è istruire le LLM a valutare i campioni di testo, un processo chiamato "LLM evaluation". Questo metodo si propone di superare l'instabilità e la difficoltà di riproduzione delle valutazioni umane tradizionali. Sebbene l'idea di utilizzare le LLM per la valutazione possa sembrare naturale, questo studio è stato presentato all'ACL 2023 quando non c'erano ancora lavori precedenti che esplorassero l'idea della valutazione LLM in questo contesto specifico.

Per verificare l'utilità e la significatività della valutazione LLM, sono stati condotti degli esperimenti. Le LLM sono state utilizzate per valutare storie generate da GPT-2 e scritte da esseri umani, considerando quattro attributi: grammatica, coerenza, piacevolezza e pertinenza. I risultati delle LLM sono stati poi confrontati con le valutazioni di insegnanti di inglese, considerati esperti in questo tipo di compito. È emerso che gli esperti umani preferiscono chiaramente le storie scritte da esseri umani. Interessante notare che alcune LLM più piccole (T0 e curie) non hanno mostrato una preferenza significativa per le storie scritte da esseri umani. Tuttavia, le LLM più grandi (davinci e ChatGPT) hanno mostrato una chiara preferenza per i testi scritti da esseri umani, proprio come gli insegnanti di inglese. Ciò suggerisce che le LLM più grandi potrebbero essere un'alternativa valida alle valutazioni umane.

Il video accenna anche a ulteriori domande di ricerca, come l'accordo tra LLM e valutatori umani sulle storie individuali, l'effetto delle variazioni nelle istruzioni e nei metodi di campionamento delle risposte delle LLM, e i pro e i contro della valutazione LLM rispetto a quella umana.</sample>
    <sample id="259">Il relatore, Yusen Zhang della Penn State University, presenta il suo lavoro su XSemPLR: "Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Il parsing semantico è il compito di costruire rappresentazioni semantiche di query utente, come SQL e Lambda Calculus. Il parsing semantico cross-lingue (CLSP) è il compito di tradurre query in più linguaggi naturali in più rappresentazioni di significato.
I modelli CLSP esistenti sono proposti e valutati separatamente su set di dati di attività e applicazioni limitate. Per esempio, c'è una mancanza di copertura su determinati linguaggi naturali, come il cinese, e su determinate rappresentazioni di significato, come il Lambda Calculus. C'è anche una mancanza di copertura su determinati modelli neurali.
Per ovviare a ciò, il relatore propone XSemPLR, un set di dati unificato per il parsing semantico cross-lingue in più linguaggi naturali e rappresentazioni di significato. Contiene 9 set di dati in vari domini, 5 attività di parsing semantico, 8 rappresentazioni di significato e 22 linguaggi naturali in 15 famiglie linguistiche.
Per valutare il benchmark, considera sei impostazioni per l'addestramento e la valutazione:
1. Translate-Test: utilizza l'API di traduzione di Google per tradurre la sorgente nella lingua target, quindi utilizza un modello monolingue per addestrare e valutare.
2. Monolingual Model: la lingua sorgente è la stessa della lingua target, ad esempio, dal tedesco al tedesco.
3. Monolingual Few-shot: addestra modelli monolingue con solo il 10% dei dati di addestramento.
4. Multilingual Model: addestra un modello multilingue per tutti i linguaggi.
5. Cross-lingual Zero-shot: addestra su un linguaggio sorgente e trasferisce su un altro linguaggio.
6. Cross-lingual Few-shot transfer: addestra su un linguaggio sorgente e trasferisce su un altro linguaggio, utilizzando pochi esempi.

Tra le scoperte ci sono:
- i modelli Enc-Dec (mT5) superano i lavori precedenti o raggiungono risultati comparabili.
- il preaddestramento sul linguaggio naturale inglese può migliorare significativamente le prestazioni di pochi scatti sui linguaggi naturali target.
- i Large Language Model (LLMs) multilingue (Codex &amp; BLOOM) sono ancora inadeguati per i compiti di parsing semantico cross-lingue.
- il transfer learning cinese e l'addestramento monolingue inglese hanno il divario di prestazione più grande, mentre il tedesco di solito ha il più piccolo.
- FunQL supera le altre tre rappresentazioni di significato e SQL ottiene le prestazioni peggiori.

In conclusione, XSemPLR è un benchmark unificato per il parsing semantico cross-lingue con più linguaggi naturali e rappresentazioni di significato. I risultati mostrano che mT5 con addestramento monolingue offre le migliori prestazioni, mentre gli LLMs multilingue sono ancora inadeguati per il CLSP. Inoltre, il divario di prestazioni tra l'addestramento monolingue e il transfer learning cross-lingue è ancora significativo.</sample>
    <sample id="260">Undici autori sono coinvolti nell'articolo.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.</sample>
    <sample id="262">Ci sono 9 autori coinvolti nell'articolo.</sample>
    <sample id="263">Il relatore introduce un lavoro sulla mitigazione dei bias di etichetta nell'apprendimento in-context. Questo approccio è popolare per i Large Language Models (LLM), ma la sua stabilità è compromessa da scelte progettuali, come la selezione e l'ordine degli esempi in-context, che introducono bias. Lo studio attuale propone una discussione sistematica per categorizzare i bias esistenti, identificare nuovi tipi e mitigarne gli effetti, focalizzandosi sulla classificazione del testo.

Lo studio categorizza tre tipi di bias di etichetta: vanilla-label bias (preferenza del modello non contestuale per i nomi delle etichette), context-label bias (effetti dal contesto, come il bias di etichetta di maggioranza) e un nuovo tipo, il domain-label bias (effetti del corpus del task sulla predizione del modello). Quest'ultimo è stato evidenziato da esperimenti in cui la presentazione di parole casuali in-domain influenzava significativamente le predizioni del modello, a differenza delle parole inglesi casuali.

È stato scoperto che gli LLM mostrano comportamenti di apprendimento in-context diversi in base al livello di domain-label bias. Per i task con un basso bias, l'apprendimento in-context funziona bene e può essere migliorato ulteriormente con la calibrazione. Tuttavia, per i task con un alto bias, la performance è limitata al livello di possibilità casuale, anche con metodi di calibrazione preesistenti.

Per affrontare il domain-label bias, lo studio propone la calibrazione domain-context (DC). A differenza dei metodi precedenti che utilizzano un token predefinito e content-free, DC impiega parole casuali in-domain dal corpus del task per stimare il bias e calibrare le predizioni. Gli esperimenti hanno dimostrato che DC migliora l'apprendimento in-context, in particolare per i task con un alto domain-label bias. L'analisi ha rivelato che i token predefiniti possono essere essi stessi distorti e che l'uso di un solo token content-free è sub-ottimale. L'utilizzo di più parole casuali in-domain si è rivelato più efficace nel rimuovere il domain-label bias e migliorare i confini decisionali del modello.</sample>
    <sample id="264">This presentation introduces TAVT, a novel framework for Transferable Audio-Visual Text Generation, which aims to overcome limitations in existing uni-modal and multi-modal text generation tasks. Current methods for multi-modal text generation, such as audio-visual text generation, suffer from arduous and expensive data annotation, leading to severe degradation when applied to varying conditions across domains. The core challenge lies in multi-modal domain shifts.

The authors propose leveraging a unified auditory semantic space to align visual concepts across different domains. This approach is motivated by the observation that while audio and visual content are often correlated in natural events, they possess distinct characteristics; specifically, visual content shifts significantly with changes in image style or shooting angle, whereas changes in audio content (rhythm, energy) have minimal impact on event comprehension.

The TAVT framework consists of three key components:
1. **Audio-Visual Meta-Mapper Network (AVMM):** This component maps diverse visual concepts from various domains into a unified auditory semantic space, addressing semantic distribution shifts. It uses learnable visual prefix tokens for audio clusters, generated through K-means clustering of audio clips. The AVMM reconstructs audio using visual content as a query, optimizing for coherence and aligning visual content with the auditory space.
2. **Audio-Visual Encoder &amp; Language Model Generator (AVE &amp; LMG):** Built on a transformer-based architecture, this module uses an attention mechanism (alpha) to evaluate the contribution of each modality to word generation, balancing visual and auditory information.
3. **Counterfactual Contrastive Learning:** This component introduces distribution-based and dependency-based contrastive losses to provide fine-grained supervision for AVMM and directly optimize visual-text alignment without relying on the quality of random negative samples.

The training process involves a meta-learning approach, randomly selecting domains as support and query sets for meta-training and meta-testing. Experiments on cross-dataset (MSVD, MSR-VTT+) and cross-domain (Animation, Music, Animal, Kids, Beauty) benchmarks demonstrate that TAVT significantly outperforms existing state-of-the-art methods, especially in low-resource domains, highlighting its robustness and transferability. Ablation studies confirm the effectiveness of audio features and the proposed modules.</sample>
    <sample id="265">La relatrice si chiama Vasudha Varadarajan.</sample>
    <sample id="266">Gli autori sono affiliati all'Istituto di Informatica dell'Accademia Polacca delle Scienze e all'Università di Varsavia.</sample>
    <sample id="268">I più comuni errori di PaLM sono quelli di omissione.</sample>
    <sample id="269">Ciao, sono James Finch.
E io sono Sarah Finch.
E oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale.

Questo lavoro è stato svolto dal laboratorio NLP di Emory,
guidato dal Professor Jinho Choi all'Università di Emory,
in collaborazione con Amazon Alexa AI.

Quindi, supponiamo che abbiate appena sviluppato un modello di dialogo, e volete vedere quanto si comporta bene rispetto allo stato dell'arte.
La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore, o di valutare le conversazioni utilizzando una scala Likert.

Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti.
Perciò, potreste voler valutare più dimensioni della qualità della chat, per capire i punti di forza e di debolezza del modello a un livello più granulare.

Un approccio è semplicemente quello di chiedere a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del bot, utilizzando i metodi comparativi o a scala Likert esistenti.
Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo.

Il nostro approccio tenta di ridurre la soggettività della valutazione umana, annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddirsi.
Chiamiamo questo approccio "Annotating Behaviors in Chat", o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente.

ABC-Eval è in grado di misurare le frequenze con cui i modelli di chat commetteranno vari errori tematici. Per esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante.
Si contraddice o contraddice il suo partner.
Allucina fatti scorretti o viola il buon senso. E quando il modello riesce o fallisce nel mostrare empatia.

Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni tra umani e bot per ogni modello, utilizzando ABC-Eval.
Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: le valutazioni Likert a livello di turno, le valutazioni Likert a livello di dialogo e i confronti a coppie a livello di dialogo.

Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti del dialogo più comunemente misurati, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni.

Dalle nostre analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di ABC-Eval sono complessivamente più affidabili rispetto alle etichette raccolte con i metodi esistenti, misurate dall'accordo inter-annotatore su 100 conversazioni etichettate due volte.

Inoltre, le etichette di ABC-Eval sono più predittive della qualità complessiva della conversazione, rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare.
Ad esempio, potete vedere come la misurazione della proporzione di turni con auto-contraddizioni e contraddizioni del partner, spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi medi di coerenza Likert spiegano solo il 4% o meno.

Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare step-wise. Potete vedere come la combinazione di tutte le metriche di ABC-Eval spiega oltre il 25% della qualità della conversazione. E man mano che rimuovete le metriche una alla volta, la maggior parte di esse comporta la perdita di una discreta quantità di informazioni sulla qualità.
D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche contengono informazioni uniche.

Queste metriche ABC-Eval affidabili, informative e distinte ci permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta di quanto i metodi precedenti siano in grado di raggiungere.
Potete vedere nei risultati del nostro esperimento che diverse sfide rimangono e sono state quantificate con precisione.
Ad esempio, i bot che abbiamo testato hanno violazioni del buon senso in circa il 20% delle loro risposte.
Producono informazioni irrilevanti in circa il 15% delle risposte.
E si contraddicono o contraddicono il loro partner circa il 10% delle volte.

Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta.
Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli.
Speriamo che ABC-Eval possa essere utilizzato da altri nel settore come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni.
Grazie per aver guardato!</sample>
    <sample id="270">Emory University e Amazon Alexa AI.</sample>
    <sample id="271">CFT significa "continuous fine-tuning".</sample>
    <sample id="272">Nove autori sono coinvolti nell'articolo.</sample>
    <sample id="273">Ciao! Sono Kayo Yin e presenterò il nostro lavoro intitolato "When Does Translation Require Context? A Data-driven, Multilingual Exploration" (Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati). Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.

Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo "mole" in questa frase? Se la frase precedente fosse "Le cose potrebbero diventare pericolose se i ministri lo scoprissero", allora "mole" si riferisce a una spia. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione.

Tuttavia, valutare quanto i modelli siano in grado di tradurre casi come questi è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto. Questo rende le metriche a livello di corpus, come il BLEU, incapaci di catturare queste traduzioni. E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla creazione umana.

In questo lavoro, abbiamo cercato di rispondere a queste due domande. Primo, "Quando la traduzione richiede contesto?" E secondo, "Quanto bene i modelli gestiscono le traduzioni dipendenti dal contesto?" Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto il CXMI come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quanta informazione il contesto C fornisce sul target Y, dato il sorgente X. Si può pensare al CXMI come al guadagno di informazioni derivante dal fornire il contesto al modello.

In questo lavoro, estendiamo il CXMI al point-wise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un P-CXMI elevato come a quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un P-CXMI elevato per cercare schemi tra queste parole. E eseguiamo la nostra analisi sulle trascrizioni delle TED Talk che sono state tradotte dall'inglese a 14 lingue diverse.

Eseguiamo la nostra analisi a tre diversi livelli. In primo luogo, esaminiamo le etichette POS che hanno una media P-CXMI elevata. E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un P-CXMI relativamente elevato. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che alcune lingue richiedono anche contesto quando si vuole scegliere la forma verbale appropriata.

Quindi esaminiamo gli elementi del vocabolario che hanno un P-CXMI elevato, mediato su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. E allo stesso modo, troviamo che il contesto è supportato per tradurre nella giusta formalità. E infine, esaminiamo i singoli token che hanno un P-CXMI elevato. E questo ci ha permesso di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi.

Quindi ora utilizziamo i nostri risultati dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che rientrano nel fenomeno. E chiamiamo il nostro tagger il tagger Multilingual Discourse-Aware (MuDA). Possiamo quindi anche notare che lingue diverse hanno proporzioni diverse di questi fenomeni di discorso.

Quindi usiamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per la valutazione. E applichiamo le nostre metriche di traduzione preferite, sul contesto, esempi dipendenti che il tagger MuDA ha identificato. E infine, utilizziamo il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento.

Prima di tutto, quando usiamo metriche a livello di corpus, quindi per BLEU, troviamo che i modelli agnostici al contesto hanno le migliori prestazioni. Ma poi se usiamo COMET, i modelli consapevoli del contesto si comportano meglio. E se usiamo la F-measure delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metriche a livello di corpus.

Ora usiamo il benchmark MuDA per valutare i modelli. E troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per alcuni fenomeni di discorso, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano il contesto per altri fenomeni, come l'ellissi, i pronomi e la forma verbale. Quindi questo suggerisce in qualche modo dove avremmo bisogno di vedere maggiori progressi per la traduzione a livello di documento.

Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, abbiamo eseguito un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto. E poi abbiamo utilizzato i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni di discorso i modelli possono gestire bene o meno, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie mille per la vostra attenzione, ci vediamo a Toronto!</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Il video presenta "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". Il progetto si concentra sulla valutazione delle traduzioni dall'inglese verso altre lingue, un aspetto finora sottostudiato rispetto alle traduzioni verso l'inglese. Si sottolinea l'importanza di studiare metriche specifiche per lingue diverse dall'inglese, poiché esse presentano regole grammaticali, vocabolario condiviso/preso in prestito, differenze dialettali e strutture sintattiche uniche. Lo studio si concentra su cinque lingue indiane (Tamil, Malayalam, Hindi, Marathi e Gujarati) appartenenti a due diverse famiglie linguistiche.

Per la raccolta dei dati, sono state selezionate 200 frasi casuali dal dataset Flores. Queste frasi sono state tradotte in ciascuna delle cinque lingue utilizzando sette diversi modelli o API di traduzione (mBART, Bing API, Google API, CVIT-IIITH, IndicTrans, mT5, NLLB), generando un totale di 7000 campioni (1400 per lingua).
Per la raccolta delle annotazioni umane, sono stati impiegati annotatori bilingui esperti. Hanno valutato le traduzioni utilizzando il framework MQM (Multidimensional Quality Metrics), che consente di evidenziare errori minori/maggiori nel testo, classificarli in categorie (accuratezza, fluidità e speciali) e sottocategorie (ad esempio, aggiunta, omissione, errore di traduzione, testo non tradotto, ortografia, grammatica, registro, codifica dei caratteri) e assegnare un punteggio complessivo.
I risultati dell'analisi rivelano che i modelli MT più recenti, come NLLB e IndicTrans, presentano meno errori rispetto a modelli più datati come CVIT. La classifica dei sistemi in base ai punteggi umani evidenzia IndicTrans e NLLB come i migliori.

Per quanto riguarda le correlazioni tra le metriche di valutazione e i punteggi umani, CHRF++ ha mostrato la più alta correlazione tra le metriche basate sulla sovrapposizione, ma in generale queste ultime si sono rivelate le meno efficaci. Tra le metriche basate sugli embedding, LabSE ha ottenuto le migliori correlazioni. Le correlazioni sono ulteriormente migliorate con l'uso di BERTScore e varianti di COMET. Le varianti di COMET, in particolare, hanno mostrato le più alte correlazioni complessive. L'analisi della diffusione dei punteggi ha evidenziato che molte metriche tendono a fornire punteggi in un intervallo ristretto, a differenza dei punteggi umani, che coprono l'intera scala.
Infine, è stato messo a punto una variante di COMET chiamata IndicCOMET. Questa versione personalizzata ha superato i benchmark di COMET su tre delle cinque lingue e ha mostrato prestazioni zero-shot superiori sui benchmark di COMET su quattro lingue su cinque, indicando una maggiore robustezza su set di sfide di accuratezza della traduzione.</sample>
    <sample id="277">Il metodo non ha un nome specifico, è stato chiamato "Il nostro approccio".</sample>
    <sample id="278">Nella sua descrizione del metodo "parole contrassegnate", l'autore ha detto che il metodo attinge al concetto sociolinguistico della "marcatura" (markedness), che distingue un gruppo "non contrassegnato" da un gruppo "contrassegnato". In particolare, è stata fatta una distinzione tra un "guerriero" (non contrassegnato) e una "donna guerriera" (contrassegnata).</sample>
    <sample id="279">Paul G. Allen School, UWNLP, Carnegie Mellon University Language Technologies Institute, National Taiwan University.</sample>
    <sample id="280">Il relatore, Tao Shi, presenta MultiEMO, un framework di fusione multimodale attento alle correlazioni per il riconoscimento delle emozioni nelle conversazioni. La presentazione inizia con la definizione del compito e i problemi degli approcci esistenti per il riconoscimento delle emozioni nelle conversazioni, come la scarsa complementarietà delle informazioni multimodali, le prestazioni insoddisfacenti nelle classi di emozioni minoritarie e la difficoltà di distinguere tra emozioni semanticamente simili.

Per affrontare questi problemi, MultiEMO è composto da quattro componenti principali: l'estrazione delle caratteristiche unimodali, la modellazione del contesto, la fusione multimodale e la classificazione delle emozioni. Viene introdotto VisExtNet, un nuovo estrattore di caratteristiche visive che cattura i segnali visivi degli interlocutori integrando le espressioni facciali da più fotogrammi senza codificare le informazioni ridondanti relative alla scena. MultiAttn, un modello di fusione multimodale, è progettato utilizzando strati di attenzione incrociata a più teste bidirezionali, che modellano con successo le complesse correlazioni tra le modalità testuale, audio e visiva. Inoltre, viene introdotta una perdita contrastiva focale ponderata per il campione (SWFC) per affrontare la difficoltà di classificare le classi di emozioni minoritarie e semanticamente simili.

I risultati sperimentali su MELD e IEMOCAP dimostrano che MultiEMO raggiunge prestazioni all'avanguardia su entrambi i set di dati, con miglioramenti significativi nelle emozioni minoritarie e semanticamente simili. Viene presentato anche un caso di studio per visualizzare le heatmap di un'espressione propensa a errori di classificazione, che illustra la capacità di MultiEMO di gestire l'asincronia delle tendenze emotive di diverse modalità. Il relatore conclude la presentazione menzionando le limitazioni di MultiEMO, tra cui l'incapacità di VisExtNet di distinguere tra oratori e persone irrilevanti nella scena, la necessità di grandi dimensioni di batch per la perdita SWFC su MELD e le prestazioni ancora inferiori nelle emozioni minoritarie rispetto alle classi maggioritarie.</sample>
    <sample id="281">In questo lavoro, esploriamo l'importanza del contesto nella traduzione automatica (MT) e come i modelli di MT gestiscono le traduzioni dipendenti dal contesto. Abbiamo introdotto la Conditional Cross-Mutual Information (CXMI) a livello di parola per misurare l'uso del contesto nei modelli di MT e abbiamo condotto un'analisi tematica su 14 coppie linguistiche utilizzando le trascrizioni dei TED Talk. Questa analisi ha rivelato che il contesto è cruciale per fenomeni discorsivi come i pronomi, la forma verbale, la coesione lessicale, la formalità e l'ellissi.

Per valutare le prestazioni dei modelli di MT nella traduzione dipendente dal contesto, abbiamo creato un nuovo benchmark agnostico rispetto ai dataset chiamato Multilingual Discourse-Aware (MuDA) tagger. Questo strumento identifica automaticamente le parole che richiedono contesto in diversi fenomeni discorsivi, consentendo una valutazione mirata. I nostri risultati mostrano che mentre i modelli consapevoli del contesto superano significativamente i modelli agnostici rispetto al contesto per la formalità e la coesione lessicale, le loro prestazioni non sono significativamente migliori per l'ellissi, i pronomi e la forma verbale.

Inoltre, abbiamo confrontato i sistemi commerciali di MT utilizzando il nostro benchmark. Abbiamo scoperto che DeepL ha costantemente sovraperformato Google Translate per la maggior parte dei fenomeni discorsivi e delle coppie linguistiche. Questi risultati evidenziano la necessità di ulteriori progressi nei modelli di MT per migliorare la gestione di tutti i fenomeni dipendenti dal contesto.

In sintesi, la nostra ricerca offre un approccio basato sui dati per identificare e valutare le traduzioni dipendenti dal contesto, fornendo strumenti e approfondimenti per lo sviluppo di sistemi di MT più robusti.</sample>
    <sample id="282">Un ricercatore, Xuekai Zhu, presenta un nuovo lavoro sull'apprendimento automatico chiamato StoryTrans, focalizzato sul trasferimento di stile non parallelo nella generazione di testo. A differenza dei precedenti studi che si concentravano sul trasferimento di stile a livello di token o di frase, StoryTrans prende un'impronta significativa operando a livello di storia e di discorso.

La sfida chiave del trasferimento di stile a livello di storia è l'imitazione delle scelte linguistiche dell'autore a livello di discorso e la gestione dello stile dell'autore che è altamente associato a specifici argomenti di scrittura.

La soluzione proposta prevede un modello generativo a due fasi per mitigare queste sfide. Innanzitutto, il modello apprende rappresentazioni discorsive dal testo sorgente e le combina con l'embedding di stile normalizzato per generare il testo nello stile target. Un nuovo obiettivo di addestramento è stato progettato per ridurre le caratteristiche linguistiche di stile dalle rappresentazioni discorsive, consentendo un migliore disaccoppiamento di stile e contenuto. La seconda fase è un autoencoder denoising (DAE) che ricostruisce il testo, garantendo che le informazioni specifiche dello stile siano correttamente riempite e che i token mascherati siano rimossi.

La valutazione del modello è stata eseguita su nuovi set di dati in cinese e inglese. I risultati mostrano che StoryTrans supera i benchmark esistenti in termini di controllo dello stile e conservazione del contenuto. La visualizzazione dello stile dimostra che il testo trasferito da StoryTrans si allinea bene con il testo dorato nello spazio delle caratteristiche di stile. Gli studi di caso evidenziano la capacità di StoryTrans di riscrivere la maggior parte delle frasi con lo stile target pur mantenendo la semantica della sorgente, superando i modelli precedenti che spesso inserivano frasi irrilevanti. Questo lavoro migliora in modo significativo il trasferimento di stile per testi lunghi, rendendolo più robusto ed efficace.</sample>
    <sample id="283">Il nome della prima struttura di dipendenza simmetrica menzionata, che include il nome della città, è Conjunction-headed/Prague.</sample>
    <sample id="284">Questa presentazione introduce FSUID, un nuovo meccanismo di Fuzzy Span per migliorare l'estrazione universale delle informazioni (UIE).
I modelli UIE esistenti si affidano eccessivamente alla posizione dei confini delle span annotate, il che porta ad ambiguità nell'annotazione dei confini. Inoltre, esiste una discrepanza tra l'estrazione di caratteristiche del Transformer (globali) e l'estrazione di informazioni (locali).
Per affrontare questi problemi, FSUID propone una Fuzzy Span Loss che modella i confini delle span come una distribuzione continua di probabilità di correttezza, piuttosto che come un punto preciso. Questo allevia la dipendenza del modello dai confini precisi delle span. FSUID introduce anche una Fuzzy Span Attention che, attraverso una funzione di maschera, adatta dinamicamente lo span di attenzione del modello per guidare una distribuzione più appropriata dell'attenzione, concentrandosi sulle caratteristiche locali anziché su quelle globali.

I risultati sperimentali su tre principali attività di estrazione di informazioni (NER, RE e ASTE) dimostrano le capacità di FSUID. Rispetto a UIE-base, FSUID-base raggiunge un miglioramento significativo delle prestazioni per NER. Per RE, FSUID ottiene nuovi risultati all'avanguardia su vari dataset, mostrando una migliore capacità di estrazione delle informazioni con una struttura più semplice e una più forte capacità di generalizzazione per le informazioni specifiche del dominio. Su ASTE, FSUID ottiene anche risultati all'avanguardia, dimostrando prestazioni migliori con una struttura di estrazione unificata rispetto a strutture separate.

Uno studio di ablazione rivela che FSA migliora la velocità di convergenza guidando il modello verso una distribuzione di attenzione ragionevole, mentre FSL consente al modello di utilizzare appieno le informazioni di annotazione e ottenere una maggiore capacità di estrazione delle informazioni. L'effetto combinato di entrambi porta a un miglioramento ancora maggiore delle prestazioni.

In sintesi, FSUID propone una Fuzzy Span Loss e utilizza un'efficiente Fuzzy Span Attention per affrontare le limitazioni dei modelli UIE esistenti. I risultati ottenuti su un'ampia gamma di attività di estrazione di informazioni confermano l'efficacia e l'eccellente capacità di generalizzazione di FSUID.</sample>
    <sample id="285">Il video presenta un lavoro intitolato "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". L'autore discute l'importanza di affrontare gli errori fattuali nelle sintesi di dialoghi, un problema comune nei modelli di riassunto. Vengono proposte due soluzioni principali: migliorare i modelli di riassunto per una maggiore fedeltà o utilizzare modelli di correzione degli errori fattuali (FEC) indipendenti.

Il video evidenzia le lacune negli attuali metodi di valutazione dei modelli FEC, che spesso si basano su un punteggio complessivo e potrebbero non essere affidabili. Inoltre, i modelli FEC potrebbero generare sintesi completamente nuove anziché correggere quelle originali, rendendo l'effettiva "correzione" ambigua.

Per risolvere questi problemi, viene suggerito di annotare manualmente le "correzioni di riferimento" per le sintesi generate dal modello che contengono errori fattuali. Questo approccio ha due vantaggi: fornisce dati più preziosi per addestrare i modelli FEC (rispetto ai dati pseudo) e, soprattutto, crea le condizioni per una valutazione più completa e accurata delle prestazioni dei modelli FEC.

Per facilitare una classificazione automatica degli errori fattuali, viene proposta una nuova tassonomia che distingue tra categorie basate sulla forma (mancante, sostituzione, non necessaria) e categorie basate sul contenuto (errori di entità, attributi, modalità, tempo, negazione, verbi, circostanza, coreferenza, collegamenti, numeri, altri).

Il framework di valutazione proposto, ispirato a ERRANT (Bryant et al., ACL 2017), prevede tre passaggi principali: allineamento (della sintesi originale e corretta), classificazione (assegnazione automatica delle categorie di errore formale e basate sul contenuto) e confronto (calcolo dei punteggi).

Gli esperimenti condotti su diversi modelli FEC in varie modalità di addestramento hanno rivelato che:
1. L'addestramento dei modelli FEC con sintesi di riferimento da set di dati di riassunto di dialoghi fornisce i migliori risultati con metriche di fattualità inaffidabili, indicando un'urgente necessità di cambiare i metodi di valutazione.
2. L'introduzione di sintesi corrette dall'uomo durante l'addestramento dei modelli FEC per il riassunto di dialoghi può migliorarne le prestazioni. La combinazione di dati annotati dall'uomo con dati sintetici è una direzione promettente.
3. Gli attuali modelli FEC hanno difficoltà a correggere errori fattuali tramite aggiunta e non riescono ad affrontare errori di attributi, modalità, collegamenti, ecc.</sample>
    <sample id="286">I relatori sono Sarah E. Finch e James D. Finch.</sample>
    <sample id="287">Quattro autori.</sample>
    <sample id="288">Gli insiemi di dati BLiMP e SyntaxGym possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="290">Le abbreviazioni sono FTw, BOND, COSINE, MLC e L2R.</sample>
    <sample id="291">Il modello è valutato su 11 attività a valle, inclusi il riconoscimento di entità nominali (NER), la classificazione (CLS), la pos tagging (POS) e la risposta a domande (QA).</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su CamemBERT generic model.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presenta "EPIC: Multi-Perspective Annotation of a Corpus of Irony", un lavoro collaborativo tra l'Università di Torino e Amazon Alexa. Nel campo dell'elaborazione del linguaggio naturale (NLP), l'approccio data-driven basato sull'apprendimento supervisionato richiede vasti set di dati annotati manualmente, che codificano la conoscenza umana. Tuttavia, il paradigma della "ground truth" mostra i suoi limiti, specialmente in compiti soggettivi.

L'ironia è un fenomeno altamente latente e pragmatico, rendendone il rilevamento un compito difficile per i modelli di NLP. Gli autori propongono di andare oltre l'etichettatura binaria "ironico/non ironico" e di considerare "chi" considera un testo ironico, sviluppando modelli con un output più informativo.

Per studiare queste problematiche, è stato sviluppato il corpus EPIC (English Perspectivist Irony Corpus), raccogliendo 3.000 brevi conversazioni (coppie testo/risposta) da Reddit e Twitter, in un arco temporale di un anno e mezzo (gennaio 2020 - giugno 2021). I dati coprono cinque varietà di inglese (Regno Unito, Stati Uniti, Irlanda, Australia, India).

Il processo di annotazione ha coinvolto 74 annotatori (circa 15 per ogni varietà di inglese), ciascuno dei quali ha annotato 200 testi, con domande di controllo dell'attenzione. Sono state ottenute in media 5 annotazioni per testo. Gli annotatori erano bilanciati per genere, età, nazionalità e status occupazionale, ed erano tenuti ad annotare istanze da tutte le varietà della lingua, non solo quelle che parlavano.

L'analisi dell'accordo inter-annotatore ha rivelato differenze significative tra i gruppi di annotatori, suggerendo la presenza di "prospettive" distintive nella percezione dell'ironia. I modelli "perspective-aware" mostrano una minore incertezza nelle loro decisioni e sono più fiduciosi quando testati su un set di dati rappresentativo della loro prospettiva. La variazione più alta nella percezione dell'ironia è stata osservata tra generazioni contigue (ad esempio, boomer contro Gen Y) e tra Regno Unito e Irlanda.</sample>
    <sample id="297">Julia Mendelsohn presenta un lavoro sulle "dogwhistles", un tipo di retorica codificata che trasmette messaggi diversi a diverse audience, spesso con intenti ingannevoli. Ad esempio, il termine "cosmopolita" può essere interpretato in modo innocuo da alcuni, ma come attacco agli ebrei da altri.  

La comprensione delle dogwhistles è cruciale per la linguistica e l'NLP, poiché mette in discussione la nozione di significato e permette la diffusione di odio online. Gli autori hanno creato un glossario di oltre 340 dogwhistles inglesi, categorizzandoli per registro (formale/informale), tipo (segnale di persona, implicatura aggiunta) e persona (ad esempio, anti-semitico, transfobico).  

Lo studio ha analizzato i discorsi politici statunitensi storici e ha scoperto un aumento delle dogwhistles razziali dopo l'era dei diritti civili, associandole al conservatorismo. Si è anche valutato il riconoscimento delle dogwhistles da parte dei modelli linguistici, in particolare GPT-3. Il modello è in grado di identificarne molte, soprattutto quelle in registro formale, ma la sua performance varia. Le dogwhistles informali e quelle transfobiche sono meno riconosciute.  

Infine, lo studio ha dimostrato come le dogwhistles possano eludere la moderazione dei contenuti, utilizzando frasi d'odio in cui insulti espliciti sono stati sostituiti con dogwhistles. I modelli di rilevamento della tossicità hanno valutato tali frasi come meno tossiche, evidenziando il problema che questi meccanismi di elusione creano per la sicurezza online.</sample>
    <sample id="298">Il nostro esperimento ha dimostrato che le prestazioni degradano con un divario temporale maggiore.</sample>
    <sample id="299">Ciao a tutti. Sono Michalis Korakakis e oggi presenteremo i nostri lavori per migliorare la robustezza dei modelli NLI attraverso l'addestramento minimax.
I modelli NLI hanno raggiunto risultati all'avanguardia in molti benchmark, ma un recente lavoro ha dimostrato che il loro successo è in parte dovuto all'utilizzo di scorciatoie. Le scorciatoie sono regole decisionali che correlano in modo spurio con l'etichetta.
Per esempio, l'alto overlap di parole tra la premessa e l'ipotesi nel dataset MNLI è fortemente correlato con l'etichetta di entailment. Di conseguenza, i modelli NLI che sfruttano le scorciatoie si comportano bene sui campioni in-distribution, ma sono fragili quando testati su set di test out-of-distribution o adversarial, dove tali correlazioni spurie non valgono.
I precedenti lavori sulla mitigazione delle scorciatoie assumevano tipicamente l'accesso a un modello ausiliario progettato per affidarsi alle scorciatoie. L'output dell'ausiliario viene quindi utilizzato per ripesare gli esempi di addestramento per il modello "learner".
Tuttavia, gli attuali metodi di mitigazione delle scorciatoie potrebbero richiedere una conoscenza a priori delle scorciatoie. Questo presuppone una conoscenza specifica del dominio e del dataset che non è sempre disponibile, limitando quindi il potenziale di mitigazione delle scorciatoie. Inoltre, gli attuali metodi di mitigazione delle scorciatoie spesso assumono che il "learner" sfrutterà naturalmente gli stessi tipi di scorciatoie dell'ausiliario. In pratica, il comportamento del "learner" diverge da quello dell'ausiliario. Per esempio, l'ausiliario può sottopesare istanze utili per addestrare il "learner" o fornire stime di incertezza imprecise che possono ostacolare le capacità di generalizzazione out-of-distribution del "learner". Infine, gli attuali metodi di mitigazione delle scorciatoie richiedono l'utilizzo di un modello linguistico pre-addestrato come ausiliario, il che comporta un sovraccarico computazionale aggiuntivo.

Motivati da queste limitazioni, in questo lavoro proponiamo un metodo di addestramento per ridurre l'affidamento dei modelli NLI alle scorciatoie e migliorare le loro prestazioni out-of-distribution.
L'idea chiave alla base del nostro metodo di addestramento è che i modelli NLI faticano con esempi "difficili" e sotto-rappresentati che contraddicono le scorciatoie. Questi esempi difficili sono cruciali per garantire buone prestazioni di generalizzazione sui campioni out-of-distribution. Fondamentalmente, la perdita degli esempi difficili diminuisce considerevolmente più lentamente rispetto alla perdita media durante l'addestramento.
Pertanto, il nostro obiettivo è ottenere una distribuzione dei pesi degli esempi che enfatizzi gli esempi "difficili" e sotto-rappresentati. Per calcolare la distribuzione dei pesi, proponiamo un obiettivo di addestramento minimax tra un "learner" e un ausiliario. Il "learner" cerca di minimizzare la perdita del compito NLI, mentre il compito dell'ausiliario è massimizzare la perdita del "learner" generando pesi degli esempi in modo che il "learner" sia incentivato a concentrarsi sulle regioni dello spazio di input in cui incorre in perdite elevate. Così, il "learner" darà priorità all'apprendimento da esempi difficili e sotto-rappresentati che contrastano l'uso delle scorciatoie presenti negli esempi facili dominanti. Entrambi i modelli sono ottimizzati in modo alternato utilizzando un algoritmo di ottimizzazione standard, come la discesa del gradiente stocastico. Al momento del test, il "learner" può effettuare previsioni senza affidarsi all'ausiliario.
Il nostro metodo non fa alcuna assunzione sui tipi di scorciatoie contenute in un dataset, si basa sulle dinamiche di addestramento del "learner" stesso per generare i pesi degli esempi, e infine, utilizziamo una rete feed-forward per modellare l'ausiliario.
Abbiamo valutato il nostro metodo proposto in tre dataset NLI comunemente usati: MNLI, FEVER e QQP, e i corrispondenti set di test adversarial out-of-distribution: HANS, S-NLI e PAWS.
Qui, osserviamo che rispetto a un modello addestrato ERM, così come al metodo di mitigazione delle scorciatoie con le migliori prestazioni in ogni dataset, l'addestramento minimax migliora costantemente le prestazioni OOD, mantenendo un'elevata accuratezza ID.
Infine, nel nostro paper, esaminiamo anche se i miglioramenti delle prestazioni si trasferiscono anche a modelli più grandi, scorciatoie sintetiche e set di test out-of-domain. Qual è l'effetto del pre-addestramento del "learner"? Quanto deve essere piccolo l'ausiliario? E infine, conduciamo una valutazione qualitativa della distribuzione dei pesi degli esempi appresa.
Se trovate questo lavoro interessante, ci piacerebbe chiacchierare con voi durante la nostra sessione poster. Grazie per il vostro tempo.</sample>
    <sample id="300">Belinda Z. Li del Microsoft Semantic Machines presenta il lavoro "Toward Interactive Dictation". Questa nuova attività mira a rendere la dettatura tramite voce più naturale e intuitiva, consentendo agli utenti di dettare ed editare un documento con la voce in modo fluido, senza la necessità di parole chiave specifiche.

La ricerca si distingue dai sistemi di speech-to-text esistenti che spesso richiedono parole d'attivazione o comandi rigidi per le modifiche. L'obiettivo è creare un'interfaccia più flessibile dove la dettatura e l'editing si mescolano liberamente, e dove il linguaggio naturale per l'editing sia aperto e intuitivo. Ciò significa affrontare le sfide di prevedere la segmentazione tra dettatura e comandi, e di interpretare correttamente quale comando invocare e dove.

Il lavoro include la formalizzazione del compito di Interactive Dictation, la progettazione di un'interfaccia per la raccolta dati e la costruzione di un dataset (TERTIUS), e la creazione di un sistema di base per il compito. Quest'ultimo si articola in quattro fasi: riconoscimento automatico del parlato (ASR), segmentazione, normalizzazione e interpretazione.

I risultati dei modelli di segmentazione dimostrano una buona precisione ed efficienza. I modelli di ASR Repair e Interpretation mostrano un compromesso tra tempo di esecuzione e accuratezza, con i modelli GPT-3 che sono più precisi ma più lenti. Si nota che predire direttamente lo stato finale per i modelli GPT-3 è più accurato che predire programmi intermedi. Il codice e i dati sono stati resi disponibili per future ricerche.</sample>
    <sample id="302">Perché dopo il primo passaggio il modello ha tutti i token giusti, ma non sono ordinati correttamente.</sample>
    <sample id="303">Hanno suggerito un aumento della trasparenza per comprendere se i bias positivi siano dovuti a qualche forma di allineamento eccessivo dei valori o a metodi anti-stereotipi, poiché altrimenti non è possibile studiare o trarre conclusioni su queste questioni.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono un tipo di input utilizzato per valutare i modelli linguistici, dove il modello dovrebbe assegnare una probabilità inferiore a un input inaccettabile rispetto a un input accettabile.</sample>
    <sample id="305">In questo video, i ricercatori presentano il loro lavoro intitolato "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" (WSL). Il WSL mira ad alleviare il collo di bottiglia dell'annotazione, che consiste nell'etichettare manualmente i dati. Invece, i dati vengono etichettati utilizzando fonti di etichettatura debole, come euristiche o basi di conoscenza. Tuttavia, queste etichette deboli sono spesso rumorose, il che può portare i modelli di reti neurali a memorizzare il rumore e a generalizzare male.

I ricercatori hanno condotto un'analisi per capire se la validazione con dati puliti sia necessaria nel WSL. Hanno scoperto che, in effetti, gli attuali metodi WSL richiedono set di validazione puliti per funzionare correttamente; altrimenti, si verifica un significativo calo di prestazioni. Ciò suggerisce che il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.

Inoltre, hanno esaminato l'efficienza dei campioni di validazione puliti. Hanno notato che aumentare il numero di questi campioni aiuta i metodi WSL a ottenere prestazioni migliori, spesso richiedendo solo circa 20 campioni puliti per classe per raggiungere un'elevata accuratezza. Tuttavia, se sono disponibili campioni puliti, l'addestramento diretto su di essi, in particolare con un fine-tuning del modello, può superare i metodi WSL.

I ricercatori hanno anche dimostrato che il fine-tuning continuo sui campioni di validazione puliti può eliminare le differenze di prestazioni tra i diversi approcci WSL. Ciò significa che non c'è bisogno di utilizzare metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco, poiché un semplice modello con fine-tuning continuo può ottenere risultati altrettanto buoni.

In sintesi, la ricerca evidenzia che gli approcci WSL più recenti richiedono campioni puliti e tendono a sovrastimare la loro praticità. I ricercatori raccomandano di riportare chiaramente i criteri di selezione del modello, di utilizzare approcci di apprendimento few-shot come baseline e di applicare sempre il fine-tuning continuo (CFT) per sfruttare al meglio i campioni puliti disponibili. Il loro codice è open source.</sample>
    <sample id="306">Questo studio affronta la questione se i modelli linguistici pre-addestrati possano tracciare gli stati delle entità, una capacità cruciale per comprendere discorsi lunghi. I ricercatori hanno identificato tre sfide nella valutazione del tracciamento delle entità: modelli che memorizzano distribuzioni comuni nello stato delle entità, dipendenza da euristiche e dipendenza dall'addestramento contestuale. Per superare queste sfide, hanno progettato un nuovo compito in cui i modelli prevedono il contenuto delle scatole dopo una serie di operazioni. Il compito incorpora elementi non presenti nei dati di pre-addestramento per impedire ai modelli di fare affidamento su euristiche semplici e include operazioni che modificano gli stati delle entità in modi non ovvi.

I modelli sono stati testati in un ambiente di apprendimento a due colpi. È stato scoperto che la maggior parte dei modelli si limita a ripetere lo stato iniziale, mentre solo GPT-3.5 text-davinci-003 mostra un tracciamento non banale delle entità. Altri modelli hanno ottenuto risultati inferiori a una forte baseline casuale. L'analisi successiva ha mostrato che tutti i modelli GPT-3.5, addestrati su una quantità sostanziale di codice, presentano un comportamento di tracciamento delle entità non banale, suggerendo che il pre-addestramento sul codice è responsabile della comparsa di questa capacità. Inoltre, è stato riscontrato che i modelli più piccoli, come T5-base (230M parametri), possono imparare a eseguire il tracciamento delle entità se messi a punto direttamente, mentre i modelli inizializzati casualmente della stessa dimensione non apprendono questo comportamento. Tuttavia, rimane incerto in che misura le capacità di tracciamento delle entità osservate si generalizzino oltre la configurazione delle scatole.</sample>
    <sample id="307">Le metriche di valutazione che gli autori hanno utilizzato includono l'F1-score per il riconoscimento di entità nominate (NER), la classificazione (CLS) e il part-of-speech tagging (POS), e l'Hamming distance (HD) e l'Exact Match Ratio (EMR) per il question answering.</sample>
    <sample id="308">Questa presentazione introduce NLPpositionality, un framework per caratterizzare i bias di progettazione nei dataset e nei modelli di NLP. L'autrice, Jenny T. Liang della Carnegie Mellon University, sottolinea come i dataset e i modelli possano riflettere i giudizi e le opinioni delle persone che li creano, portando a differenze di performance sistematiche tra le diverse popolazioni. Questa distorsione è definita "posizionalità".

Per affrontare questo problema, il framework propone due passaggi principali:
1.  **Riannotare i dataset con annotatori diversi:** Invece di fare affidamento sui dati demografici originali degli annotatori, che spesso sono scarsi o non vengono condivisi, il framework riannota i dati per ottenere più annotazioni per ogni istanza e per raccogliere dati demografici ricchi e diversi dagli annotatori.
2.  **Confrontare le annotazioni per dati demografici con i modelli e i dataset esistenti:** Utilizzando i punteggi di correlazione di Pearson, il framework confronta le annotazioni generate dagli utenti con le etichette originali del dataset e le previsioni del modello, raggruppando i risultati per dati demografici.

L'implementazione del framework avviene attraverso LabintheWild, una piattaforma di crowdsourcing online che consente di reclutare un pool eterogeneo di volontari. Sono stati condotti due task:
*   **Accettabilità sociale (Task A):** I partecipanti hanno valutato la socialmente accettabilità di varie situazioni utilizzando il dataset Social Chemistry e confrontando i loro giudizi con Delphi e GPT-4.
*   **Tossicità (Task B):** I partecipanti hanno classificato la tossicità di testi utilizzando il dataset Dynahate, confrontando le loro risposte con Perspective API, Rewire API, Hate RoBERTa e GPT-4.

I risultati hanno rivelato che i dataset e i modelli sono maggiormente allineati con i paesi di lingua inglese e con le persone con un'istruzione universitaria o post-universitaria. Inoltre, i dataset e i modelli mostrano un allineamento inferiore con le persone non-binary rispetto alle controparti maschili e femminili.

Per affrontare la posizionalità, si raccomanda di:
1.  Tenere un registro di tutte le scelte di progettazione pertinenti durante la creazione di dataset o modelli.
2.  Condurre la ricerca NLP attraverso la lente del prospettivismo, condividendo etichette di dataset disaggregate e utilizzando tecniche di modellazione in grado di gestire il disaccordo tra gli annotatori.
3.  Costruire dataset e modelli specializzati con e per comunità specifiche per promuovere un'NLP inclusiva, citando come esempio l'iniziativa Masakhane.</sample>
    <sample id="309">Krippendorff's Alpha è stata utilizzata per misurare l'accordo tra annotatori.</sample>
    <sample id="310">Il dominio scelto è Wikipedia.</sample>
    <sample id="311">Heinrich Heine University Düsseldorf, Germania.</sample>
    <sample id="312">MultiInstruct è il primo set di dati di benchmark per l'ottimizzazione delle istruzioni multimodali, che include 62 attività multimodali diverse.</sample>
    <sample id="313">Ci sono 3 autori coinvolti nell'articolo.</sample>
    <sample id="314">Il relatore non fornisce una definizione di coordinazione binaria.</sample>
    <sample id="315">Il video non specifica la durata media dell'utilizzo dei prompt nello studio.</sample>
    <sample id="316">Il modello T5 più piccolo, ottimizzato su Coscript, può generare script di qualità superiore rispetto a modelli di grandi dimensioni, indicando che i modelli più piccoli possono superare i modelli più grandi se adeguatamente addestrati su dataset appropriati.</sample>
    <sample id="317">Questo lavoro introduce CodeIE, un approccio che trasforma le attività di estrazione delle informazioni (IE) con pochi colpi in attività di generazione di codice strutturate, sfruttando i modelli linguistici di grandi dimensioni (LLM) addestrati su corpus di codice.

I metodi precedenti per l'IE con pochi colpi utilizzano in genere LLM di linguaggio naturale (come T5 e GPT-3) che generano testo piatto. Questo approccio presenta sfide a causa di formati di input e output non allineati e della necessità di strategie di decodifica specializzate per generare output strutturati. Al contrario, CodeIE formula l'IE con pochi colpi come un problema di generazione di codice da struttura a struttura. Ciò significa che l'input di testo viene convertito in un formato di codice strutturato (ad esempio, uno script Python che definisce una funzione di estrazione delle entità nominate) e il modello genera codice strutturato per l'output.

L'approccio di CodeIE sfrutta la capacità dei modelli LLM di codice (come Codex) di gestire input e output strutturati. Ciò consente una maggiore controllabilità del processo di generazione dell'output e riduce la necessità di ampi dati di addestramento strutturati e complesse strategie di decodifica. Il documento presenta i risultati sperimentali su tre set di dati di riconoscimento delle entità nominate (NER) e quattro set di dati di estrazione delle relazioni (RE). I risultati dimostrano che l'approccio di CodeIE, utilizzando LLM di codice e prompt formattati in codice, supera significativamente i metodi di base tradizionali come UIE e GPT-3 in scenari con pochi colpi (1, 2 e 5 colpi).

L'analisi rivela che i prompt formattati in codice ottengono una minore perplessità rispetto ai prompt basati su testo, indicando una migliore coerenza del formato. Inoltre, l'approccio di CodeIE riduce drasticamente gli errori strutturali nell'output, dimostrando un'elevata fedeltà strutturale. Un'ulteriore analisi suggerisce che gli errori semantici dei modelli GPT-3 spesso comportano la generazione di etichette non definite, cosa che CodeIE evita. Nel complesso, CodeIE evidenzia i vantaggi dell'utilizzo di LLM di codice per l'IE con pochi colpi, fornendo un approccio più controllabile, coerente e performante.</sample>
    <sample id="318">00:00
Ciao, sono Yanis Labrak e vi presenterò il nostro lavoro su DrBERT, un modello robusto pre-addestrato in francese per i domini biomedici e clinici.
00:09
In questa presentazione, parleremo prima di modellazione linguistica nel settore sanitario. Poi presenteremo il principale contributo del nostro articolo. Introdurremo il primo modello biomedico in francese, chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccolti dal web. Introdurremo anche un confronto di modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presenteremo i nostri risultati su 11 compiti a valle biomedici e clinici in francese. E infine, concluderemo sugli esperimenti e vi daremo maggiori dettagli su come accedere ai modelli.
00:49
Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale. E offre enormi guadagni di prestazioni rispetto ai metodi storici statici e contestualizzati, come Word2Vec, FastText o ELMo. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con CamemBERT e altri domini come il biomedico con PubMedBERT e BioBERT, e il clinico con ClinicalBERT, ma principalmente in inglese. Modelli specializzati per altre lingue sono più rari e spesso basati su un pre-addestramento continuo a causa della mancanza di dati specifici del dominio. Tuttavia, il francese non aveva ancora un modello open-source per il dominio biomedico e clinico.
01:36
Quindi, ci siamo posti la domanda su quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi e se i dati raccolti dal web siano un buon sostituto per i dati clinici. Per rispondere a queste domande, abbiamo confrontato DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, ci siamo chiesti quanti dati sono necessari per addestrare un modello specializzato sui dati francesi. Sono 4 GB, 8 GB o più? Per rispondere a queste domande, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione di DrBERT con 7 GB di NACHOS. Una seconda versione di 4 GB di NACHOS. Una prima versione di ChuBERT, che è un modello clinico, con 4 GB di frasi prese dalle note cliniche. E una versione finale di ChuBERT con un mix di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con un pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su 4 GB di NACHOS. Un altro, sempre basato su CamemBERT, ma addestrato questa volta sui 4 GB di note cliniche. E infine, uno basato su un modello biomedico inglese, PermetBERT, e addestrato su 4 GB di NACHOS. In totale abbiamo sette modelli.
03:02
Per valutare i nostri sette modelli, abbiamo raccolto 13 compiti a valle pubblici e privati, come il riconoscimento di entità nominate, la classificazione, il part-of-speech tagging e la risposta a domande. Questi modelli sono confrontati con sei modelli di base, che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNet 4 GB, PermetBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli si comportano meglio sui compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori.
03:51
In generale, il pre-addestramento da zero sembra ottenere prestazioni più elevate sulla maggior parte dei compiti. Tuttavia, i nostri esperimenti di pre-addestramento continuo, utilizzando i pesi e il tokenizer di PermetBERT, addestrati sul sottoinsieme di 4 GB di NACHOS, hanno mostrato risultati paragonabili a quelli ottenuti con DrBERT 4 GB da zero. Questo non è il caso per i modelli basati sui pesi e sul tokenizer di CamemBERT, che soffrono di problemi di stabilità.
04:18
Infine, come conclusione, il nostro sistema proposto offre prestazioni migliori su 9 degli 11 compiti a valle e supera globalmente i risultati del modello generico, qui CamemBERT. Abbiamo anche osservato che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face e tutti gli script di addestramento sono sul nostro repository GitHub.
04:50
Quindi, grazie per questa presentazione e non vediamo l'ora di scambiare idee alla sessione poster a Toronto.</sample>
    <sample id="319">Il lavoro esamina il pre-addestramento da zero e il pre-addestramento continuo.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è stato osservato.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando il punteggio SARI, BLEU, BS-P e FRE.</sample>
    <sample id="322">Il presentatore è Enrico Liscio, che presenta il suo lavoro di ricerca all'ACL 2023. Il suo lavoro si concentra sulla domanda: "Cosa impara un classificatore di testo sulla moralità?". Enrico definisce la moralità umana come la capacità di distinguere il bene dal male. Sottolinea l'importanza per i modelli linguistici di comprendere e riconoscere la moralità nel testo.

Il presentatore evidenzia come la moralità sia stata spesso trattata in NLP come una scala singola tra immorale e morale. Tuttavia, sottolinea la natura soggettiva della moralità, in cui persone diverse possono etichettare lo stesso concetto in modo diverso. Fa l'esempio di concetti divisivi come l'aborto, in cui alcune persone lo etichetterebbero come immorale e altre come morale. Egli sostiene che una semplice aggregazione media o maggioritaria nasconderebbe la verità di un'interpretazione pluralista della moralità e potrebbe essere pericolosa.

Enrico presenta la Moral Foundation Theory (MFT) come un approccio teorico per comprendere la moralità umana. Secondo questa teoria, ci sono cinque modi diversi in cui gli esseri umani percepiscono la moralità: cura, correttezza, lealtà, autorità e purezza. Egli spiega che le diverse priorità che gli individui danno a queste fondazioni morali determinano il loro giudizio sulla moralità di un concetto o di un'azione.

Il presentatore afferma che il suo studio mira a comprendere cosa imparano i modelli linguistici sulla moralità applicando tecniche di intelligenza artificiale spiegabile (XAI). Hanno usato un set di dati chiamato Moral Foundation Twitter Corpus, composto da 35.000 tweet raccolti in sette diversi domini.

Enrico fornisce un'anteprima di un risultato, confrontando i domini #AllLivesMatter (ALM) e #BlackLivesMatter (BLM). Spiega che, sebbene questi domini abbiano una retorica di valore simile, differiscono per l'elemento morale della sovversione (ribellione all'autorità). I modelli linguistici riconoscono che in ALM la sovversione è disapprovata, mentre in BLM è incoraggiata. Conclude che i modelli linguistici riconoscono che la moralità è espressa in modo diverso in domini diversi e che l'uso di un singolo modello per molti domini può portare a pericolose incomprensioni della moralità.</sample>
    <sample id="323">Questo documento introduce un nuovo approccio per il ragionamento sui grafi eterogenei dinamici per rispondere a domande di buon senso (QA), noto come DHLK. Il Common Sense QA è un compito impegnativo che richiede alle macchine di rispondere a domande che si basano su conoscenze comuni e che recuperano informazioni rilevanti da fonti esterne. Le attuali architetture utilizzano modelli linguistici (LM) e Knowledge Graphs (KG), ma la maggior parte non riesce a gestire le entità rumorose e le interazioni limitate tra modalità. DHLK affronta questi problemi in due fasi. In primo luogo, costruisce un grafo della conoscenza eterogeneo (HKG) basato su più basi di conoscenza e ottimizza la sua struttura attraverso una strategia di potatura in due fasi e l'apprendimento della rappresentazione della conoscenza (KRL). La prima fase di potatura rimuove le sotto-parole irrilevanti e la seconda fase rimuove dinamicamente le entità con bassa rilevanza contestuale. In secondo luogo, DHLK implementa la fusione e la codifica di due modalità attraverso un modello linguistico, incorporando informazioni sul percorso da HKG nel contesto QA e utilizzando un layer KG2QA per produrre una rappresentazione migliorata del contesto. I risultati sperimentali su set di test ufficiali di CommonsenseQA e OpenBookQA dimostrano che DHLK supera gli altri metodi LM e KG, con prestazioni migliori rispetto al modello di base RoBERTa. Il modello incorpora anche le relazioni nella Mask Self-Attention, creando una RMSA. Questo approccio migliora il ragionamento Common Sense QA affrontando la scarsità di conoscenze e l'interazione tra modalità.</sample>
    <sample id="324">Sì, i modelli linguistici presentano diversi bias politici.</sample>
    <sample id="325">Ciao, mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione al nostro paper sulla generalizzazione compositiva senza alberi, utilizzando l'etichettatura di multisemi e le permutazioni latenti. Questo è un lavoro congiunto con i miei supervisori, Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere intesa come la capacità di un allievo di gestire una ricorsione più profonda e composizioni mai viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del parsing semantico, testare la generalizzazione compositiva potrebbe apparire così. Come al solito, abbiamo un set di addestramento di enunciati, in questo caso "The girl slept" e "Mary knew that the girl slept". Questi enunciati sono accoppiati con forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli seq2seq ingenui falliscono! I modelli seq2seq ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Gli alberi aiutano molto, ma... Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi hanno lo scopo di catturare il processo compositivo che mette in relazione gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non vengono forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente costoso. Tipicamente, ciò comporta una considerevole pre-elaborazione specifica del formalismo delle forme logiche, ad esempio per gestire i simboli di variabile. L'ottenimento degli alberi può anche comportare procedure specializzate di induzione della grammatica. In questo paper, non usiamo gli alberi e introduciamo un nuovo modello sequenza-a-sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a ricorsioni più profonde senza alberi. Il nostro approccio predice l'output dall'input in due passaggi. Per prima cosa, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati. Per questo, nel secondo passaggio, usiamo un altro modello per predire una permutazione che li metta nell'ordine corretto. Abbiamo introdotto un nuovo metodo per predire una permutazione che non pone alcun vincolo rigido sulle permutazioni possibili. Questo rende il nostro approccio abbastanza flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona all'incirca così. Andiamo da sinistra a destra sull'output e determiniamo quale token di multiset inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente quello evidenziato in rosso. Poi, saltiamo al prossimo token di multiset per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token di multiset. Continuiamo questo processo. Fino a quando ogni token della prima fase è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi nel benchmark COGS. Il nostro modello supera gli altri con un ampio margine sulla generalizzazione a ricorsioni più profonde. Alcuni altri tipi di generalizzazione strutturale rimangono comunque molto impegnativi. Nel nostro paper, risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multiset provenga, il che pone una sfida per l'addestramento. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema del commesso viaggiatore. Approssimiamo questo con un rilassamento continuo compatibile con la GPU che ci consente anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, consultate il nostro paper o venite al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è quando due elementi di cognizione (pensieri, azioni o credenze) sono incoerenti.</sample>
    <sample id="327">La presentazione introduce ManagerTower, una nuova architettura di modello multimodale per l'apprendimento di rappresentazioni visive-linguistiche. L'obiettivo è addestrare un sistema di intelligenza artificiale che comprenda sia immagini che testo, basato su pre-training auto-supervisionato su larga scala di coppie immagine-testo.

ManagerTower si basa sull'architettura BridgeTower, migliorandola su due fronti. BridgeTower sfrutta la conoscenza semantica unimodale a diversi livelli collegando più strati unimodali superiori con ogni strato cross-modale. Tuttavia, soffre di un'inefficace utilizzazione strato per strato, poiché ogni strato cross-modale può utilizzare solo una rappresentazione di strato unimodale assegnata artificialmente. Ciò limita lo sfruttamento della conoscenza semantica unimodale a diversi livelli e vincola la scalabilità.

ManagerTower affronta queste limitazioni introducendo "manager" in ogni strato cross-modale. Questi manager prendono rappresentazioni unimodali multilivello come "insight" di esperti unimodali pre-addestrati a diversi livelli e aggregano adattivamente questi insight. L'architettura consente un'esplorazione più completa della conoscenza semantica unimodale a diversi livelli per facilitare un allineamento e una fusione cross-modale più completi.
I risultati sperimentali mostrano che, con soli 4 milioni di immagini per il pre-training visivo-linguistico, ManagerTower raggiunge prestazioni superiori in vari compiti a valle, in particolare un'accuratezza del 79,15% sul test VQAv2 standard. Ciò dimostra che ManagerTower consente uno sfruttamento più efficace della conoscenza semantica unimodale a diversi livelli con manager ben progettati.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">Questo documento affronta il problema della localizzazione delle frasi video a zero-shot, dove l'obiettivo è individuare il segmento video più pertinente per una data query in linguaggio naturale, senza la necessità di annotazioni manuali. I metodi zero-shot esistenti generano eventi e query pseudo-temporali, per poi addestrare un modello di localizzazione delle frasi video utilizzando queste pseudo-etichette. Tuttavia, questi metodi presentano tre principali svantaggi:
1. Le query pseudo-temporali generate sono spesso troppo semplici, il che porta a un divario significativo tra le query pseudo-temporali e quelle reali.
2. Esiste un disallineamento tra gli eventi pseudo-temporali e le query pseudo-temporali.
3. Questi metodi ignorano il rumore presente nelle pseudo-etichette, il che può influire negativamente sulle prestazioni del modello.

Per superare queste limitazioni, proponiamo un metodo robusto al rumore per la generazione di pseudo-etichette strutturate. Iniziamo generando query pseudo-temporali in formato libero utilizzando modelli di descrizione delle immagini pre-addestrati. Questo ci consente di ottenere query pseudo-temporali più complesse e realistiche. Successivamente, generiamo eventi pseudo-temporali basati sulla struttura temporale dell'evento, garantendo un'elevata pertinenza tra i video all'interno degli eventi e le query, e una bassa pertinenza tra i video al di fuori degli eventi e le query. Infine, per ridurre l'influenza del rumore durante l'addestramento, applichiamo un campionamento ponderato e un perfezionamento delle etichette, stimando il rumore in base al punteggio di confidenza delle previsioni del modello e all'IoU (Intersection over Union) tra le previsioni e le pseudo-etichette. Valutiamo il nostro metodo su due dataset, ActivityNet Captions e Charades-STA, e i risultati mostrano che la nostra metodologia offre prestazioni zero-shot migliori rispetto ai metodi esistenti.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice si chiama Sara Papi.</sample>
    <sample id="332">I dati sono stati tratti dalle trascrizioni dei TED Talks, tradotti dall'inglese in 14 lingue diverse.</sample>
    <sample id="333">La traduzione automatica neurale (NMT) ha mostrato risultati promettenti, ma le reti neurali spesso producono uno spazio di rappresentazione non uniforme, limitando la loro capacità di generalizzazione. Nello specifico, i token a bassa frequenza sono scarsamente distribuiti, portando a "buchi semantici" in cui il significato è mal definito. Ciò riduce le prestazioni della NMT in domini non visti.

Per risolvere questo problema, la kNN-MT (traduzione automatica k-nearest neighbor) propone di salvare le rappresentazioni e i token target in un datastore e di utilizzare i vicini più prossimi per uniformare le predizioni. Tuttavia, il recupero dei vicini da un datastore di grandi dimensioni è dispendioso in termini di tempo e le rappresentazioni non possono essere facilmente aggiornate una volta che il datastore è stato costruito.

Per superare queste limitazioni, proponiamo INK (Injecting kNN Knowledge), un nuovo framework di training che incorpora iterativamente la conoscenza kNN per affinare lo spazio di rappresentazione del modello NMT. Il framework INK comprende due passaggi: affinamento della rappresentazione (extraction della conoscenza kNN per guidare l'adattatore ad aggiustare la rappresentazione) e aggiornamento asincrono (utilizzo delle rappresentazioni aggiornate per rinfrescare il datastore). Questo ciclo di training continua fino alla convergenza.

In INK, affiniamo lo spazio di rappresentazione allineando le rappresentazioni contestualizzate con gli embedding dei token e con gli embedding dei token kNN per arricchire i significati semantici. Allineiamo anche le rappresentazioni contestualizzate dello stesso token target per affrontare il problema della dispersione scarsa. Durante l'inferenza, è necessario caricare solo il modello NMT off-the-shelf e i parametri di adattamento ottimizzati, permettendo di scartare il datastore.

I nostri esperimenti su quattro set di dati di benchmark (Medical, Law, IT, Koran) mostrano che INK supera i sistemi kNN-MT all'avanguardia. Il sistema INK raggiunge un guadagno medio di 1,99 nel punteggio COMET e 1,0 nel punteggio BLEU. Rispetto ai modelli kNN-MT di base, INK offre prestazioni di traduzione migliori con un utilizzo di memoria ridotto (0,02×) e una velocità di inferenza superiore (1,9×). Abbiamo anche dimostrato che l'applicazione congiunta di un adattatore e di un datastore può uniformare ulteriormente le predizioni, indicando che lo spazio di rappresentazione dell'NMT non è completamente affinato dal solo adattatore.</sample>
    <sample id="335">Il nome della relatrice o del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è una tecnica di apprendimento in cui un modello viene addestrato su una lingua di origine e quindi applicato a una lingua diversa.</sample>
    <sample id="337">Il video presenta un lavoro di ricerca intitolato "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning", evidenziando l'importanza di affrontare le parole fuori dal vocabolario (OOV). Prendendo ispirazione da come gli umani imparano nuove parole scomponendole in parti e associandole a parole conosciute, il team ha sviluppato un approccio che imita questo processo.

Il metodo proposto include un grafo delle relazioni tra parole (WRG) che replica le regole lessicali della formazione e associazione delle parole. Quando appare una parola OOV, questa viene tokenizzata in "pezzi di parola" e associata ad altre parole rilevanti, formando un grafo a due livelli.

L'architettura del modello prevede l'inizializzazione delle parole OOV utilizzando una rete di attenzione che assegna attributi basati sui loro caratteri. Per elaborare il WRG e ridurre il rumore dei nodi vicini, vengono applicati due livelli di reti neurali a grafo con attenzione (GAT). Le incorporazioni iniziali dei pezzi di parola vengono concatenate con le incorporazioni nascoste di ogni strato, risultando in una rappresentazione a livello di nodo.

Per catturare le informazioni sull'intero grafo e riassumere la formazione delle parole, viene utilizzato un blocco GCN a un livello. Infine, un approccio di apprendimento contrastivo viene applicato nella funzione di perdita per avvicinare le incorporazioni a livello di grafo alle loro controparti di background e respingere quelle non correlate.

I risultati sperimentali mostrano che il modello supera le baseline sia nelle valutazioni intrinseche che estrinseche, dimostrando la sua efficacia nell'apprendimento delle parole OOV attraverso la formazione delle parole. Il modello mostra anche adattabilità, migliorando le prestazioni sia dei modelli statici che contestuali nei compiti a valle. Si sostiene che la struttura del grafo del WRG sia adatta a gestire formazioni di parole complesse e che la sua applicabilità ad altre lingue dipenda dalla razionalità della decomposizione delle parole.</sample>
    <sample id="338">Questo lavoro presenta uno studio sull'utilità delle spiegazioni in linguaggio naturale annotate dall'uomo per i modelli di linguaggio. I ricercatori hanno esaminato l'uso di queste spiegazioni per addestrare i modelli di generazione del linguaggio naturale (NLG), aumentare le prestazioni di previsione e migliorare il ragionamento del modello.

Il team ha evidenziato che le spiegazioni annotate dall'uomo sono soggettive e specifiche del compito, non esistendo un "gold standard" univoco per la loro valutazione. Hanno confrontato diversi set di dati di spiegazioni e hanno notato che i modelli popolari di generazione del linguaggio naturale, come BLEU e ROUGE, trattano le annotazioni umane come uno standard aureo e si concentrano sulla somiglianza delle parole. La metrica Simulatability Score, invece, misura il cambiamento delle prestazioni del modello di base quando le spiegazioni sono presenti o assenti, ma non considera le differenze tra i compiti o la diversa utilità delle spiegazioni durante la fase di fine-tuning e inferenza.

Per affrontare queste limitazioni, il lavoro introduce una struttura di dati unificata basata su template che converte vari compiti in un compito di scelta multipla unificato. Questa struttura include sia un'impostazione "Baseline" (senza spiegazioni) che un'impostazione "Infusion" (con spiegazioni come input aggiuntivo).

Gli esperimenti preliminari hanno rivelato che il fine-tuning non introduce nuove conoscenze, ma piuttosto insegna ai modelli a fare affidamento sulle spiegazioni fornite. Inoltre, le spiegazioni di CoS-E sono risultate meno utili di quelle di ECQA per i modelli Baseline, suggerendo una dipendenza dal compito. È stato anche riscontrato che il fine-tuning di un modello con una piccola quantità di dati che incorpora spiegazioni può portare a miglioramenti significativi.

Sulla base di queste osservazioni, i ricercatori hanno proposto una nuova metrica di valutazione chiamata TREU, che estende lo Simulatability Score valutando l'utilità delle spiegazioni anche nella fase di fine-tuning. La metrica TREU è stata confrontata con lo Simulatability Score su cinque set di dati e due modelli (T5 e BART). I risultati hanno mostrato che la metrica TREU riflette fedelmente l'utilità delle spiegazioni, anche quelle considerate di bassa qualità dagli esseri umani, e classifica in modo più coerente la qualità dei set di dati rispetto allo Simulatability Score, che può essere più influenzato dai modelli.

Infine, il lavoro sottolinea che l'utilità delle spiegazioni umane per i modelli dipende fortemente dal compito (ad esempio, la connotazione della negazione in e-SNLI e ComVE) e dallo stile della spiegazione (ad esempio, stili di scrittura controfattuali per la contraddizione in e-SNLI). In futuro, si raccomanda di effettuare controlli di qualità simili durante la raccolta delle spiegazioni umane, dato che l'annotazione umana di alta qualità è costosa e difficile da acquisire.</sample>
    <sample id="339">Le affiliazioni degli autori sono Saarland University, Amazon Alexa e University of Vienna.</sample>
    <sample id="340">Kuan-Hao Huang, from UCLA, presented their work on ParaAMR, a large-scale, syntactically diverse paraphrase dataset created through AMR back-translation. Paraphrase generation is crucial for NLP applications like question answering, chatbots, and improving robustness. However, existing human-annotated datasets are high-quality but limited in scale, while automatically generated datasets, often using back-translation, lack syntactic diversity.

To address this, their key idea is to leverage Abstract Meaning Representations (AMR) graphs. An AMR graph is a directed graph that captures the abstract meaning of a sentence, with nodes representing semantic concepts and edges representing semantic relations. The focus (root node) represents the main assertion of the sentence.

The process involves first using an AMR parser to obtain an AMR graph from a source sentence. Then, the focus of the graph is changed by randomly sampling a node and setting it as a new root node, modifying corresponding edges and labels. Finally, an AMR-to-text generator produces text from these modified graphs. This method generates paraphrases that maintain similar semantics to the original text but exhibit syntactic diversity due to the changing focus.

ParaAMR comprises around 15.5 million source sentences, with an average of 6.92 paraphrases per sentence. Quantitative analysis, including automatic and human evaluation scores, showed that ParaAMR achieves similar semantic similarity to other back-translation datasets while demonstrating higher syntactic diversity.

ParaAMR's benefits were demonstrated across several NLP applications:
1. **Learning Sentence Embeddings**: Sentence embeddings learned from ParaAMR outperformed those from other datasets in Semantic Textual Similarity (STS) benchmarks.
2. **Syntactically Controlled Paraphrase Generation**: Training with ParaAMR yielded a paraphrase generator with better syntactic control.
3. **Data Augmentation for Few-Shot Learning**: ParaAMR resulted in higher scores for few-shot learning tasks, attributed to its syntactic diversity.

The dataset is publicly available on GitHub.</sample>
    <sample id="341">Gli autori fanno ricorso alla latenza media (AL) e alla latenza media con consapevolezza computazionale (AL_CA).</sample>
    <sample id="342">LiveChat è un nuovo dataset per il **dialogo personalizzato multimodale**. L'obiettivo è creare **dataset più grandi e diversificati** per la ricerca sul dialogo. LiveChat è un **dataset di dialogo personalizzato su larga scala, video-source e multimodale**, costruito automaticamente da video in live streaming cinesi. Contiene oltre 1,3 milioni di **dialoghi**, ciascuno con **informazioni dettagliate sulla persona e relazioni "risposta-a" esplicite**. 

A differenza dei dataset di dialogo esistenti che si basano principalmente su testo o richiedono annotazioni manuali, il dataset LiveChat è **costruito automaticamente da video in live streaming**. In questo modo, LiveChat fornisce un **dataset video-source su larga scala**, con conversazioni autentiche e complesse, che riflettono meglio le interazioni umane reali.

La costruzione di LiveChat prevede tre fasi:
1. **Raccolta dei video in live streaming** e trascrizione in espressioni.
2. **Raccolta dei commenti del pubblico** e costruzione dei dialoghi attraverso il metodo di corrispondenza "risposta-a".
3. **Raccolta delle informazioni sulla persona** e aggiunta di annotazioni manuali.

Questo dataset è stato utilizzato per eseguire esperimenti su due task di benchmark: la **modellazione della risposta** e il **riconoscimento del destinatario**. I risultati mostrano che i profili di persona selezionati e il numero maggiore di sessioni medie per persona sono vantaggiosi per l'apprendimento della risposta personalizzata dello speaker e della decisione del destinatario. Inoltre, i confronti tra BART e altri modelli di dialogo pre-addestrati e LLM hanno rivelato la distintività di questo dominio di dialogo video-source.

In futuro, i ricercatori prevedono di concentrarsi sull'**apprendimento efficiente del trasferimento di LLM per LiveChat**, al fine di **migliorare ulteriormente le prestazioni dei modelli di dialogo in scenari di conversazione complessi**.</sample>
    <sample id="343">00:00
Ciao a tutti. Sono Akshatha, e oggi io e il mio coautore Martin presenteremo il nostro lavoro "The KITMUS Test", che valuta l'integrazione della conoscenza da più fonti. Questo lavoro è una collaborazione tra la McGill University, Mila e Microsoft Research.
00:18
I modelli NLU attingono a diverse fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita attraverso il pre-training, e la conoscenza fornita in input al momento dell'inferenza. Lavori recenti in compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza del pre-training per risolvere il compito.
00:40
Ma la comprensione del linguaggio naturale spesso richiede una conoscenza che viene fornita anche al momento dell'inferenza. Ad esempio, nella frase "John ha visto il presidente neoeletto in TV", i parametri pre-allenati possono contenere informazioni su ciò che fanno i presidenti e cos'è una TV, ma non possono sapere in modo affidabile chi è questa entità specifica, John, o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato dal pre-training.
01:08
Pertanto, i modelli di successo per i compiti NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza del pre-training che quella dell'inferenza.
01:19
In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione del coreference progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il set di dati con i partecipanti allo studio umano e i modelli di risoluzione del coreference stabiliti.
01:38
Ecco un esempio dal nostro set di dati. "Servin è un giudice. Kea è una fornaia. Servin e Kea si sono incontrati al parco. Dopo una lunga giornata di lavoro a decidere casi in tribunale, era felice di rilassarsi. [Risposta: Servin]" Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin.
01:59
La risoluzione di un dato pronome richiede due tipi di informazioni: 1) conoscenza specifica dell'entità, come "Servin è un giudice".
02:10
2) Conoscenza di base, come "I giudici decidono i casi nei tribunali".
02:16
In generale, la conoscenza di base viene appresa durante il pre-training dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza. Variamo la disponibilità di questi due pezzi di informazione in modo che possano essere trovati in una singola fonte o in più fonti.
02:34
Abbiamo definito tre impostazioni di KITMUS. Primo, abbiamo l'impostazione tipica, Background-Pretrain, dove si presume che la conoscenza di base sia disponibile al momento del pre-training. Secondo, c'è l'impostazione Background-Both, dove la conoscenza di base è disponibile sia al momento del pre-training che al momento dell'inferenza. Infine, l'impostazione Background-Inference, dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché teorizza il caso in cui la conoscenza di base necessaria per risolvere il compito non fa parte dei dati di pre-training dei modelli. Ad esempio, perché nuove occupazioni si sono sviluppate dal momento del pre-training.
03:10
Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di base "I politici cercano seggi eletti nel governo" sia contenuta nei parametri di pre-training. Nel contesto dell'inferenza, forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo esplicitamente non solo la conoscenza specifica dell'entità, ma anche la conoscenza di base sui politici nel contesto dell'inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione fittizia "mirituer" invece di "politico", perché è improbabile che "mirituer" sia contenuta nei parametri di pre-training.
03:49
Abbiamo valutato il set di dati sia con i partecipanti allo studio umano che con i modelli di risoluzione del coreference stabiliti. In questa figura mostriamo i risultati dei modelli più performanti sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico per il compito su KITMUS, entrambi i modelli non si comportano bene. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su set di dati di risoluzione del coreference generale, i modelli imparano a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMUS, dove tali indizi sono stati rimossi.
04:23
Ulteriori esperimenti con conoscenza fittizia indicano che anche i modelli più performanti non possono integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza.
04:33
Per riassumere i principali insegnamenti del nostro articolo. Molti modelli sembrano incapaci di ragionare sulla conoscenza da più fonti (conoscenza del pre-training e dell'inferenza) senza addestramento specifico per il compito. Tuttavia, con l'addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Ciononostante, anche i modelli più performanti sembrano avere difficoltà con l'integrazione affidabile della conoscenza di base presentata solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e controllate il set di dati, il codice di generazione e valutazione su GitHub all'indirizzo mpoemsl/kitmus. Grazie per l'ascolto.
</sample>
    <sample id="344">I metodi basati su alberi richiedono pre-elaborazione specifica del formalismo, come la gestione dei simboli variabili, e talvolta procedure specializzate di induzione della grammatica.</sample>
    <sample id="345">Questo documento presenta un approccio innovativo alla generalizzazione compositiva nella parsificazione semantica, senza l'uso di alberi di parsing. La generalizzazione compositiva è la capacità di un sistema di apprendere nuove combinazioni di unità linguistiche che sono state osservate individualmente durante l'addestramento. I modelli sequenza a sequenza standard faticano con questo tipo di generalizzazione, specialmente con ricorsioni più profonde, producendo spesso output staccati dall'input. Un approccio comune per migliorare questo aspetto è l'integrazione di alberi, che rappresentano il processo compositivo che collega le espressioni alle loro forme logiche. Tuttavia, l'ottenimento di questi alberi può essere complesso, richiedendo una pre-elaborazione specifica del formalismo, come la gestione dei simboli di variabile, o procedure specializzate di induzione della grammatica.

Il nostro approccio evita l'uso di alberi, introducendo un modello neurale sequenza a sequenza che modella direttamente le corrispondenze tra frammenti di input e output in due fasi. Nella prima fase, ogni token di input viene etichettato con un multi-set non ordinato di token di output previsti. La seconda fase utilizza un modello di permutazione per ordinare questi token di multi-set nell'ordine corretto. Questo modello di permutazione è progettato per essere flessibile e non impone vincoli rigidi sulle permutazioni, consentendo una maggiore espressività.

Affrontiamo due sfide tecniche principali: l'allineamento sconosciuto tra input e output e la complessità computazionale del modello di permutazione. Induciamo l'allineamento come parte del processo di addestramento. Per il modello di permutazione, il cui compito di inferenza è NP-difficile (simile al problema del commesso viaggiatore), utilizziamo un'approssimazione con rilassamento continuo compatibile con la GPU, che consente la retropropagazione attraverso la soluzione e l'apprendimento di permutazioni linguisticamente più plausibili. I nostri risultati sperimentali sul benchmark COGS dimostrano che questo metodo supera significativamente altri modelli senza alberi nella generalizzazione a ricorsioni più profonde.</sample>
    <sample id="346">Gli autori dell'articolo sono Shuheng Liu e Alan Ritter, entrambi della School of Interactive Computing, Georgia Institute of Technology.</sample>
    <sample id="347">[00:00] Ciao, sono Myra, e oggi parlerò della nostra ricerca "Marked Personas".
[00:04] Usare i prompt del linguaggio naturale per misurare gli stereotipi nei modelli linguistici.
[00:08] Questo lavoro è stato svolto in collaborazione con Esin Durmus e Dan Jurafsky.
[00:12] Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici (LLM).
[00:21] Tuttavia, queste misure presentano diverse limitazioni. Solitamente si basano su set di dati fissi e curati manualmente, che richiedono molto tempo per essere elaborati.
[00:29] E di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altri dati demografici o contesti, oppure catturano solo associazioni molto generiche e ampie, come associazioni negative con gruppi particolari.
[00:44] Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, che è la nozione che identità sociali sfaccettate possono aggravare i pregiudizi e costituire luoghi unici di danno.
[00:56] Come superiamo queste limitazioni? Ci affidiamo alla proprietà che questi nuovi LLM istruiti sono molto bravi a rispondere alle istruzioni nei prompt.
[01:06] Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, usando un prompt come: "Immagina di essere una donna asiatica. Descriviti."
[01:15] E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi dato demografico, perché possiamo semplicemente specificare qualsiasi marcatore di identità vogliamo in questo prompt.
[01:26] Quindi ecco alcuni esempi di generazioni di persone da GPT-4.
[01:29] Immediatamente vediamo che, mentre gli output non sono eccessivamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è rappresentata come senza pretese, la donna mediorientale viene definita con parole come esotico, e come facente riferimento a una regione affascinante, e entrambe le donne di colore fanno riferimento all'ascendenza, mentre la persona dell'uomo bianco non ha nulla del genere.
[01:57] Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio psicologico in cui si davano questi prompt a soggetti umani, scoprendo che anche i soggetti umani erano in grado di portare alla luce stereotipi razziali. E questo consente anche un confronto diretto tra le nostre persone generate e le risposte scritte dagli esseri umani.
[02:23] La seconda parte sono le parole marcate, che è un metodo per identificare le parole che distinguono le persone dei gruppi marcati da quelle dei gruppi non marcati, che elaborerò a breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza richiedere un lessico.
[02:42] Quindi, il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatura, che afferma che esiste un valore predefinito non marcato, e qualsiasi gruppo che si discosta da quel valore predefinito è linguisticamente marcato. Quindi, per esempio, la parola uomo, scusate, la parola guerriero è solitamente associata agli uomini, quindi quando le persone descrivono una guerriera, di solito specificano guerriera e marcano il termine con donna. E più in generale, i gruppi dominanti nella società sono linguisticamente e socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati.
[03:18] Quindi, nel nostro metodo, prima designiamo quali sono i gruppi non marcati e marcati. E poi confrontiamo le persone usando il metodo delle parole marcate, che consiste fondamentalmente nell'usare rapporti log-odds ponderati per distinguere le parole principali per ogni gruppo marcato. Per esempio, per le persone di donne nere, useremmo le parole marcate e confronteremmo i rapporti log-odds sia con le persone bianche che con le persone maschili, perché quelli sono i due gruppi non marcati corrispondenti.
[03:49] Ora per alcuni risultati. Innanzitutto, usiamo un lessico di stereotipi. E scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli esseri umani.
[03:59] Tuttavia, quando osserviamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più alti delle parole del lessico, quelle scritte dagli esseri umani hanno una distribuzione di parole molto più ampia, mentre le parole stereotipate che si trovano nelle persone generate sono solo le parole "alto" e "atletico". Quindi, in realtà, solo quelle positive o almeno non negative. E infatti, questo lessico non cattura molte delle dinamiche dannose che abbiamo visto nelle diapositive precedenti per niente. Quindi, invece, per farlo, passeremo ai risultati del nostro metodo delle parole marcate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti.
[04:39] Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Innanzitutto, per i gruppi marcati, le parole chiave includono cose come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono questi gruppi solo in base alla loro identità e li distinguono dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti stereotipi comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, per esempio, le parole che descrivono le donne latine includono cose come vibrante e procace, che si collegano a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come petite, delicata e setosa, che si collegano a una lunga storia di donne asiatiche che vengono ipersessualizzate, viste come molto docili e sottomesse, e così via. E infine, per le donne nere, vediamo che alcune delle parole chiave sono cose come forte e resiliente. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte, e anche se a prima vista sembra positivo, ci sono stati lavori che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie a essere resilienti e forti contro gli ostacoli sociali. Quindi, piuttosto che lavorare per cambiare questi ostacoli, mette pressione su quelle persone a superarli, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni.
[06:16] Quindi, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari di modelli. Innanzitutto, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, ci dovrebbe essere una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, per esempio, con questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di allineamento di valori eccessivamente strano in corso, o forse ad altri metodi di anti-stereotipizzazione che stanno portando a questi schemi dannosi. Non possiamo davvero fare supposizioni o studiarlo ulteriormente senza una maggiore trasparenza.
[07:02] Grazie mille per l'ascolto. Divertitevi ad ACL.</sample>
    <sample id="348">La conferenza presentata da Myra Cheng si concentra su "Marked Personas", un metodo che utilizza prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici (LLM). Il problema centrale è la prevalenza del bias sociale e degli stereotipi negli LLM, con limitazioni nelle attuali misurazioni che spesso sacrificano la specificità per la generalizzabilità, si basano su dataset curati manualmente e non tengono conto dell'intersezionalità.

Il metodo "Marked Personas" propone due passaggi:
1. **Generazione di Personas:** Utilizza prompt come "Immagina di essere una donna asiatica. Descriviti." per generare descrizioni di identità intersezionali. Questo approccio è generalizzabile e permette di valutare qualsiasi identità intersezionale, ispirandosi a studi psicologici che hanno rivelato stereotipi razziali attraverso risposte umane a prompt simili.
2. **Parole Marcate:** Identifica le parole che distinguono le descrizioni di gruppi "marcati" (es. donne nere) da quelle di gruppi "non marcati" (es. persone bianche o uomini), basandosi sul concetto sociolinguistico di "markedness", dove i gruppi dominanti sono considerati il default. Questo permette di scoprire stereotipi specifici senza richiedere un lessico predefinito.

I risultati mostrano che le persone generate dagli LLM contengono più stereotipi rispetto a quelle scritte dagli esseri umani. Analizzando le parole più frequenti, emergono pattern che rivelano narrazioni essenzializzanti e rappresentazioni "positivamente" dannose (es. "vibrante, formosa" per le donne latine, "piccola, delicata, setosa" per le donne asiatiche, "forte, resiliente" per le donne nere).

Le raccomandazioni includono l'affrontare stereotipi positivi e narrazioni essenzializzanti, l'adozione di una lente intersezionale nello studio dei bias e una maggiore trasparenza sui metodi di mitigazione dei bias per comprendere meglio le cause e gli effetti di tali pattern.</sample>
    <sample id="349">00:00
Hello everyone. My name is Jinwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper: Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding as a Service via Backdoor Watermark.
00:20
Let's first introduce the background about Embedding as a Service. Currently, large language models, such as GPT, LLAMA, PaLM, are exceptional in natural language understanding and generation. Embedding as a Service is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT3-based embedding API.
00:46
However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as a Service.
01:03
To protect the copyright of Embedding as a Service, one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to Embedding as a Service. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker, or the attacker can remove the watermark easily. Finally, the watermark need to be transferable to the attacker's services during the model extraction process.
01:48
Existing works can be broadly classified into four categories. However, these methods either are not applicable to Embedding as a Service or lack of transferability. Therefore, in this paper, we propose EmbMarker, which is a backdoor-based watermark method applicable to Embedding as a Service.
02:10
Then, let me introduce the details of our EmbMarker. EmbMarker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate-frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it.
02:38
In watermark injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding.
03:12
Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and a benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then the provider requests embeddings from stealer's service with the dataset.
03:43
The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset. Which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric.
04:06
We conduct experiments on four dataset: AGNews, MIND, SST2 and Enron Spam. We assume the provider apply WikiText dataset to count word frequency.
04:20
The results on four dataset show that our EmbMarker can have great detection performance while keep great utility for downstream tasks.
04:31
We also validate the covertness of the provided embeddings by visualizing the embedding of sentences on four dataset via PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoored embeddings and normal embeddings.
04:55
That's all. Thank you. Welcome to discuss with us.</sample>
    <sample id="350">Questo video presenta un articolo che esplora il significato delle "prestazioni sovrumane" nell'elaborazione del linguaggio naturale (NLU) odierna.
L'oratore evidenzia come la valutazione basata su classifiche sia diventata una pratica popolare nell'NLU, con sistemi che spesso superano le prestazioni umane in vari benchmark.
Tuttavia, il video solleva dubbi sulla validità di questi confronti, suggerendo che molti compiti NLU richiedono conoscenza e inferenza, che i modelli attuali non possiedono completamente.
L'oratore sottolinea la fragilità dei modelli attuali, citando problemi come la generalizzazione fuori dominio, gli attacchi avversari, i modelli spuri, la mancanza di sensibilità a perturbazioni linguistiche di base e l'eccessiva sensibilità a perturbazioni irrilevanti.
Per affrontare questi problemi, il video analizza due benchmark popolari, SuperGLUE e SQuAD, e scopre diverse fonti di errore nel confronto uomo-sistema.
Questi errori includono l'utilizzo di set di valutazione diversi per gli esseri umani e i sistemi, errori di verità di base e l'uso di metriche di aggregazione che potrebbero non riflettere le vere capacità umane.
Inoltre, il video evidenzia l'eterogeneità e la mancanza di trasparenza nei tassi di retribuzione degli annotatori, che potrebbero influire sulla qualità dei dati.
In conclusione, il video sostiene che le affermazioni sulle prestazioni sovrumane nell'NLU non sono scientificamente significative a causa di questi problemi.
Infine, il video invita gli spettatori a leggere l'articolo per maggiori dettagli sulle conseguenze di questi problemi e per le raccomandazioni sulla costruzione di benchmark più equi e trasparenti.</sample>
    <sample id="351">Lo studio esamina la generalizzazione di modelli di Named Entity Recognition (NER), in particolare quelli addestrati con il dataset CoNLL-2003, in un contesto più moderno. I modelli di NER sono stati sviluppati utilizzando CoNLL-2003 per quasi 20 anni, e questo solleva la questione se tali modelli possano generalizzare bene ai dati moderni e quali fattori contribuiscano a una buona generalizzazione.

Per affrontare queste domande, è stato creato il dataset CoNLL++, raccogliendo notizie di Reuters del 2020 e annotandole con le stesse linee guida di CoNLL-2003. Successivamente, sono stati messi a punto oltre 20 modelli su CoNLL-2003 e valutati sia sul test set di CoNLL-2003 che sul nuovo CoNLL++. La generalizzazione è stata valutata calcolando la variazione percentuale nell'F1-score.

I risultati hanno rivelato che una buona generalizzazione dipende da tre ingredienti principali: l'architettura del modello (i modelli Transformer generalizzano meglio), la dimensione del modello (i modelli più grandi tendono a generalizzare meglio) e il numero di esempi di fine-tuning (più esempi portano a una migliore generalizzazione).

Riguardo al calo delle prestazioni, lo studio ha esplorato due ipotesi: l'overfitting adattativo e il drift temporale. L'overfitting adattativo, causato dal riutilizzo dello stesso test set, non è stato osservato. Tuttavia, il drift temporale, ovvero il degrado delle prestazioni dovuto all'aumento del divario temporale tra i dati di training e di test, è emerso come la causa principale del calo di prestazioni.

In sintesi, i tagger CoNLL-2003 possono ancora funzionare bene nel 2023, a patto che vengano impiegati modelli con architetture migliori, dimensioni maggiori e che ricevano un fine-tuning con un numero adeguato di esempi. La sfida principale per la loro applicazione a dati moderni è il drift temporale, piuttosto che l'overfitting al dataset originale.</sample>
    <sample id="352">ABC-Eval è l'acronimo di "Annotating Behaviors in Chat".</sample>
    <sample id="353">Questo documento presenta un approccio interattivo alla generazione di codice Python, che mira ad affrontare il problema della sottospecificazione dell'input (natural language description - NLD). Questo problema si verifica quando le descrizioni del linguaggio naturale fornite ai modelli di generazione di codice mancano di informazioni cruciali, con conseguente codice Python non corretto.

Per risolvere questo problema, il documento introduce un framework che consente ai modelli di generazione di codice di porre domande di chiarimento (CQ) all'utente. Queste CQ aiutano a raccogliere le specifiche mancanti, migliorando così la qualità del codice generato. Il documento si concentra in particolare sulla chiarificazione delle specifiche a livello di operazione.

Gli autori propongono un metodo per creare un dataset sintetico chiamato CodeClarQA. Questo dataset è progettato per addestrare i modelli di generazione di codice a identificare le operazioni chiave mancanti nella NLD e a porre domande di chiarimento pertinenti. Il dataset contiene chiarimenti sulle operazioni chiave, sotto forma di domande sì/no o a scelta multipla.

Il documento delinea una pipeline completa per la generazione di codice guidata da CQ. Questa pipeline comprende tre moduli principali:
1.  **Predittore del fabbisogno di chiarimento:** un classificatore binario che determina se un'operazione chiave necessita di chiarimento.
2.  **Selettore di domande:** un modello di recupero che seleziona le CQ più pertinenti da porre all'utente.
3.  **Generatore di codice:** un modello di linguaggio causale o un generatore seq2seq che produce codice Python basato sulla NLD originale e sulle risposte alle CQ.

I risultati sperimentali mostrano che il metodo proposto funziona bene nell'identificazione delle operazioni chiave mancanti. I modelli di generazione di codice addestrati su dati con CQ rispondono in modo più efficace alla sottospecificazione dell'input, producendo output di codice migliori.</sample>
    <sample id="354">Secondo il grafico, la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali solo dal 2016 in poi.</sample>
    <sample id="355">00:00
Ciao, mi chiamo Vasudha e sono una studentessa di dottorato in informatica alla Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come long paper: "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge".
00:19
Cominciamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è quando due convinzioni o azioni sono incoerenti.
00:31
Ad esempio, una persona afferma: "So che le sigarette potrebbero uccidermi", e poi dice: "Ho fumato un paio di sigarette dopo la riunione di oggi". Questa convinzione e questa azione sono incoerenti e in dissonanza.
00:47
Inoltre, l'affermazione "Non credo che potrei mantenere il mio lavoro senza di loro" giustifica la seconda occorrenza e hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nelle decisioni quotidiane, è molto rara da trovare espressa nel linguaggio tra altri tipi di relazioni di discorso.
01:06
Quindi, perché la dissonanza? Studiare la dissonanza cognitiva può aiutarci a capire gli effetti del disaccordo tra le persone, a monitorare le tendenze delle convinzioni, dei valori e degli atteggiamenti nella popolazione. L'elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali.
01:43
Per raggiungere l'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio "dissonanza prima", come mostrato nel diagramma di flusso qui. I tweet sono stati analizzati utilizzando un parser PTB e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro paper. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate.
02:13
Dopo aver raccolto circa 1000 esempi di coppie di unità di discorso, abbiamo eseguito l'addestramento di un classificatore iniziale. Addestrato solo su 43 esempi di dissonanza. Non a sorpresa, il classificatore non ha ottenuto risultati molto migliori del caso. Data la bassa occorrenza della dissonanza e l'assenza di un set di dati di ricerca precedente, stiamo affrontando il problema della rarità assoluta.
02:35
Per alleviare questo, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo in modo da poter raccogliere più campioni di dissonanza con meno cicli di annotazione, riducendo i costi complessivi di annotazione e migliorando il rilevamento della dissonanza.
02:50
Poiché il modello iniziale non è stato in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati.
03:03
Abbiamo trasferito da due diversi compiti: classificazione dell'opinione indipendente dall'argomento, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo, qui chiamato "Dibattito", e sulla classificazione binaria delle classi di espansione e confronto di PTB, poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza e le chiamiamo CE qui. Abbiamo scoperto che il trasferimento, la prestazione zero-shot sul set di dati annotato è già molto migliore del caso, con un AUC di 0,62.
03:40
Inoltre, ottimizzando iterativamente su entrambi i compiti, scopriamo che l'ottimizzazione del compito CE, seguita da un'ulteriore ottimizzazione sul Dibattito, produce una prestazione zero-shot molto migliore. Questo è il modello che abbiamo usato per il cold-start dell'apprendimento attivo.
03:56
Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Il cumulativo accumula tutti i dati raccolti dall'annotazione attiva finora, mentre l'iterativo aggiorna il modello addestrando sul set più recente di dati raccolti.
04:12
Tra le diverse strategie, abbiamo scoperto che il cumulativo ha ottenuto risultati uguali o migliori dell'iterativo in tutti i casi.
04:20
Successivamente, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara, PRC, per selezionare principalmente gli esempi che sono altamente probabili essere dissonanti dal modello corrente in qualsiasi fase dell'AL. Abbiamo confrontato questo con le altre strategie AL all'avanguardia comunemente utilizzate nella comunità.
04:41
Abbiamo scoperto che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, sebbene la differenza sia piccola. Si noti che la prestazione è significativamente inferiore per la strategia random.
04:53
Nei cicli successivi di AL con le due migliori strategie, abbiamo migliorato la classificazione della dissonanza con un AUC di 0,75, che è la migliore prestazione che abbiamo ottenuto finora nel compito.
05:04
Abbiamo anche verificato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Abbiamo scoperto che il PRC ha la percentuale più alta di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori trovano anche gli esempi difficili.
05:20
In sintesi, abbiamo scoperto che il PRC è una strategia AL semplice ed efficiente per l'acquisizione di campioni rari, e il cold-start dell'AL con un compito di apprendimento per trasferimento opportunamente progettato può aiutare in modo significativo. Abbiamo anche scoperto che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-domain beneficiano dell'aggiornamento cumulativo.
05:40
Questi sono i link al nostro codice, al nostro dataset e al nostro paper. Non esitate a contattarci se avete domande.
05:48
Grazie!</sample>
    <sample id="356">L'Università di Edimburgo, la Saarland University e l'Università di Amsterdam.</sample>
    <sample id="357">La relatrice si chiama Siyu Yuan.</sample>
    <sample id="358">Cinque.</sample>
    <sample id="359">L'approccio è confrontato con l'architettura simulST dedicata CAAT.</sample>
    <sample id="361">CounterComp: Usando il contrasto controfattuale per migliorare la generalizzazione compositiva per il ragionamento quantitativo multistep. Questa presentazione, tenuta da Armineh Nourbakhsh della Carnegie Mellon University, affronta la sfida della generalizzazione compositiva nel contesto del **question answering (QA)** quantitativo multistep, in particolare nelle tabelle finanziarie. I modelli neurali attuali faticano a gestire domande che richiedono più di due passaggi di ragionamento, spesso a causa della memorizzazione di schemi spuri. La proposta, CounterComp, mira a migliorare l'attenzione dei modelli ai token di input appropriati utilizzando esempi controfattuali. La presentazione dimostra come le domande stesse possano fungere da esempi controfattuali. Ad esempio, cambiare "net change" in "percent change" nella domanda modifica l'output aggiungendo operazioni di divisione e moltiplicazione. Questo principio è utilizzato in CounterComp per minare scenari controfattuali dall'input. Viene spiegato come gli esempi controfattuali vengano utilizzati in una perdita di apprendimento metrico ausiliario con un margine dinamico. Questa perdita incoraggia il modello a distinguere tra cambiamenti nell'input che influenzano l'output (esempi negativi) e quelli che non lo fanno (esempi positivi). I risultati sperimentali mostrano che l'aggiunta di questa perdita ausiliaria migliora costantemente le prestazioni di tre baseline state-of-the-art su campioni in-distribuzione e, cosa più importante, su campioni fuori distribuzione, inclusi programmi non visti. Inoltre, l'analisi qualitativa indica che CounterComp aiuta i modelli a prestare attenzione a token più significativi nell'input durante la generazione dell'output, rafforzando così la loro capacità di generalizzazione compositiva.</sample>
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono:
* patents.google.com
* en.wikipedia.org
* en.m.wikipedia.org
* www.nytimes.com
* www.latimes.com
* www.theguardian.com
* journals.plos.org
* www.forbes.com
* www.huffpost.com
* patents.com
* www.scribd.com
* www.washingtonpost.com
* www.fool.com
* lgfs.io
* www.frontiersin.org
* www.businessinsider.com
* www.chicagotribune.com
* www.booking.com
* www.theatlantic.com
* link.springer.com
* www.kickstarter.com
* caselaw.findlaw.com
* www.ncbi.nlm.nih.gov
* www.npr.org</sample>
    <sample id="1">Università McGill, Mila e Microsoft Research.</sample>
    <sample id="2">Questo documento affronta i problemi di ordinamento della lettura nella comprensione di documenti visivamente ricchi (VRDU) utilizzando un nuovo modello multimodale di pre-addestramento chiamato LayoutMask. La motivazione principale è che i modelli esistenti soffrono di problemi di ordinamento della lettura, soprattutto nei documenti con layout complessi come ricevute e moduli.

A differenza degli approcci precedenti che utilizzano la posizione 1D globale per rappresentare l'ordine di lettura dei token, LayoutMask introduce un nuovo tipo di posizione 1D locale. Questa posizione 1D locale è specifica per i segmenti (ovvero i blocchi di testo come righe o colonne) all'interno del documento, rappresentando l'ordine dei token all'interno di quel segmento. Per catturare le relazioni tra i segmenti, LayoutMask combina questa posizione 1D locale con la posizione 2D (coordinate della casella di delimitazione) e le informazioni semantiche.

Il modello è pre-addestrato con due obiettivi principali:
1.  **Masked Language Modeling (MLM):** Un'attività standard in cui i token mascherati devono essere previsti. Per migliorare le interazioni testo-layout, vengono utilizzate due nuove strategie di mascheramento:
    *   **Whole Word Masking (WWM):** Invece di mascherare i singoli token, WWM maschera intere parole. Questo costringe il modello a usare un contesto più ampio (sia testuale che di layout) per prevedere le parole mascherate, migliorando l'interazione testo-layout.
    *   **Layout-Aware Masking (LAM):** Questa strategia assegna una maggiore probabilità di mascheramento alle prime e ultime parole di ogni segmento di testo. Questo incoraggia il modello a prestare maggiore attenzione all'inferenza dell'ordine di lettura tra segmenti, che è cruciale per la VRDU.
2.  **Masked Position Modeling (MPM):** Questo è un nuovo obiettivo di pre-addestramento introdotto in LayoutMask. È simile al cloze test, in cui le posizioni 2D (caselle di delimitazione) di parole selezionate casualmente vengono mascherate e il modello deve prevederle. Ciò costringe il modello a sfruttare le relazioni semantiche e gli indizi posizionali per inferire le posizioni mascherate, contribuendo a una migliore rappresentazione del layout.

Gli esperimenti dimostrano che l'uso della posizione 1D locale e delle strategie di mascheramento proposte migliora le prestazioni di LayoutMask sui benchmark di comprensione di documenti visivamente ricchi come FUNSD e SROIE. In particolare, l'approccio basato sul segmento locale ha superato la posizione 1D globale in FUNSD e SROIE, con una leggera flessione in CORD. LayoutMask è in grado di gestire efficacemente i layout complessi e i numeri fuorvianti, riconoscendo meglio le entità totali nei documenti rispetto ai metodi che utilizzano solo l'ordinamento di lettura convenzionale. Questo suggerisce che il modello è più adattivo a tali casi, integrando in modo più efficace gli indizi semantici e spaziali.</sample>
    <sample id="3">00:00
Ciao. Benvenuti alla nostra presentazione di Plain. Un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase. Il mio nome è Regina Stotten e vi guiderò attraverso la prima parte della presentazione.
00:18
Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la sua comprensione da parte di un gruppo target specifico, come le persone con problemi di lettura o i non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie di testi paralleli, ad esempio di documenti o di frasi. Nell'esempio qui, potete vedere una coppia di frasi allineate in parallelo di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio: sostituzione lessicale, eliminazione di clausole, riordino o inserimento di parole.
01:04
Ora proponiamo il nostro nuovo corpus, De Plain. Perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti. Per questo motivo, proponiamo il nostro nuovo corpus De Plain, che è suddiviso in due sottocorpora: De Plain APA e De Plain Web. De Plain APA si basa su testi di notizie. In De Plain APA abbiamo allineato 483 documenti, tutti manualmente. Questo si traduce in circa 13.000 coppie di frasi parallele. Per De Plain Web, questo corpus include diversi domini e abbiamo anche allineato tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi.
02:17
Abbiamo analizzato un po' più a fondo le nostre coppie di frasi, ad esempio il tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più fortemente semplificati rispetto, ad esempio, ai testi di notizie, o anche ai testi per studenti di lingue. A tutti i livelli, ad esempio per quanto riguarda la semplificazione lessicale, la semplificazione strutturale, o anche il livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus De Plain ha un'elevata varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus De Plain API abbiamo molte più riordini e aggiunte di parole rispetto al corpus De Plain Web. D'altra parte, nel corpus Web abbiamo molte più riformulazioni.
03:04
Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset De Plain. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati proposti molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli, scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi in entrambi i documenti, ma nel nostro caso d'uso stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno la stessa lingua, hanno lo stesso contenuto, ma sono a un diverso livello di complessità. E ora che abbiamo il nostro dataset De Plain, che ha frasi allineate manualmente, possiamo usare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti. E abbiamo fatto alcuni adattamenti ai metodi proposti e abbiamo pubblicato tutti questi adattamenti e il codice per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da usare per la semplificazione del testo tedesco è il metodo di Mass Align. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo mostrato nel nostro documento è il caso della semplificazione automatica del testo, attraverso l'ottimizzazione di modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo ottimizzato due modelli diversi, abbiamo ottimizzato il modello Long BART per produrre semplificazioni a livello di documento, e abbiamo anche ottimizzato il normale BART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete esaminare più in dettaglio i punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questa ottimizzazione di base potrebbe produrre, o potrebbe ottenere, punteggi migliori rispetto ai punteggi di base e proponiamo questi risultati come benchmark, un benchmark di base, per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Hanno utilizzato il modello T5 XL.</sample>
    <sample id="6">Questo lavoro unifica la riassunto multilingue (MLS) e la riassunto cross-lingue (CLS) in un'impostazione più generale, chiamata riassunto molti-a-molti (M2MS). M2MS mira a costruire un singolo modello di riassunto per elaborare un documento in qualsiasi lingua sorgente e generare il suo riassunto in qualsiasi lingua target. Abbiamo condotto studi preliminari per fornire analisi più approfondite tra MLS, CLS e M2MS. Abbiamo scoperto che M2MS potrebbe aiutare il modello a trasferire meglio le conoscenze tra diverse lingue rispetto alle precedenti MLS e CLS. Inoltre, proponiamo PISCES, un modello M2MS pre-addestrato che apprende la modellazione del linguaggio, la capacità cross-lingue e la capacità di riassunto attraverso un pre-addestramento a tre fasi. Il pre-addestramento a tre fasi comprende: (a) meta pre-addestramento, che richiede al modello di generare frasi originali basate su controparti mascherate; (b) pre-addestramento cross-lingue, che genera le frasi nella lingua target basate su frasi parallele rumorose nella lingua sorgente; e (c) pre-addestramento specifico per il compito, che utilizza campioni pseudo per pre-addestrare il modello. I risultati sperimentali mostrano che PISCES supera i precedenti forti baseline, inclusi mBART-50 e mT5. Abbiamo anche condotto studi di ablazione per verificare l'efficacia di ogni fase di pre-addestramento e studi umani per mostrare la validità di PISCES.</sample>
    <sample id="7">Sì, funzionano ancora.</sample>
    <sample id="8">La novità del metodo di valutazione umana proposto è la sua capacità di misurare la frequenza con cui i modelli di chat commettono errori tematici, come l'irrilevanza o le auto-contraddizioni, riducendo la soggettività delle valutazioni umane.</sample>
    <sample id="9">Si basa in larga misura su un set di convalida pulito.</sample>
    <sample id="10">Ci sono molti margini di miglioramento per il punteggio, specialmente quando il modello di linguaggio ha accesso solo ai nomi delle entità. Il punteggio di precisione è del 60%, ma può aumentare all'82%-87% con una conoscenza di base parzialmente sovrapposta e raggiungere il 92%-95% con la stessa conoscenza di base degli annotatori.</sample>
    <sample id="11">Questo video introduce un nuovo set di benchmark di comprensione dell'umorismo basato sul concorso di didascalie del New Yorker. Il ricercatore principale, Jack Hessel, ha spiegato come i modelli linguistici di grandi dimensioni come ChatGPT e PaLM siano stati lodati per la loro capacità di generare e persino spiegare le barzellette. Tuttavia, Hessel ha anche evidenziato le carenze di questi modelli, in particolare la loro incapacità di cogliere sfumature e giochi di parole. Per affrontare queste carenze, la sua squadra ha creato tre compiti di benchmark: abbinamento, classificazione della qualità e generazione di spiegazioni. Hanno anche raccolto un nuovo corpus di oltre 700 cartoni animati, che sono stati annotati con descrizioni delle scene, dettagli insoliti e collegamenti alle entità. Inoltre, hanno raccolto 650 spiegazioni delle barzellette generate dagli esseri umani. I risultati dei benchmark mostrano che, sebbene i modelli come CLIP (con addestramento fine) abbiano ottenuto buoni risultati nel compito di abbinamento (62,3% di accuratezza rispetto a un 20% casuale), c'è ancora un divario significativo con l'accuratezza umana del 94%. Anche con l'aiuto di descrizioni delle immagini generate dagli esseri umani, GPT-4 ha mostrato delle carenze nella spiegazione delle barzellette, con le spiegazioni generate dagli umani preferite in oltre due terzi dei casi. Questi risultati suggeriscono che i modelli linguistici di grandi dimensioni hanno ancora molta strada da fare per comprendere veramente l'umorismo.</sample>
    <sample id="12">5 autori sono coinvolti nell'articolo.</sample>
    <sample id="13">Il relatore Daniel Rotem ha presentato il suo lavoro intitolato "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings".

L'inferenza adattiva è una tecnica per ridurre il tempo di inferenza nei grandi modelli linguistici. Si basa sull'idea che i dati del mondo reale variano in complessità, quindi è possibile utilizzare modelli a bassa capacità per i campioni "facili" e in questo modo ridurre i costi medi di inferenza (tempo/denaro).

I due metodi di inferenza adattiva più comuni sono il multi-modello (MM) e l'early-exit (EE).

Il metodo MM utilizza più modelli, ciascuno addestrato separatamente, ed esegue l'inferenza in sequenza finché un classificatore non interrompe il calcolo.
Il metodo EE utilizza un singolo modello, con più classificatori posti dopo strati intermedi. Il campione viene elaborato finché un classificatore non interrompe il calcolo, risparmiando il tempo di computazione degli strati rimanenti.

Il metodo MM è più versatile (può usare modelli e dimensioni diverse) e più facile da estendere, ma è costoso in termini di archiviazione e ha un overhead di calcolo (il campione viene elaborato da tutti i modelli precedenti, anche se non vengono usati).
Il metodo EE è più veloce (nessun overhead) ed efficiente in termini di memoria, ma i parametri del modello sono condivisi tra tutti i classificatori.

È stato dimostrato che i modelli multi-modello superano i modelli early-exit del 2,3% in media. Questo suggerisce che i modelli early-exit soffrono di "gradienti contrastanti", in cui i segnali di gradiente dei diversi classificatori possono interferire tra loro, degradando le prestazioni.

Per risolvere questo problema, è stato presentato SWEET (Separating Weights in Early Exit Transformers), un nuovo metodo di fine-tuning per le architetture early-exit. Ogni strato del trasformatore riceve aggiornamenti solo dalla funzione di perdita del classificatore successivo. SWEET chiude la maggior parte del divario tra EE e MM.</sample>
    <sample id="14">Certo, ecco la traduzione del contenuto inglese in italiano:

00:00:00
Ciao, il mio nome è Adam Przepiórkowski e questa discussione riguarda la struttura di dipendenza della coordinazione.

00:00:07
Come forse saprete, ci sono diverse strutture di dipendenza assunte da diverse teorie e approcci del corpus.

00:00:15
Ad esempio, nelle dipendenze universali, la struttura di coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è il capo dell'intera struttura di coordinazione, in questo caso Lisa.

00:00:26
Un approccio simile è assunto nella teoria del significato del testo di Igor Milchuk, dove ancora una volta l'intera struttura di coordinazione è guidata dal primo congiunto.

00:00:36
Questi due approcci sono asimmetrici, evidenziano uno dei congiunti.

00:00:41
Ora, ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio congiunzione-headed assunto nei treebank di dipendenza di Praga, dove le strutture coordinate sono guidate dalla congiunzione.

00:00:54
Quindi otteniamo dipendenze da "and" a tutti i congiunti.

00:00:59
E infine, c'è anche un approccio multi-headed, usato ad esempio nella grammatica delle parole di Kates, dove, per così dire, tutti i congiunti sono i capi della struttura coordinata.

00:01:12
Quindi otteniamo dipendenze dal governatore, qui "loves", a tutti i congiunti separatamente, Lisa, Bart e Maggie.

00:01:17
Ora, lo scopo di questo documento è produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come queste due.

00:01:30
Ok. L'argomento si basa sul principio di minimizzazione della lunghezza di dipendenza, che spiegherò sulla base di questi esempi.

00:01:39
Quindi, in inglese, come forse saprete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani, giusto? Quindi "Marge read it yesterday" va bene, perché l'oggetto diretto "it" è vicino al verbo, mentre "Marge read yesterday it" è molto peggio, giusto? Perché qui, tra il verbo e l'oggetto diretto, c'è un aggiunto "yesterday".

00:01:59
Tuttavia, questo effetto può essere mitigato, quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'aggiunto.

00:02:12
Questo è illustrato qui. Quindi entrambe queste frasi vanno bene: "Marge read this absolutely fascinating book about the bees yesterday" va bene, dove invece di "it" abbiamo questa lunga sintagma nominale. Ma va bene anche dire "Marge read yesterday this absolutely fascinating book about bees".

00:02:30
Quindi il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, essa soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più brevi sono preferite.

00:02:51
Quindi, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture.

00:03:03
Quindi qui abbiamo una dipendenza da "read" all'aggiunto di lunghezza 7, misurata in parole, e da "read" a "book" di lunghezza 4. Quindi insieme fa 11.

00:03:14
Quando sposti, quando scambi questi due costituenti, la somma di queste due dipendenze diventa 6, giusto? Quindi invece di 11, 6, molto più corto, ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro.

00:03:30
Ok. Quindi, abbiamo estratto varie statistiche sulla coordinazione da una versione migliorata del Penn Treebank, e nel documento si spiega perché non abbiamo usato le dipendenze universali.

00:03:43
E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono ad essere più corti, quindi "salt and pepper" e non "pepper and salt", misurati in sillabe. E anche l'osservazione che è stata fatta di sfuggita, che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo più forte.

00:04:14
Quindi la proporzione è maggiore per i congiunti corti di sinistra.

00:04:17
Tuttavia, ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto? Quindi il governatore è a sinistra in questo esempio: "I saw Bart and Lisa", quindi il governatore è a sinistra.

00:04:33
È assente nel secondo esempio: "Homer came and sneezed", qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi in questi casi il congiunto di sinistra preferisce essere più corto, tanto più quanto maggiore è la differenza tra i due congiunti.

00:04:56
Tuttavia, quando il governatore è a destra, come qui, "laughed" governa la coordinazione "Ted and Ned", questo effetto scompare.

00:05:01
Quindi, mostriamo che misurando la lunghezza in caratteri, questa è la prima colonna, in sillabe, la colonna centrale, e in parole, la colonna di destra. Quindi mi concentrerò su quella di destra.

00:05:10
Quello che vediamo qui è che, quando il governatore è a sinistra, la tendenza del congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole, e lo stesso si osserva quando non c'è governatore, come nella coordinazione di frasi, ma quando il governatore è a destra, questa tendenza scompare.

00:05:27
E nel documento mostriamo come questo fornisce un argomento contro le strutture asimmetriche di coordinazione, come queste due, e per le strutture simmetriche, come queste due. Quindi, vedi il documento per l'argomento completo, argomento, scusa, e parla con noi alla sessione poster. Grazie.</sample>
    <sample id="15">Tre autori sono coinvolti.</sample>
    <sample id="16">Dall'analisi delle coppie di frasi, emerge che i testi biblici sono stati semplificati in modo molto più significativo rispetto ai testi di notizie o a quelli destinati agli studenti di lingue.</sample>
    <sample id="17">This presentation introduces a novel approach to Multimodal Relation Extraction (MRE) called "Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling." The speaker, Shengqiong Wu, highlights that MRE is crucial for social media scenarios where data is mixed-modal and text lengths are often short. They identify two key problems: internal-information over-utilization (too much irrelevant information) and external-information under-exploitation (lack of sufficient context).

To address these, the proposed framework incorporates two main ideas:
1. **GIB-guided Feature Refinement**: This addresses internal-information over-utilization by fine-grained pruning of input image and text features. It involves filtering task-irrelevant nodes and adjusting edges in the cross-modal graph (CMG), guided by a graph information bottleneck-like principle.
2. **Multimodal Topic Integration**: This tackles external-information under-exploitation by enriching CMG features with additional semantic contexts via multimodal topic features. It retrieves textual and visual topic keywords and integrates their embeddings through an attention mechanism.

Experiments conducted on a Twitter dataset demonstrate that the proposed model achieves superior performance compared to text-based and existing multimodal baselines. Ablation studies confirm the effectiveness of both information screening and external-information exploiting components. Further analysis shows that GIB-guided Feature Refinement is more beneficial for inputs with higher text-vision relevance, while Multimodal Topic Integration is more useful for inputs with lower text-vision relevance, compensating for information deficiency. The overall system significantly improves MRE by simultaneously subtracting redundant information and adding supplementary semantic context.</sample>
    <sample id="18">L'esempio di preferenza per i congiunti a sinistra più brevi è "sale e pepe" invece di "pepe e sale".</sample>
    <sample id="19">La relatrice, Shangsi Chen dell'Università di Shenzhen, presenta un sondaggio sull'efficienza della risposta a domande a dominio aperto (ODQA). Spiega che la maggior parte dei sistemi ODQA utilizza un framework a due fasi: un *retriever* (recuperatore) e un *reader* (lettore). Il *retriever* recupera i contesti di evidenza da un corpus (ad esempio, Wikipedia), mentre il *reader* estrae la risposta basandosi sulla domanda e sui contesti recuperati.

Shangsi Chen sottolinea le sfide che i sistemi ODQA devono affrontare. Il corpus di Wikipedia è enorme, composto da 26 milioni di documenti e occupa 13 GB. L'indicizzazione di questo corpus da parte dell'*encoder* di documenti genera un file di indice di 65 GB. Questo rende la ricerca di evidenze e l'inferenza della risposta processi lunghi e costosi in termini di memoria. I modelli linguistici di grandi dimensioni con milioni di parametri aggiungono ulteriori sfide, limitando l'implementazione in applicazioni in tempo reale e su dispositivi con risorse limitate.

Per affrontare queste sfide, la relatrice presenta diverse tecniche di ottimizzazione. Per la ricerca di evidenze più veloce, suggerisce l'uso della ricerca approssimata del vicino più prossimo (ANN), con metodi come il file invertito (IVF), l'hashing sensibile alla località (LSH) e i grafici a piccolo mondo navigabili gerarchici (HNSW). Per velocizzare la lettura, propone lo "skip reading", ad esempio con la *adaptive computation* (AC), che interrompe la lettura dei contesti meno rilevanti. Per ridurre la dimensione dell'indice, consiglia il filtraggio dei documenti e la compressione delle *embedding* tramite la quantizzazione del prodotto. Per ridurre la dimensione del modello, suggerisce modelli leggeri come MobileBERT, la condivisione dei parametri come in ALBERT e l'utilizzo di meno modelli, ad esempio un singolo modello per recupero e lettura.

Analizzando i sistemi ODQA esistenti, Shangsi Chen osserva che i sistemi *retriever-reader* bilanciano bene velocità, memoria e prestazioni. I sistemi *retriever-only* (solo recuperatore) utilizzano indici di grandi dimensioni ma offrono un'inferenza rapida. I sistemi *generator-only* (solo generatore) non richiedono un indice, ma impiegano modelli di grandi dimensioni e mostrano prestazioni inferiori. In conclusione, se le risorse sono limitate, si può considerare la riduzione delle dimensioni dell'indice tramite sistemi *generator-only* o la compressione delle *embedding*, oppure la riduzione delle dimensioni del modello tramite *knowledge distillation* (distillazione della conoscenza) o un modello a stadio singolo. Per un feedback in tempo reale, i sistemi *retriever-only* sono una buona scelta, mentre per un compromesso tra prestazioni, memoria e velocità, i sistemi *retriever-reader* sono i più appropriati.</sample>
    <sample id="20">Sì, i modelli DrBERT e il dataset NACHOS, insieme agli script di addestramento, sono disponibili gratuitamente con licenza MIT.</sample>
    <sample id="21">Il DEplain-APA contiene testo di notizie.</sample>
    <sample id="22">Per una buona generalizzazione, sono necessari i seguenti fattori:
- Una migliore architettura del modello
- Una dimensione maggiore del modello
- Più esempi di fine-tuning</sample>
    <sample id="23">Questa presentazione discute il problema dei modelli text-to-image che faticano a generare testo accurato. Sebbene i recenti progressi abbiano migliorato la qualità complessiva delle immagini, le parole generate sono spesso illeggibili o errate.

Il problema risiede principalmente nel "text encoder", che converte il testo in un formato comprensibile al modello di generazione di immagini. I text encoder, come T5, utilizzano la "tokenizzazione SentencePiece", che suddivide le parole in sottoparole invece che in singoli caratteri. Ciò significa che il modello non riceve informazioni dettagliate sulle lettere che compongono una parola, rendendo difficile la corretta riproduzione dell'ortografia.

Gli esperimenti dimostrano che la capacità di ortografia dei modelli T5 migliora con l'aumentare delle loro dimensioni, ma anche il modello più grande (T5-XXL) raggiunge solo il 70% di accuratezza nell'ortografia. Questo perché la rappresentazione delle parole è a livello di sottoparola e i modelli devono decodificare implicitamente l'ortografia. Si osserva anche che T5 ha difficoltà a scrivere le parole più frequenti, presumibilmente perché queste sono spesso rappresentate come un unico token, il che riduce la visibilità dei singoli caratteri.

In contrasto, i modelli "character-aware", come ByT5, che elaborano l'input a un livello più granulare (byte o caratteri), mostrano un'accuratezza di ortografia quasi perfetta a tutte le scale e frequenze. Ciò suggerisce che fornire informazioni a livello di carattere al text encoder è cruciale per una generazione di testo accurata.

Per migliorare la resa del testo nei modelli text-to-image esistenti, il relatore propone di aumentare il text encoder esistente (ad esempio, T5-XXL) concatenando la sua rappresentazione con quella di un modello character-aware più piccolo (ad esempio, ByT5-small). Questa "concatenazione" aggiunge solo un aumento marginale dei parametri (circa il 5%), ma offre al modello di diffusione la capacità di accedere alle informazioni a livello di carattere. Questo approccio migliora significativamente le metriche di generazione dell'immagine, inclusa la fedeltà, l'allineamento e la qualità del testo, senza aumentare eccessivamente la complessità del modello.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base al numero di caratteri, sillabe e parole.</sample>
    <sample id="25">Gli esperimenti sono stati progettati per studiare l'effetto della posizione del governatore attraverso l'estrazione di statistiche sulla coordinazione da una versione migliorata del Penn Treebank. Queste statistiche hanno esaminato la tendenza dei congiunti di sinistra ad essere più corti e come questa tendenza sia influenzata dalla posizione del governatore (a sinistra, a destra o assente).</sample>
    <sample id="26">Se addestrato su dati non bilanciati, il classificatore di base non è molto più efficace del caso, dato che le occorrenze di dissonanza sono rare.</sample>
    <sample id="27">Nell'articolo sono coinvolti quattro autori.</sample>
    <sample id="28">I personaggi presi a esempio sono Bob e Alice.</sample>
    <sample id="29">Sulla formalità e sulla coesione lessicale.</sample>
    <sample id="30">"LLM-BLENDER: Ensembling LLMs with Pairwise Ranking &amp; Generative Fusion" introduce un framework a due fasi per migliorare le prestazioni dei modelli linguistici di grandi dimensioni (LLM) attraverso l'apprendimento d'insieme. La presentazione sottolinea che nessun singolo LLM è il migliore per tutte le attività; le prestazioni ottimali variano significativamente tra gli esempi di input.

Il framework LLM-Blender inizia eseguendo N LLM diversi in parallelo per un dato input (X), generando N candidati output (Y1-YN). La fase successiva impiega un modulo PairRanker, basato su un modello a cross-attention come RoBERTa, per confrontare a coppie gli output candidati con l'input originale. Questa fase valuta quale candidato è migliore per X e genera una matrice di confronto, che viene poi aggregata per produrre una classifica finale. I metodi di aggregazione includono "Max Logits", "Max Wins" e "Bubble Sort", con "Max Logits" che si dimostra il più efficace.

Nella fase finale, un modulo GenFuser seleziona i primi K (ad esempio, tre) candidati classificati dal PairRanker. Questi vengono concatenati con l'input originale e inseriti nel GenFuser, un modello sequence-to-sequence che fonde i migliori candidati per generare un output finale migliorato. Per valutare questo framework, gli autori hanno creato MixInstruct, un nuovo dataset di 110.000 esempi di istruzione-seguito derivati da varie fonti, e hanno utilizzato 11 LLM open-source. Le metriche di valutazione includono BERTScore, BARTScore, BLEURT e GPT-Rank.

I risultati empirici mostrano che LLM-Blender supera costantemente i singoli LLM e i metodi di ranking precedenti. In particolare, il framework batte Open Assistant e Vicuna in una percentuale significativa di esempi. In conclusione, LLM-Blender è un framework semplice ma efficace che migliora notevolmente le prestazioni complessive degli LLM esistenti, e il dataset MixInstruct e la codebase unificata sono stati rilasciati per facilitare ulteriori ricerche.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati a Johns Hopkins University, Purdue University, MIT e Meta AI.</sample>
    <sample id="33">Il framework quantifica la posizionalità confrontando le annotazioni di utenti diversi con le etichette dei dataset esistenti e le previsioni dei modelli tramite i punteggi di correlazione di Pearson, raggruppati per demografia.</sample>
    <sample id="34">In questo video, Marcos Treviso presenta il lavoro "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", un risultato della sua collaborazione con Alexis Ross, Nuno Guerreiro e André Martins.
Treviso sottolinea come i metodi attuali per spiegare le decisioni dei classificatori NLP, come la razionalizzazione selettiva e la generazione controfattuale, abbiano ciascuno punti di forza e di debolezza complementari. La razionalizzazione selettiva evidenzia le parti importanti di un input, mentre la generazione controfattuale modifica l'input per cambiare la decisione del classificatore, più in linea con il ragionamento causale umano.

CREST combina questi approcci. Il componente di generazione controfattuale di CREST inizia con un input e utilizza un "Masker addestrabile" per produrre una razionalizzazione. Questa razionalizzazione viene poi utilizzata per mascherare l'input originale, aggiungendo l'etichetta obiettivo desiderata, e l'input mascherato viene passato a un "Editor" (un modello linguistico mascherato) che riempie gli spazi vuoti, generando un controfattuale.

Per valutare la qualità dei controfattuali prodotti, Treviso presenta un esperimento di valutazione umana. Agli utenti è stato chiesto di valutare 100 esempi di controfattuali generati manualmente, da CREST e da MICE (un lavoro correlato) in base alla loro validità e naturalezza, utilizzando una scala Likert a 5 punti. Come previsto, i controfattuali manuali hanno ottenuto un punteggio superiore. Tuttavia, tra gli approcci automatici, CREST ha ottenuto un punteggio significativamente più alto in entrambe le categorie rispetto a MICE.

Questi controfattuali di alta qualità possono essere utilizzati per l'aumento dei dati o per la razionalizzazione. Treviso presenta CREST-Rationalization, che sfrutta la struttura accoppiata degli esempi fattuali e controfattuali. Il sistema utilizza un razionalizzatore condiviso che evidenzia i razionali significativi. I razionali risultanti sono alimentati a un "Predictor" che produce una decisione finale. Per incoraggiare i nuovi razionali a concentrarsi sulle parti specifiche dell'input che codificano i ragionamenti fattuali e controfattuali, viene aggiunta una nuova regolarizzazione.

Gli esperimenti sui dataset IMDB e SNLI mostrano che CREST-Rationalization ottiene la massima accuratezza sui dataset in-domain e out-of-domain. Una "Analisi di interpretabilità" su plausibilità, simulabilità diretta e una nuova metrica proposta (simulabilità controfattuale) mostra che i razionali di CREST-Rationalization sono più plausibili e ottengono una simulabilità controfattuale significativamente più elevata.

In sintesi, CREST unisce i punti di forza della razionalizzazione selettiva e della generazione controfattuale, producendo controfattuali validi, fluidi e diversi, controllando la quantità di perturbazione e portando a spiegazioni plausibili con un'elevata simulabilità controfattuale.</sample>
    <sample id="36">Il relatore introduce il **Multilingual Machine Translation (MMT)**, che offre diversi vantaggi, tra cui scalabilità, velocità e riduzione degli errori a cascata, oltre a miglioramenti per le lingue con poche risorse. Tuttavia, il MMT presenta anche delle sfide, principalmente una capacità limitata per lingua. Per ovviare a ciò, il team del relatore ha sviluppato le **Language-Specific Layers (LSLs)**.

L'idea chiave è avere un layer del trasformatore regolare per ogni lingua, in cui i sub-layer sono selezionati in base alla lingua di origine o di destinazione. Durante l'inferenza, vengono utilizzati solo i pesi del sub-layer della lingua selezionata, mantenendo così i costi di inferenza costanti.

Per determinare il posizionamento ottimale degli LSL, il team ha utilizzato un approccio che consente al modello di apprendere la migliore collocazione. Hanno addestrato un modello ampio con tre pesi per ogni layer (condiviso, di origine e di destinazione) e poi hanno analizzato l'importanza relativa di ciascun peso per scegliere l'architettura finale. È emerso che i pesi della lingua di origine sono più importanti nei layer inferiori dell'encoder, mentre quelli della lingua di destinazione sono più importanti nei layer superiori.

Negli esperimenti condotti su dati WMT21 news translation per 10 lingue, l'architettura LSL-NAS, appresa attraverso l'approccio descritto, ha mostrato miglioramenti significativi rispetto alle baseline e agli adattatori linguistici, pur mantenendo meno parametri durante l'inferenza. Questi miglioramenti sono stati evidenti per tutte le lingue, con particolare risalto per quelle a basse risorse, e sono risultati statisticamente significativi in 84 delle 90 direzioni di traduzione.</sample>
    <sample id="37">Lo studio precedente ha rivelato stereotipi razziali nei soggetti umani a cui sono stati dati gli stessi prompt di persona.</sample>
    <sample id="38">Lo studio ha utilizzato un database migliorato chiamato Penn Treebank per estrarre statistiche sul coordinamento.</sample>
    <sample id="39">Ci sono due autori nell'articolo.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva includono:

* Analisi degli effetti del disaccordo tra le persone
* Monitoraggio delle tendenze di credenze, valori e atteggiamenti
* Studio dei disturbi d'ansia
* Comprensione dell'estremismo e della polarizzazione
* Analisi degli stili cognitivi individuali per migliorare la comprensione dei processi decisionali.</sample>
    <sample id="41">Il presentatore introduce il lavoro su "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives", una collaborazione con Sony Group Corporation. Il lavoro sottolinea l'importanza di comprendere le personalità degli oratori, degli ascoltatori o dei personaggi per sostenere narrazioni coerenti e coinvolgenti. Questo studio ha affrontato la sfida della rappresentazione di persone reali e delle loro ricche conoscenze del mondo, comprese le interazioni complesse.

Il team ha proposto PeaCoK, un grafo di conoscenza del buon senso incentrato sulla persona, per rappresentare la conoscenza della persona a livello mondiale su larga scala. PeaCoK contiene circa 100.000 fatti sulla persona, 3.800 persone e 40.000 attributi distintivi. Questi formano circa 100.000 inferenze o fatti sulla persona. Il grafo è stato costruito in tre fasi: selezione della persona, induzione di attributi potenziali e classificazione della relazione. Il processo ha combinato gli esistenti grafici di conoscenza del senso comune con modelli linguistici pre-addestrati e ha impiegato un meccanismo di voto a maggioranza con IA-umano per la classificazione delle relazioni, che ha prodotto annotazioni accurate con un'accuratezza media di circa l'87%.

Per valutare la capacità di PeaCoK di generalizzare la conoscenza, è stato addestrato un generatore di conoscenza di Comet-BART. I risultati hanno mostrato che Comet-BART ha superato GPT-3 e GPT-3.5 in metriche automatiche e valutazioni umane, indicando che PeaCoK può essere una base affidabile per l'apprendimento e la generalizzazione della conoscenza delle persone da parte di modelli linguistici leggeri.

Infine, il team ha esplorato il potenziale di PeaCoK per migliorare la modellazione narrativa a valle, in particolare nei sistemi di dialogo. Integrando PeaCoK in un sistema di dialogo P2Bot, hanno osservato un miglioramento significativo nella fluidità, coerenza, coinvolgimento ed espressione della persona, soprattutto quando gli interlocutori condividevano più attributi comuni. Ciò ha evidenziato il ruolo cruciale della conoscenza interconnessa delle persone a livello mondiale di PeaCoK nel migliorare le conversazioni.</sample>
    <sample id="42">Nell'articolo sono coinvolti due autori.</sample>
    <sample id="43">Sette.</sample>
    <sample id="44">Il framework si differenzia dai lavori precedenti confrontando le annotazioni degli utenti finali con le previsioni e le etichette di modelli e set di dati, invece di limitarsi a esaminare l'accordo tra gli annotatori o le distribuzioni di modelli/annotatori.</sample>
    <sample id="45">La configurazione GPT-3.5 P_Black si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="46">Il confronto è stato fatto tra DeepL e Google Translate.</sample>
    <sample id="47">00:00</sample>
    <sample id="48">Nell'articolo sono coinvolti 7 autori.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 900 token di lunghezza del contesto.</sample>
    <sample id="50">Il video presenta il corpus DEPLAIN, un nuovo dataset parallelo tedesco per la semplificazione di testi. La semplificazione testuale è il processo di adattamento di un testo per migliorarne la comprensione da parte di un pubblico specifico, come persone con difficoltà di lettura o parlanti non nativi. Il corpus DEPLAIN è stato sviluppato per superare le limitazioni dei corpora esistenti, spesso troppo piccoli o con allineamenti automatici meno precisi.
DEPLAIN è suddiviso in due sottocorpora: DEPLAIN-APA, basato su testi di notizie allineati manualmente (483 documenti, circa 13.000 coppie di frasi), e DEPLAIN-web, contenente testi di diversi domini, allineati sia manualmente che automaticamente (756 documenti, circa 34.000 coppie di frasi). L'analisi del corpus ha rivelato che i testi biblici sono semplificati in modo più significativo rispetto ai testi di notizie o a quelli per studenti di lingue, su tutti i livelli di semplificazione (lessicale, strutturale e generale). DEPLAIN-APA mostra più riordini e aggiunte di parole, mentre DEPLAIN-web presenta più riformulazioni.

Il corpus DEPLAIN può essere utilizzato per diversi casi d'uso:
1.  **Valutazione dell'allineamento automatico:** Poiché DEPLAIN include allineamenti manuali, può fungere da gold standard per valutare l'accuratezza dei metodi di allineamento automatico delle frasi. Sono state apportate modifiche ai metodi proposti per adattarli alla semplificazione testuale, e i risultati mostrano che MASAlign è il metodo più efficace per l'allineamento di testi semplificati in tedesco.
2.  **Semplificazione automatica del testo:** Il corpus consente di affinare i modelli linguistici per produrre testi semplificati. I ricercatori hanno affinato modelli come long-mBART per la semplificazione a livello di documento e mBART per la semplificazione a livello di frase. I risultati ottenuti con DEPLAIN hanno superato i punteggi di riferimento, fornendo un nuovo benchmark per la semplificazione automatica del testo in futuro.

Tutti gli adattamenti, il codice e i checkpoint dei modelli sono disponibili nel documento e nella presentazione per ulteriori analisi e sperimentazioni.</sample>
    <sample id="51">Nel loro set di dati sono stati inclusi tre domini: musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è definita come "le prospettive che le persone hanno come risultato dei loro dati demografici, identità ed esperienze di vita".</sample>
    <sample id="53">Il nome della relatrice o del relatore è Dawei Zhu.</sample>
    <sample id="54">Ciao, mi chiamo Vasudha e sono una studentessa di dottorato in informatica alla Stony Brook University. Vorrei presentare il nostro lavoro, accettato come long paper all'ACL 2023, intitolato "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge".
La dissonanza cognitiva si riferisce a due elementi della cognizione (pensieri, azioni, credenze) che sono incoerenti. Ad esempio, una persona che dice "So che le sigarette potrebbero uccidermi", e poi "Oggi, dopo la riunione, ho fumato un paio di sigarette" manifesta dissonanza. Se poi aggiunge "Non credo che potrei mantenere il mio lavoro senza di loro", giustifica l'azione.

Sebbene la dissonanza sia un fenomeno comune nella presa di decisioni quotidiana, è relativamente rara nel linguaggio rispetto ad altre relazioni discorsive. Abbiamo condotto un'ampia annotazione di relazioni di dissonanza su dati di Twitter, utilizzando un approccio "dissonanza-prima". Abbiamo scoperto che la dissonanza è stata trovata solo nel 3,5% delle coppie annotate, mostrando la sua rarità.

Per affrontare questa rarità, abbiamo sperimentato una combinazione di transfer learning e active learning per l'annotazione, in modo da raccogliere più campioni di dissonanza con meno cicli di annotazione, riducendo i costi e migliorando il rilevamento della dissonanza.
Abbiamo iniziato il processo di active learning trasferendo pesi da compiti correlati, in particolare la classificazione dello "stance" di dissonanza indipendente dal tema e la classificazione binaria delle classi di espansione e confronto del Penn Discourse Treebank. Abbiamo osservato che il trasferimento da questi compiti migliorava già significativamente le prestazioni a zero-shot. Successivamente, abbiamo confrontato gli approcci di aggiornamento cumulativo e iterativo per il modello e abbiamo scoperto che l'aggiornamento cumulativo era migliore in tutti gli scenari.

Infine, abbiamo introdotto una strategia di acquisizione "probabilità di classe rara" (PRC) per l'active learning. Questa strategia seleziona esempi che sono altamente probabili essere dissonanti secondo il modello attuale. La strategia PRC supera le altre strategie di active learning, inclusi l'acquisizione casuale, l'entropia, il CoreSet e il CAL. Sebbene il miglioramento sia modesto, la strategia PRC ha anche un costo di annotazione inferiore per gli annotatori, pur raccogliendo la più alta percentuale di esempi di dissonanza.</sample>
    <sample id="55">Sì, EDAtt si basa su un modello ST offline esistente.</sample>
    <sample id="56">Ci sono 4 autori coinvolti nell'articolo.</sample>
    <sample id="57">Sì, i modelli testati funzionano sulla suite di test quando sono addestrati con l'addestramento specifico del compito.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:
1. **Background-Pretrain**: Il modello sfrutta la conoscenza pre-appresa durante l'addestramento e la conoscenza specifica dell'entità fornita al momento dell'inferenza.
2. **Background-Both**: Sia la conoscenza pre-appresa che quella specifica dell'entità vengono fornite esplicitamente nel contesto al momento dell'inferenza.
3. **Background-Inference**: Entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza, il che rende difficile per i modelli integrarli in modo affidabile.</sample>
    <sample id="59">Yanis Labrak ha presentato il lavoro su DrBERT, un modello robusto pre-addestrato in francese per i domini biomedico e clinico. Il modello è basato su RoBERTa ed è stato addestrato su NACHOS, un set di dati medico crawlato dal web. Labrak ha anche introdotto un confronto di modelli con diverse impostazioni di pre-addestramento e fonti di dati.
Lo studio mira a rispondere a domande sull'idoneità delle fonti di dati per un'ampia gamma di usi e sull'efficacia dei dati crawlati rispetto a quelli clinici. Sono stati confrontati i modelli DrBERT e ChuBERT, quest'ultimo basato su dati anonimi dell'Università Ospedaliera di Nantes. L'analisi si è concentrata sulla quantità di dati necessari per addestrare un modello specializzato in francese, considerando set di dati da 4GB a 8GB.
I modelli sono stati confrontati su 11 task biomedici e clinici a valle, inclusi riconoscimento di entità nominate, classificazione, part-of-speech tagging e question answering. I risultati hanno rivelato che i modelli DrBERT hanno superato i modelli generici CamemBERT in nove task su undici. I modelli CamemBERT, addestrati con un pre-addestramento continuo, hanno mostrato una maggiore variabilità tra le diverse esecuzioni. I dati eterogenei sono risultati più versatili, e una maggiore quantità di dati di pre-addestramento ha generalmente portato a prestazioni migliori, sebbene questo non si sia tradotto in una scalabilità illimitata.
In sintesi, i modelli DrBERT offrono prestazioni all'avanguardia in francese per i task medico-orientati. Il pre-addestramento su dati eterogenei come NACHOS è cruciale, essendo più robusto rispetto all'utilizzo esclusivo di dati clinici privati. Inoltre, il pre-addestramento continuo è efficace quando basato su modelli specifici del dominio inglese. I modelli DrBERT, il set di dati NACHOS e gli script di addestramento sono disponibili gratuitamente con licenza MIT.</sample>
    <sample id="60">Gli autori dell'articolo sono affiliati con Google Research.</sample>
    <sample id="61">L'ultima domanda di ricerca è: "Come utilizzare in modo più efficiente i campioni puliti disponibili?".</sample>
    <sample id="62">Il video introduce un articolo ACL 2023 intitolato "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training". Il relatore, Nitay Calderon, illustra il problema principale: i sistemi di Natural Language Generation (NLG) basati su Large Language Models (LLM) sono diventati massivi in termini di requisiti computazionali, di storage e finanziari. C'è una crescente domanda nell'industria per comprimere questi modelli preservandone le prestazioni. L'obiettivo dello studio è esplorare il potenziale della compressione NLG.

Per raggiungere questo obiettivo, lo studio si concentra sulla compressione del modello tramite pruning (rimozione di parametri meno informativi) e Knowledge Distillation (KD). La KD prevede il trasferimento di conoscenze da un modello "insegnante" grande a un modello "studente" più piccolo, addestrando lo studente a mimare l'insegnante. Esistono due tipi principali di KD in NLG: KD a livello di parola (logits KD), che minimizza la divergenza KL tra i logit dello studente e dell'insegnante, e KD a livello di sequenza, che addestra lo studente su pseudo-target (PT) generati dall'insegnante.

Lo studio si distingue dai lavori precedenti che si sono concentrati su attività NLU, KD agnostica al compito (pre-addestramento) o un singolo compito di generazione con grandi dataset etichettati. Invece, questo studio conduce uno studio sistematico della KD specifica per il compito in NLG, considerando una varietà di compiti NLG in contesti realistici.

I contesti realistici sono definiti da cinque criteri: (1) un dataset etichettato con risorse medie (diverse migliaia), (2) abbondanti dati non etichettati, (3) LLM off-the-shelf di dimensioni medio-piccole e ottimizzati, (4) l'efficienza del tempo di inferenza come obiettivo principale (alta compressione), e (5) risorse computazionali di addestramento una tantum trascurabili.

Lo studio include quattro compiti NLG: riassunto (XSUM), generazione di domande (SQuAD), ragionamento di buon senso (ART) e semplificazione/trasferimento di stile (Shakespeare). Tutti i dataset hanno un rapporto 1:4 di esempi etichettati rispetto a quelli non etichettati.

Il documento riassume lo studio in otto fasi: (S1) Architettura, (S2) Pruning, (S3) Obiettivo, (S4) PTs, (S5) Dati non etichettati, (S6) Numero di PTs, (S7) Decodifica, (S8) Joint-Teaching.
Le prime due fasi esplorano le decisioni architettoniche (encoder-decoder vs. solo-decoder) e l'impatto del pruning sulle prestazioni. Le fasi successive esplorano estensioni dell'uso degli pseudo-target, inclusa l'importanza dei dati non etichettati, la generazione di più PT tramite campionamento (invece della ricerca a raggio) e l'introduzione di una nuova tecnica di KD chiamata "Joint-Teaching".

Joint-Teaching affronta il bias di esposizione dello studente, mette a terra l'apprendimento e insegna allo studente a correggere i propri errori, applicando la KD a livello di parola su pseudo-target generati sia dall'insegnante che dallo studente.

Per maggiori dettagli, lo spettatore è invitato a leggere l'articolo completo o a scansionare il codice QR nella prima slide.</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre risultati coerenti per lo stesso compito, indipendentemente dalle piccole variazioni nella formulazione delle istruzioni.</sample>
    <sample id="64">Il nome della relatrice è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità suggerisce che la performance del modello non è buona, il che significa che il modello non produce risultati consistenti per lo stesso compito.</sample>
    <sample id="66">Il **ragionamento matematico** è un aspetto fondamentale dell'intelligenza umana, che ci permette di comprendere e prendere decisioni basate su dati numerici e linguaggio. Lo sviluppo di macchine capaci di risolvere problemi matematici e dimostrare teoremi è stato a lungo un obiettivo dell'intelligenza artificiale e del **Natural Language Processing (NLP)**. Recentemente, si è registrato un forte interesse per le applicazioni dell'**apprendimento profondo** in questo campo.

Il documento esamina sia i compiti di ragionamento matematico che i metodi di apprendimento profondo sviluppati per affrontarli. Vengono illustrate varie tipologie di problemi, come i **problemi matematici a parole**, che richiedono operazioni aritmetiche, e i **problemi geometrici**, che necessitano di un ragionamento neuro-simbolico su diagrammi, teoremi e risolutori. Viene anche analizzato l'**automated theorem proving**, dove un sistema AI dimostra la veridicità di un teorema attraverso una sequenza di argomenti logici.

Vengono presentati diversi modelli di reti neurali, tra cui i **Seq2Seq Neural Networks** che mappano sequenze di input a sequenze di output, e i **Tree-based Neural Networks** che modellano la struttura ad albero delle espressioni matematiche. Le **Large Language Models (LLMs)**, come GPT-3, hanno mostrato notevoli capacità, specialmente con il **Chain-of-Thought (CoT) Prompting**, che le guida attraverso passaggi intermedi di ragionamento. Tuttavia, le LLMs mostrano ancora **limitazioni nella precisione e consistenza** con numeri grandi e necessitano di strategie come la **Self-Consistency** o l'aumento con strumenti esterni per migliorare le prestazioni. Il documento sottolinea anche la necessità di ulteriori ricerche in **contesti a basse risorse** e sulla **generalizzazione e robustezza** dei modelli.</sample>
    <sample id="67">Questa presentazione esplora le cause e le cure per l'interferenza nelle traduzioni multilingue.
I modelli di traduzione automatica multilingue possono beneficiare di sinergia tra coppie linguistiche o soffrire di interferenza. Ad esempio, la traduzione dall'inglese al finlandese può migliorare la qualità dell'inglese estone, mentre dall'inglese al cinese può avere un effetto negativo.

Sono stati proposti molti metodi per alleviare l'interferenza, spesso usando modelli piccoli, ma non sempre funzionano meglio di un riferimento ottimizzato. Quando si verifica l'interferenza e sono necessari algoritmi specializzati per mitigarla?

Questo lavoro identifica i principali fattori che contribuiscono all'interferenza e alla sinergia. La grave interferenza si verifica quando il modello è molto piccolo rispetto alla dimensione dei dati. L'ottimizzazione della temperatura di campionamento è fondamentale per ottenere prestazioni elevate.

Per il caso bilingue, ci sono leggi di scaling per le dimensioni del modello e dei dati che prevedono con successo la perdita. Ma nel caso multilingue ci sono altri fattori, come la dimensione dei dati di altre lingue, la somiglianza linguistica e il numero totale di lingue. Tuttavia, la somiglianza linguistica e il numero di lingue non hanno un grande impatto.

Per quantificare l'interferenza, viene definita come la differenza relativa tra la perdita di un modello bilingue e quella di un modello multilingue. Se la perdita bilingue è inferiore, il valore è negativo.

Abbiamo utilizzato quattro varianti di architettura Transformer, dalla XS (11M parametri) alla L (704M parametri). Le configurazioni "base" e "big" del Transformer sono spesso utilizzate nella letteratura sulla traduzione multilingue.

Sono state utilizzate 15 lingue provenienti da WMT, con un numero di coppie di frasi che vanno da oltre 50 milioni a circa 150.000.

Per valutare se la somiglianza linguistica ha un impatto significativo sui livelli di interferenza, abbiamo considerato una coppia linguistica di riferimento (ad es., inglese-spagnolo). Abbiamo quindi addestrato modelli trilingue per tradurre en → es e en → francese o en → russo. Successivamente, abbiamo misurato l'interferenza/sinergia per en → es tra le diverse lingue.

Abbiamo scoperto che la somiglianza linguistica non è un fattore dominante per l'interferenza.

L'interferenza si verifica in impostazioni di scarsità di parametri, ovvero quando la dimensione del modello è molto piccola rispetto alla dimensione dei dati. Con una quantità sufficiente di dati, l'interferenza diminuisce.

La soluzione più semplice per controllare i compromessi è il campionamento della temperatura. Il valore più comune è 5, spesso senza calibrazione.

L'ottimizzazione della temperatura di campionamento è fondamentale per ottenere linee di base robuste, specialmente per i modelli più grandi.

In conclusione, i fattori dominanti di interferenza/sinergia sono la dimensione del modello, la dimensione dei dati e la dimensione dei dati di altre lingue. La somiglianza linguistica e il numero di lingue influiscono molto meno. Una scala modesta e una temperatura ottimizzata possono ridurre significativamente il problema senza l'uso di altri metodi specializzati.</sample>
    <sample id="68">Durante il pre-addestramento, i modelli ricevono un contesto linguistico misto: alcune frasi sono accettabili con strutture grammaticali specifiche e altre sono inaccettabili, ma con la stessa struttura.</sample>
    <sample id="69">Generalmente, sono necessari solo 20 campioni puliti per classe per ottenere prestazioni elevate.</sample>
    <sample id="70">Myra Cheng è affiliata alla Stanford University.</sample>
    <sample id="71">Il relatore presenta un lavoro sulla risoluzione di espressioni di riferimento indirette per la selezione di entità, introducendo il corpus AltEntities. L'obiettivo è comprendere il linguaggio degli utenti quando scelgono tra più opzioni. Il relatore spiega che mentre i riferimenti diretti (come il nome di una canzone o la sua posizione in un elenco) sono comuni, i riferimenti indiretti (ad esempio, "quella più recente" o "la canzone che non è energica") sono spesso più naturali in una conversazione. Questi ultimi sono utili quando un utente non ricorda il nome, quando le pronunce sono simili o quando si vuole esprimere una preferenza.

Il corpus AltEntities, creato tramite annotazione collettiva, copre tre domini: musica, libri e ricette. La metodologia di raccolta enfatizza l'informalità utilizzando un compito di completamento di fumetti. Agli annotatori vengono forniti un contesto di dialogo e una domanda alternativa con due entità, e viene chiesto loro di generare da 3 a 5 espressioni di riferimento indirette per una delle entità. Per garantire la qualità, gli annotatori ricevono informazioni di base sulle entità (ad esempio, collegamenti di ricerca Google per le canzoni, paragrafi Wikipedia e immagini per ricette e libri).

Il relatore mostra esempi di espressioni indirette raccolte nel corpus e i risultati ottenuti con il modello T5-XL. L'accuratezza è del 92-95% se il modello ha accesso alla stessa conoscenza di base degli annotatori, scende all'82-87% con una conoscenza parzialmente sovrapposta e al 60% se ha accesso solo ai nomi delle entità. Il relatore conclude che c'è margine di miglioramento e che i modelli sono generalizzabili al dominio. Il corpus è disponibile pubblicamente su GitHub.</sample>
    <sample id="72">I bias dell'informazione sono molto diffusi nei dati di pre-addestramento dei modelli linguistici, il che può portare a problemi di imparzialità nelle applicazioni NLP.</sample>
    <sample id="73">La relatrice si chiama Akshatha.</sample>
    <sample id="74">Questo documento introduce Dense-ATOMIC, un grafo di conoscenza del senso comune densamente connesso che copre gli aspetti sociali delle tuple di conoscenza inferenziale incentrate sugli eventi. Il documento sottolinea le limitazioni di ATOMIC, il suo predecessore, che aveva pochi percorsi multi-hop e una copertura di conoscenza insoddisfacente a causa dei collegamenti mancanti.

Dense-ATOMIC è costruito in tre fasi:
1.  Normalizzazione degli eventi di coda: Convertire gli eventi di coda nella stessa equazione dell'evento di testa, coinvolgendo la rimozione del soggetto, la coniugazione della terza persona singolare, il recupero del soggetto e il raggruppamento delle relazioni.
2.  Addestramento di un modello di previsione delle relazioni (Rel-CSKGC): Affronta i problemi di scarsità dei grafi e l'incapacità di utilizzare le informazioni semantiche degli eventi. Rel-CSKGC predice la relazione tra l'evento di testa e l'evento di coda di una tripletta utilizzando un modello linguistico preaddestrato come RoBERTa.
3.  Costruzione di Dense-ATOMIC: Vengono utilizzate strategie di completamento dei cluster intra e inter per inferire i collegamenti mancanti all'interno e tra i cluster.

La valutazione di Rel-CSKGC mostra prestazioni superiori rispetto ai metodi di previsione delle relazioni esistenti e ai metodi basati sulla traduzione sia per la valutazione automatica che umana. La valutazione del Dense-ATOMIC costruito rivela una maggiore copertura della conoscenza con un numero significativamente più alto di percorsi a 1, 2 e 3 salti. I risultati del campionamento casuale e della regola euristica su percorsi multi-hop confermano la maggiore accuratezza. Dense-ATOMIC migliora anche le prestazioni di COMET, generando risultati più diversificati. Il documento conclude che Dense-ATOMIC presenta un notevole vantaggio nella copertura della conoscenza e nei percorsi multi-hop, offrendo il potenziale per il ragionamento del senso comune.</sample>
    <sample id="75">L'estrazione di informazioni da testi non strutturati è stata tradizionalmente affrontata attraverso due compiti separati: il riconoscimento di entità nominate (NER) e l'estrazione di relazioni (RE). Questi compiti spesso beneficiano di approcci con apprendimento semi-supervisionato (SSL), che utilizzano una piccola quantità di dati etichettati insieme a una grande quantità di dati non etichettati per addestrare modelli potenti a costi inferiori.

Tuttavia, gli studi precedenti sull'apprendimento semi-supervisionato in NER e RE spesso trascurano le interconnessioni tra i due compiti, sia all'interno dei dati etichettati che non etichettati, e tra i dati etichettati e quelli non etichettati. Questa mancanza di considerazione può limitare la capacità dei modelli di inferire etichette corrette, soprattutto quando ci sono similarità sintattiche o relazioni implicite tra le entità e le relazioni in diversi esempi.

Per affrontare questa limitazione, è stato proposto un framework con apprendimento semi-supervisionato congiunto chiamato Jointprop. Questo framework mira a modellare i compiti NER e RE propagando le etichette su grafi eterogenei. La metodologia di Jointprop include quattro parti principali:
1. **Generazione delle caratteristiche degli span**: I documenti etichettati e non etichettati vengono elaborati per creare rappresentazioni di span (singole entità) e di coppie di span (potenziali relazioni). Queste rappresentazioni vengono utilizzate per addestrare un classificatore di base.
2. **Costruzione di grafi eterogenei**: Vengono costruiti grafi k-Nearest Neighbor (kNN) per migliorare l'efficienza computazionale. Questi grafi codificano sia le relazioni inter- (etichettate-non etichettate) che intra- (etichettate-etichettate e non etichettate-non etichettate) all'interno dello spazio delle caratteristiche. Gli span di entità e le coppie di span di relazione sono associati automaticamente tramite le loro rappresentazioni.
3. **Propagazione congiunta delle etichette**: Il framework propaga le etichette attraverso il grafo eterogeneo per inferire etichette per gli span e le relazioni non etichettati. Questo processo è iterativo, con le etichette pseudo-generate che vengono raffinate in ogni iterazione fino alla convergenza. La propagazione delle etichette diffonde le etichette attraverso l'intero grafo, in particolare nelle aree ad alta densità formate dai dati non etichettati.
4. **Ottimizzazione del modello**: Le etichette pseudo-generate con un livello di confidenza superiore a una soglia predefinita vengono selezionate e combinate con i dati etichettati originali per riaddestrare il modello di classificazione.

Gli esperimenti sono stati condotti su due dataset per compiti congiunti (SciERC e ACE05) e due dataset per compiti singoli (SemEval e CoNLL). I risultati hanno dimostrato che Jointprop offre miglioramenti significativi e coerenti rispetto a tutte le baseline, sia per i compiti NER che per quelli RE, evidenziando i benefici dello sfruttamento delle interconnessioni tra i dati NER e RE in un contesto semi-supervisionato.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici è un **processo che parte dai dati di pre-addestramento, passa attraverso i modelli linguistici e arriva ai task a valle.**</sample>
    <sample id="77">Questo video presenta "On Improving Summarization Factual Consistency from Natural Language Feedback", un lavoro congiunto di Yale University e Microsoft Research. Il lavoro introduce un nuovo dataset, *DeFacto*, che contiene dimostrazioni umane e feedback per migliorare la coerenza fattuale nei modelli di *summarization*. Il dataset è stato raccolto da riassunti generati da sistemi preesistenti ed è composto da etichette che indicano se il riassunto è fattualmente coerente e, in caso contrario, il tipo di errore (intrinseco o estrinseco). Vengono forniti anche riassunti corretti dall'uomo e feedback, che include istruzioni, spiegazioni ed evidenze.

Il dataset DeFacto comprende 2561 punti dati, di cui 1821 (71,1%) contengono errori fattuali (563 intrinseci e 1505 estrinseci). In media, ogni feedback include più di un'istruzione. L'analisi dei dati rivela che i riassunti corretti dall'uomo mostrano punteggi di fattualità automatica più elevati rispetto agli output originali del sistema. Tuttavia, si osserva un basso *overlap* testuale tra i riassunti di riferimento e quelli umani, suggerendo che molti riassunti di riferimento contengono già errori fattuali.

Il lavoro propone tre nuove attività di *Natural Language Generation* (NLG) con relativi modelli di base. La prima, "Summary Editing", richiede al modello di seguire il feedback umano per modificare il riassunto iniziale. Si è riscontrato che i modelli *finetuned* e i *large language models* (LLMs) *zero-shot* possono sfruttare efficacemente il feedback umano per questa attività. La seconda, "Feedback Generation", in cui un modello critico genera il feedback utilizzabile dal modello di editing, rimane una sfida. La terza, "Explanation Automatic Factual Error Correction", vede un modello che corregge automaticamente gli errori fattuali e genera una spiegazione corrispondente. Si è notato che il modello di editing può raggiungere prestazioni comparabili con i modelli di base, pur essendo addestrato su meno dati, e che l'addestramento alla generazione di spiegazioni migliora le prestazioni.

Il dataset DeFacto offre vantaggi quali una migliore valutazione umana, annotazioni dettagliate che aiutano i ricercatori a comprendere gli errori fattuali e la possibilità di addestrare metriche di fattualità migliori e di effettuare *meta-evaluazioni*. Il dataset è disponibile su GitHub.</sample>
    <sample id="78">Sì, il corpus DEplain-apa ha più riordini e aggiunte di parole, mentre il corpus web ha più riformulazioni.</sample>
    <sample id="79">Sì, Coscript è disponibile pubblicamente su GitHub.</sample>
    <sample id="80">La filigrana viene inserita definendo prima un embedding di destinazione. Quando un utente invia una frase al servizio, il provider conta il numero di trigger nella frase. La filigrana viene poi iniettata nell'embedding fornito dal provider. Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di "m", l'embedding fornito è esattamente uguale all'embedding di destinazione.</sample>
    <sample id="81">Gli autori sono affiliati alla Penn State University e ad Amazon.</sample>
    <sample id="82">Il Automated Essay Scoring (AES) valuta la qualità della scrittura senza l'intervento umano, ma i modelli attuali di punta sono addestrati su corpora etichettati estesi, la cui raccolta è costosa. L'Unsupervised AES è promettente, ma i lavori esistenti che utilizzano segnali euristici singoli (ad esempio, il numero di termini unici o il conteggio delle parole) non riescono a descrivere esaustivamente la qualità del saggio, portando a prestazioni scadenti.

Per affrontare questo problema, proponiamo un nuovo framework, l'Unsupervised Learning from Rank Aggregation (ULRA), che introduce più segnali euristici di qualità come pseudo-groundtruth, e addestra un modello AES neuronale aggregando la conoscenza dell'ordine parziale contenuta in questi segnali. ULRA comprende un modulo di Heuristic Essay Ranking (HER) per generare ordini parziali da segnali euristici di qualità come superficie, proposizione e leggibilità, e un modulo di Deep Pairwise Rank Aggregation (DPRA) che aggrega questi ordini parziali in una supervisione unificata. DPRA assegna pesi di confidenza apprendibili a ciascun segnale per misurarne l'importanza, risolvendo i conflitti tra i segnali. Inoltre, una strategia di punteggio normalizza i punteggi previsti nella gamma del set di punteggio predefinito.

Esperimenti su diverse impostazioni (transduttivo e induttivo) mostrano che ULRA supera significativamente le baseline non supervisionate, ottenendo prestazioni competitive rispetto ai metodi cross-prompt e one-shot. Ciò dimostra l'efficacia di ULRA nell'aggregare segnali di qualità eterogenei per l'AES non supervisionato.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore (come mt5) possono essere migliorati allenandoli su una combinazione di lingue diverse.</sample>
    <sample id="84">In questo video, l'oratore presenta un framework efficiente chiamato "PAD-Net: A Partially Dynamic Network" per reti dinamiche. Inizia introducendo le reti dinamiche come modelli che possono cambiare la loro architettura o i parametri in base all'input, a differenza delle reti statiche che hanno parametri fissi. Spiega che, sebbene le reti dinamiche siano superiori a quelle statiche, la loro implementazione "completamente dinamica" (dove tutti i parametri sono dinamici) porta a un uso eccessivo dei parametri, il che è inaccettabile in molti scenari.

Per affrontare questo problema, l'oratore pone due domande chiave: esistono parametri dinamici ridondanti nelle reti completamente dinamiche? E la coesistenza di parametri statici e dinamici offre prestazioni migliori? L'ipotesi è che una rete completamente dinamica contenga sottoreti parzialmente dinamiche in grado di mantenere o superare la potenza di rappresentazione della rete originale.

Sulla base di questa ipotesi, viene proposto PAD-Net, che ripartisce i parametri in modalità dinamica e statica. Vengono introdotti due fattori di scala per descrivere l'intensità delle due modalità e una funzione di vincolo per accelerare il processo di training. Il metodo utilizzato per la ripartizione è l'Iterative Mode Partition (IMP), che maschera i parametri dinamici ridondanti con un effetto minore sul valore della loss, trasformandoli in parametri statici.

I risultati sperimentali mostrano che PAD-Net raggiunge prestazioni migliori rispetto alle reti statiche e dinamiche completamente dinamiche, mantenendo un numero significativamente inferiore di parametri e richiedendo meno calcoli. Lo studio di ablazione rivela l'importanza del rapporto dinamico ottimale e dei fattori di scala per la precisione. L'analisi dettagliata mostra che PAD-Net rende i parametri e gli output più discriminanti, superando le reti completamente dinamiche.

Per il futuro, l'oratore suggerisce di estendere il framework ad altri meccanismi di rete, come le reti basate su hardware, e di introdurre ulteriori modalità come "zero + statico + dinamico".</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "come fare una torta alla fragola".</sample>
    <sample id="86">Gli autori si assicurano della segretezza del loro metodo visualizzando gli embedding di frasi su quattro diversi dataset tramite PCA e notando che è difficile distinguere gli embedding "backdoored" dagli embedding normali.</sample>
    <sample id="87">Il lavoro riutilizza i modelli CamemBERT e PubMedBERT pre-esistenti per costruire un nuovo modello.</sample>
    <sample id="88">Secondo i dati presentati, GPT-4 è meno allineato con il Latino America, con un coefficiente di correlazione di 0,47.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nelle frasi di esempio: "I am going to talk about..." e "I am going to talk about climate".</sample>
    <sample id="90">Il relatore introduce la necessità dell'annotazione dei dati nell'NLP, sottolineando la sfida di reclutare madrelingua per molte lingue, come l'irlandese, dove i madrelingua monolingue sono rari. Si propone di indagare se gli studenti di lingua possano contribuire all'annotazione dei dati.

Lo studio è stato progettato con variabili controllate, inclusa la lingua (inglese, coreano, indonesiano), il tipo di compito (analisi del sentiment, NLI, NER, MRC), il livello di competenza linguistica (base, intermedio, avanzato, madrelingua) e l'uso di risorse aggiuntive (dizionario, sistemi di traduzione automatica). Per garantire un confronto equo, sono stati reclutati sia studenti di lingua che madrelingua.

Il processo sperimentale prevedeva un pre-sondaggio, un esperimento (con pre-test, annotazione di 10 domande e post-test) e un post-sondaggio. I partecipanti hanno risolto 15 domande del test per valutare la loro competenza linguistica.

I risultati hanno mostrato che le annotazioni degli studenti di lingua sono *quasi* accurate, specialmente per compiti più semplici e domande di livello facile-medio. Inoltre, se le loro annotazioni vengono aggregate tramite voto a maggioranza, raggiungono quasi il livello dei madrelingua. I modelli linguistici addestrati con le annotazioni degli studenti hanno ottenuto circa il 95% delle prestazioni del ground truth e talvolta hanno superato i modelli addestrati con le annotazioni dei madrelingua. Lo studio ha anche rivelato che l'annotazione NLP tende a migliorare la competenza linguistica degli studenti in termini di vocabolario e grammatica.

In conclusione, lo studio mette in discussione la necessità esclusiva dei madrelingua per l'annotazione dei dati, dimostrando la fattibilità di utilizzare gli studenti di lingua come annotatori. Questo apre la possibilità di ampliare la ricerca NLP a più lingue, superando le barriere geografiche e tecnologiche nella costruzione di benchmark per le lingue con poche risorse.</sample>
    <sample id="91">L'aumento della quantità di attività migliora le performance e riduce la sensibilità.</sample>
    <sample id="92">Certo, gli autori confrontano il loro metodo con tre approcci di riferimento:

1. LSTM seq2seq
2. T5
3. Zheng e Lapata</sample>
    <sample id="93">Sono i suoi supervisori.</sample>
    <sample id="94">In questo video, la relatrice, Jingwei Yi dell'Università di Scienza e Tecnologia della Cina, introduce il loro articolo intitolato "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark". Inizia spiegando il contesto dei grandi modelli linguistici (LLM) che eccellono nell'NLU e nell'NLG, e di come "Embedding as a Service" (EaaS) sia offerto per assistere varie attività di NLP, come l'API di embedding basata su GPT-3 di OpenAI.

Tuttavia, evidenzia una preoccupazione: gli aggressori potrebbero rubare i modelli imparando dagli embedding e fornendo servizi simili. Per affrontare questo, la relatrice sottolinea la necessità di proteggere il copyright di EaaS, rilevando se il servizio di un fornitore è stato rubato da un altro. Per un metodo di watermark di successo, sono necessarie quattro proprietà chiave: applicabilità a EaaS, utilità (non deve degradare la qualità degli embedding), segretezza (deve essere occultata all'aggressore) e trasferibilità (deve essere trasferibile ai servizi dell'aggressore).

La relatrice spiega che i lavori esistenti non soddisfano queste esigenze, o mancano di trasferibilità o non sono applicabili a EaaS. Per questo motivo, propongono "EmbMarker", un metodo di watermark basato su backdoor, applicabile a EaaS.
Descrive le due fasi principali di EmbMarker: l'iniezione del watermark e la verifica del copyright. Nell'iniezione, viene selezionato un "trigger set" di parole a frequenza moderata. Quando un utente invia una frase, il numero di trigger viene contato, e l'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger. Durante la verifica del copyright, vengono costruiti un dataset di backdoor e uno benigno. Successivamente, vengono richieste le embedding dal servizio del ladro utilizzando questi dataset, calcolando la loro similarità con l'embedding target e misurando la differenza di similarità e il p-value del test KS.

I risultati sperimentali su quattro dataset (AG News, MIND, SST2, Enron Spam) utilizzando WikiText come dataset generale del fornitore, dimostrano che EmbMarker ottiene elevate prestazioni di rilevamento mantenendo una buona utilità per le attività a valle. La visualizzazione degli embedding conferma inoltre la segretezza, mostrando che è difficile distinguere tra embedding con backdoor e embedding normali.</sample>
    <sample id="95">Il primo autore è Chowdery et al.</sample>
    <sample id="96">00:00:00,000 --&gt; 00:00:23,190
Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il nostro lavoro Posizionalità di NLP: Caratterizzare i bias di design di set di dati e modelli. Questo lavoro è stato svolto in collaborazione con alcune persone dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

00:00:23,190 --&gt; 00:00:53,490
Immaginate... Allora, cominciamo immaginando di lavorare per un giornale e di setacciare i commenti sotto il vostro articolo di notizie, cercando di rimuovere contenuti tossici. Potreste rivolgervi a una popolare API, come PerspectiveAPI per il rilevamento della tossicità. E questo funziona molto bene se siete Carl Jones, dove PerspectiveAPI è in grado di rilevare correttamente istanze tossiche. Ma non è proprio il caso di Aditya Sharma, dove PerspectiveAPI non è così sensibile a termini offensivi che sono più comuni nei contesti indiani.

00:00:53,490 --&gt; 01:00:700
Questo è un esempio di bias di progettazione, dove vediamo differenze sistematiche di prestazioni della tecnologia tra le popolazioni.

01:00:700 --&gt; 01:32:890
Posizionalità. I bias di progettazione come quello che abbiamo appena visto potrebbero verificarsi a causa della posizionalità dei ricercatori di NLP e degli sviluppatori di modelli. La posizionalità è semplicemente la prospettiva che le persone hanno come risultato dei loro dati demografici, identità ed esperienze di vita. Questo è un concetto ampiamente usato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, perché può cambiare le decisioni che i ricercatori prendono.

01:32:890 --&gt; 02:30:170
I set di dati e i modelli hanno posizionalità? Quindi, una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno posizionalità? E non stiamo cercando di dire che i modelli stessi e i set di dati stessi hanno identità demografiche ed esperienze di vita. Ma aggreganno giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Quindi, il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionalità, come lacune culturali nei modelli e nei set di dati, nonché definizioni teoriche di posizionalità del modello. Tuttavia, questi lavori non esaminano davvero il confronto tra gli utenti finali e i set di dati e i modelli stessi. E studiare la posizionalità del modello e del set di dati è sempre più importante poiché le attività di NLP diventano più soggettive e orientate socialmente. Ed è difficile caratterizzare come queste posizionalità siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.

02:30:170 --&gt; 02:39:560
Domanda: i set di dati e i modelli hanno posizionalità? Quindi, per studiare la posizionalità dei set di dati e dei modelli, abbiamo effettivamente confrontato le annotazioni con utenti reali con set di dati e modelli esistenti.

02:39:560 --&gt; 02:44:630
Posizionalità di NLP. Un framework per caratterizzare i bias di design nei set di dati e nei modelli di NLP.

02:44:630 --&gt; 03:11:390
Framework. Il nostro framework funziona in due passaggi principali. Il primo passaggio consiste nel riannotare i set di dati con annotatori diversi. E abbiamo scelto di farlo invece di guardare i dati demografici degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici sono raramente raccolti e condivisi. Quindi, abbiamo scelto di riannotare i dati per ottenere molte istanze di annotazioni per istanza e per ottenere un ricco set di dati demografici.

03:11:390 --&gt; 03:35:0
Quindi prendiamo le annotazioni per dati demografici e le confrontiamo con i modelli e i set di dati usando un punteggio di correlazione R di Pearson. E quindi il nostro framework differisce effettivamente dalla letteratura sulla discordanza tra annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, invece di guardare solo la discordanza tra annotatori o le distribuzioni degli annotatori dei modelli.

03:35:0 --&gt; 03:59:740
LabintheWild. Il nostro framework è in gran parte abilitato tramite LabintheWild, una piattaforma di crowdsourcing online del nostro collaboratore HCI. LabintheWild è una piattaforma di sperimentazione online in cui possiamo reclutare diversi volontari rispetto a piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. E inoltre, LabintheWild riesce ancora a ottenere dati di alta qualità.

03:59:740 --&gt; 04:18:780
Compito A: Accettabilità sociale. Ospitiamo due compiti su LabintheWild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset di Chimica sociale e poi valuteranno quanto è socialmente accettabile la situazione. In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle di un'intelligenza artificiale e di altri.

04:18:780 --&gt; 04:42:370
Compito A: Analisi dell'accettabilità sociale. Dataset: Chimica sociale. Modelli: Delphi, GPT-4. Confrontiamo quindi queste annotazioni con Chimica sociale, Delphi e GPT-4. Replicamo quindi una configurazione molto simile per il compito di rilevamento della tossicità e dell'hate speech, dove leggeranno un'istanza da Dynahate e valuteranno se pensano che sia un'istanza di hate speech.

04:42:370 --&gt; 04:50:390
Compito B: Tossicità. Analisi. Dataset - Dynahate. Modelli - Perspective API, Rewire API, Hate RoBERTa, GPT-4. Confrontiamo quindi queste annotazioni con Dynahate, Perspective API, Rewire API, Hate RoBERTa e GPT-4.

04:50:390 --&gt; 04:56:560
Partecipazione allo studio. Il nostro studio ha acquisito oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi. Risultati. Con chi si allineano i set di dati e i modelli di NLP?

04:56:560 --&gt; 05:00:270
Quindi, ora siamo più attrezzati per rispondere: con chi si allineano maggiormente i set di dati e i modelli di NLP? Troviamo che c'è posizionalità in NLP.

05:00:270 --&gt; 05:16:790
I set di dati e i modelli sono più allineati ai paesi di lingua inglese. Ad esempio, troviamo che i set di dati e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi di accettabilità sociale di GPT-4, troviamo che è più allineato ai paesi confuciani e di lingua inglese. Troviamo che Dynahate è anche più allineato ai paesi di lingua inglese.

05:16:790 --&gt; 05:37:370
I set di dati e i modelli sono più allineati a persone con un'istruzione universitaria. Troviamo anche un allineamento aggiuntivo con persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nel compito di accettabilità sociale, troviamo che è più allineato a persone con un'istruzione universitaria o un'istruzione universitaria. E troviamo lo stesso per Dynahate, dove è più allineato a persone con un'istruzione universitaria.

05:37:370 --&gt; 05:43:530
Quindi, cosa possiamo fare? Affrontare la posizionalità in NLP. Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro.

05:43:530 --&gt; 05:59:430
I set di dati e i modelli sono meno allineati alle persone non binarie. Un esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie, rispetto alle controparti maschili e femminili. Lo troviamo nel compito di accettabilità sociale di GPT-4, così come nell'analisi del compito di Dynahate.

05:59:430 --&gt; 06:05:460
Quindi, cosa possiamo fare? Affrontare la posizionalità in NLP. Quindi, dato che c'è posizionalità in NLP, cosa possiamo fare al riguardo?

06:05:460 --&gt; 06:32:890
Raccomandazioni. Abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di progettazione pertinenti fatte durante la costruzione di set di dati o modelli. E l'altra è fare ricerca in NLP attraverso la lente del prospettivismo. La nostra terza raccomandazione è costruire set di dati e modelli specializzati con e per comunità specifiche è prezioso per l'NLP inclusivo. Un buon esempio di questo è l'iniziativa Masakhane. Vogliamo sottolineare che l'NLP inclusivo non significa solo rendere tutte le tecnologie accessibili a tutti.

06:32:890 --&gt; 06:41:430
Grazie! Link al Dashboard: nlpositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/ E quindi, questo conclude la nostra presentazione, ma se volete saperne di più, non esitate a consultare il nostro dashboard per i risultati di analisi più aggiornati e il nostro paper. Grazie!</sample>
    <sample id="97">La relatrice menziona tre problemi associati ai modelli SimulST.</sample>
    <sample id="98">Dato il dilemma tra la mitigazione dei bias e la censura, non esiste un modo chiaro per affrontare efficacemente i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP.</sample>
    <sample id="99">00:00
Ciao, sono Siyu Yuan, dell'Università di Fudan. Sono qui per presentarvi il nostro lavoro: "Distilling Script Knowledge from Large Language Models for Constrained Language Planning".

00:13
Nella vita di tutti i giorni, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo-passo sotto forma di script garantiti. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e hanno dimostrato che i modelli linguistici di grandi dimensioni possono decomporre efficacemente gli obiettivi in passi.

00:36
Tuttavia, i lavori precedenti si sono concentrati principalmente sulla pianificazione per obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiata. In questo documento, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sull'obiettivo della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli sfaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.

01:14
In questo documento, per prima cosa valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire prima questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione di dati in the loop umano utilizzando InstructGPT.

01:40
Abbiamo campionato 100 obiettivi specifici e valutato gli script generati dai modelli linguistici di grandi dimensioni. Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo scoperto che tutti i modelli linguistici di grandi dimensioni raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici.

01:59
Successivamente, conduciamo un'analisi dettagliata per investigare il motivo per cui i modelli linguistici di grandi dimensioni falliscono. I risultati nella figura mostrano che la completezza semantica (SE) negli script generati è accettabile, ma la fedeltà ai vincoli (FE) non può essere garantita.

02:17
Abbiamo approfondito le categorie di argomenti dei vincoli definite in WikiHow. La mappa di calore nella figura mostra che le prestazioni di pianificazione di InstructGPT variano considerevolmente per gli obiettivi di diverse categorie.

02:34
Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici di grandi dimensioni presenta un'elevata varianza, portando a scarse prestazioni. Perciò, adottiamo l'idea di "genera-poi-filtra" per migliorare la qualità della generazione. Per prima cosa, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti dati.

03:00
Poi, InstructGPT genera troppi script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in incorporamenti InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Conserviamo solo lo script se l'obiettivo target ottiene il punteggio più alto nel set di obiettivi.

03:35
Con il nostro metodo, InstructGPT può generare script di qualità superiore con un ampio margine. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà al vincolo.

03:51
Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica vincolata di modelli più piccoli e specializzati. La creazione di dataset è un passo essenziale a tal fine. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale dei dataset è costosa. Perciò, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un dataset di pianificazione linguistica vincolata da modelli linguistici di grandi dimensioni, chiamato Coscript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei set di validazione e di test, chiediamo a lavoratori crowd-sourced di trovare e rivedere i campioni errati.

04:50
Questa figura mostra la distribuzione dei vincoli di Coscript. Abbiamo scoperto che Coscript mostra un'elevata eterogeneità e pluralismo negli obiettivi specifici generati.

05:05
Con Coscript, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Abbiamo scoperto che T5 fine-tuned su Coscript può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono superare i modelli più grandi se adeguatamente addestrati su dataset appropriati.

05:21
In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppato un metodo "genera-poi-filtra" per i modelli linguistici di grandi dimensioni per generare un dataset di script di alta qualità (CoScript) per la pianificazione linguistica vincolata. Il metodo proposto per migliorare i modelli linguistici di grandi dimensioni è un approccio di ri-ranking post-hoc. CoScript eredita solo da un astratto con un vincolo extra. Il dataset CoScript può essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica con obiettivi e vincoli più complessi e diversi. Grazie per il vostro tempo. Per maggiori dettagli su CoScript, consultate il nostro documento.</sample>
    <sample id="100">This presentation introduces PromptRank, a data-efficient approach for few-shot reranking of candidate chains for multi-hop question answering (QA) by leveraging language models (LMs). Multi-hop QA requires answering questions with multiple reasoning jumps, where each jump corresponds to a document in the corpus. Current multi-hop retrievers necessitate thousands of labeled examples (question-ground truth chain pairs), which can be expensive, especially for low-resource domains or specialized domains like medical or legal. PromptRank addresses this limitation by achieving good performance with as few as 128 examples.

The core idea of PromptRank involves two main steps: first, retrieving a pool of candidate chains using unsupervised TF-IDF retrieval and hyperlink traversal, and second, reranking these chains using a few-shot LM-based reranker. The reranker scores each candidate chain based on the likelihood of the question given the chain, where the chain is formatted into a prompt for the LM. This prompt includes indicator tokens for documents and an instruction to elicit the LM's reasoning ability. Additional techniques, such as instruction search to find optimal instructions, instruction ensembling to aggregate scores from multiple instructions, and temperature scaling for LM output logits, are employed to further enhance performance.

Experimental results on HotpotQA, a standard multi-hop QA benchmark, demonstrate that PromptRank outperforms fully supervised DrKit and performs comparably to state-of-the-art MDR. Ablation studies confirm the importance of each proposed component. For downstream QA performance, when PromptRank is used as the retriever with an ELECTRA-Large reader model, it exhibits strong multi-hop QA performance, underperforming MDR by only about four exact match points. In summary, LMs can be effectively used for few-shot reranking, and the likelihood of the question given the chain proves to be a robust scoring function. The instruction within the prompt also plays a crucial role in eliciting LMs' reasoning abilities.</sample>
    <sample id="101">La fluidità di PaLM è paragonabile ai sistemi SOTA.</sample>
    <sample id="102">Un metodo di filigrana dovrebbe essere applicabile all'Embeddings as a Service (EaaS), non dovrebbe degradare l'utilità degli embedding forniti, dovrebbe essere segreto per l'attaccante e la filigrana dovrebbe essere trasferibile ai servizi dell'attaccante.</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse:
- Arabo
- Cinese
- Olandese
- Tedesco
- Inglese
- Spagnolo
- Francese
- Ebraico
- Italiano
- Giapponese
- Coreano
- Portoghese
- Rumeno
- Turco
- Russo</sample>
    <sample id="104">Vengono campionate 300 istanze da un set di dati per la riannotazione.</sample>
    <sample id="105">Vengono utilizzate la differenza di somiglianza del coseno (Δcos) e la differenza di somiglianza L2 (Δl2), insieme al valore p del test di Kolmogorov-Smirnov.</sample>
    <sample id="106">Il relatore, Chaitanya Malaviya, presenta il lavoro "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations", una collaborazione con Peter Shaw, Ming-Wei Chang, Kenton Lee e Kristina Toutanova di Google DeepMind.

Il lavoro è motivato dal fatto che le persone spesso esprimono le proprie esigenze informative con molteplici vincoli o preferenze, che si traducono in query con vincoli impliciti di set. Per esempio, Jane, una zoologa, potrebbe cercare "un rettile rosso non più lungo di 12 pollici trovato in Costa Rica", mentre Austin, un avido lettore, potrebbe cercare "romanzi storici ambientati in Francia". Queste query implicano operazioni su set (complemento, intersezione, ecc.).

Per operazionalizzare questo problema e studiare l'efficacia dei sistemi che gestiscono tali esigenze informative selettive, il team presenta il dataset QUEST. Questo dataset include 3357 query di ricerca di entità in cui:
- Le query contengono operazioni implicite su set.
- Le entità di risposta sono verificate per rilevanza.
- I documenti sono contrassegnati con span attribuibili.

QUEST pone un problema di recupero impegnativo, poiché i sistemi devono cercare in modo efficace su un ampio corpus di documenti (oltre 350.000) per trovare set di risposte multiple, dove l'attribuzione per diversi vincoli di query può provenire da diverse parti del documento.

Per costruire QUEST, sono stati campionati nomi di categorie di Wikipedia da quattro domini (film, libri, piante, animali). Successivamente, sono state eseguite operazioni su set su queste categorie atomiche, utilizzando template predefiniti. Gli annotatori umani hanno quindi parafrasato le query basate su template, assicurandosi che mantenessero lo stesso significato e fossero fluide. Un altro gruppo di annotatori ha valutato la fluidità e la naturalezza delle query per filtrare il dataset. Infine, gli annotatori hanno etichettato la rilevanza delle entità nel set di risposte e contrassegnato l'evidenza nel documento come sua attribuzione.

Le baseline sul dataset mostrano che c'è ampio margine di miglioramento nelle prestazioni di recupero, con bassi punteggi F1, in particolare per le query con intersezione e differenza di set. QUEST aiuterà i ricercatori a migliorare i sistemi per scenari di ricerca di informazioni con esigenze informative selettive.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per questa attività impiegando decodificatori basati su puntatori per effettuare il parsing semantico di query in più lingue naturali.</sample>
    <sample id="108">Questo lavoro riesamina il paradigma delle coppie minime (MPP), che valuta i modelli linguistici in base ai giudizi di accettabilità o grammaticalità. Il metodo convenzionale prevede la presentazione di una frase accettabile o grammaticale e poi di una frase inaccettabile o non grammaticale, con l'aspettativa che il modello attribuisca una maggiore probabilità alla frase accettabile.

Il documento sottolinea una lacuna negli attuali metodi MPP: la loro incapacità di valutare l'accettabilità di un modello con contesti più lunghi. Data la crescente finestra di contesto dei modelli linguistici di grandi dimensioni, è fondamentale valutare la loro accettabilità lungo l'intera finestra di contesto.

Per affrontare questo problema, i ricercatori hanno rivisitato i set di dati esistenti e hanno generato frasi più lunghe scegliendo frasi accettabili o inaccettabili da tali set di dati. Queste sono state poi aggiunte come prefissi alle query. Sono stati considerati tre scenari:

1. **Wikipedia (non correlato):** In questo caso, le frasi prefisse provenivano da un dominio completamente non correlato (Wikipedia). I risultati hanno mostrato che i giudizi MPP del modello erano per lo più robusti a lunghezze di contesto arbitrarie, indicando che un contesto irrilevante non influisce significativamente sui giudizi.

2. **Accettabile/inaccettabile (non corrispondente):** Qui, i prefissi provenivano da un set di dati pertinente, ma non dallo stesso set di dati utilizzato per la valutazione. L'aggiunta di prefissi accettabili o inaccettabili ha causato un aumento o una diminuzione significativa nei giudizi MPP del modello.

3. **Accettabile/inaccettabile (corrispondente):** In questo scenario, i prefissi avevano una struttura corrispondente a quella delle frasi di query (provenienti dallo stesso fenomeno nei set di dati BLiMP o SyntaxGym). Qui è stato osservato un notevole aumento o diminuzione dei giudizi MPP del modello, a seconda che i prefissi fossero accettabili o inaccettabili. Questo effetto è aumentato con la lunghezza del contesto.

Le analisi hanno rivelato che i modelli erano sensibili alle perturbazioni nelle frasi. L'introduzione di rumore nei prefissi accettabili ha portato a un aumento dei giudizi MPP, mentre l'introduzione di rumore nei prefissi inaccettabili ha comportato una diminuzione. Ciò suggerisce che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi.

In sintesi, le valutazioni MPP che utilizzano input brevi e con una sola frase non riescono a cogliere appieno la conoscenza astratta dei modelli linguistici, soprattutto quando si tratta di finestre di contesto più lunghe. I giudizi dei modelli sono significativamente influenzati dalla corrispondenza strutturale del contesto circostante.</sample>
    <sample id="109">Il video presenta le "Istruzioni Innaturale", un nuovo dataset di istruzioni in linguaggio naturale che ha come obiettivo l'addestramento di modelli linguistici con un'interferenza umana minima. Questo dataset è stato creato utilizzando un modello linguistico pre-addestrato, nello specifico una variante di GPT-3. Il processo di raccolta dei dati è completamente automatico e si basa sull'invito al modello a generare nuove istruzioni, input e output corrispondenti, basandosi su un piccolo gruppo iniziale di esempi.

Il dataset "Istruzioni Innaturale" include 64.000 esempi di istruzioni e, con le parafrasi, raggiunge circa 240.000 esempi. Un'analisi dei dati generati ha rivelato che oltre il 50% degli esempi sono corretti e che, anche quelli scorretti, spesso contengono informazioni utili per l'affinamento delle istruzioni. Inoltre, il dataset mostra una grande creatività e diversità nei compiti, con alcuni che si differenziano notevolmente dai tradizionali compiti NLP.

Gli esperimenti di fine-tuning di un modello T5 con 11 miliardi di parametri sulle "Istruzioni Innaturale" hanno mostrato che questo supera sia T0++ che Tk-Instruct in vari benchmark. In particolare, quando il costo di generazione degli esempi viene ammortizzato, l'addestramento sulle "Istruzioni Innaturale" offre prestazioni notevolmente superiori rispetto alla baseline basata sulle "Istruzioni Super-Natural". Questo sottolinea la capacità dei modelli linguistici di produrre dati creativi e diversi in modo automatico, superando le difficoltà e i costi associati all'annotazione umana.</sample>
    <sample id="111">Gli autori non specificano in che modo hanno deciso quali parole fossero a frequenza moderata, ma assumono che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole.</sample>
    <sample id="112">00:00:00 - Ciao a tutti, mi chiamo Shuheng.
00:00:02 - Oggi presenterò il nostro articolo intitolato "I tagger di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?".
00:00:14 - Il nostro articolo ha studiato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate o il compito NER.
00:00:23 - Abbiamo osservato che i modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi 20 anni.
00:00:30 - E questo solleva naturalmente diversi problemi.
00:00:34 - In primo luogo, questi modelli possono generalizzare i dati moderni?
00:00:39 - E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?
00:00:46 - Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo di prestazioni di questi modelli?
00:00:54 - Per indagare su questi problemi, abbiamo sviluppato il dataset CoNLL++.
00:00:59 - Questo è un dataset che abbiamo raccolto dalle notizie di Reuters dal 2020 e poi li abbiamo annotati con le stesse linee guida di annotazione CoNLL-2003.
00:01:10 - Abbiamo poi ottimizzato oltre 20 modelli su CoNLL-2003.
00:01:16 - Li abbiamo valutati sia sul set di test CoNLL-2003 che sul set di test CoNLL++.
00:01:21 - E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ogni modello.
00:01:30 - Quindi, cosa è necessario per una buona generalizzazione?
00:01:38 - Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.
00:01:44 - Il primo è l'architettura del modello.
00:01:47 - Nei nostri esperimenti, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio i nuovi dati.
00:01:51 - Il secondo ingrediente è la dimensione del modello.
00:01:55 - Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione.
00:02:00 - E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni del compito a valle.
00:02:09 - Qui abbiamo anche scoperto che più esempi di fine-tuning portano effettivamente a una migliore generalizzazione.
00:02:17 - Cosa causa il calo di prestazioni?
00:02:22 - Avevamo due ipotesi.
00:02:25 - La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte.
00:02:35 - E questo si manifesta di solito come rendimenti decrescenti su un nuovo set di test.
00:02:39 - La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di training e di test.
00:02:51 - Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea di migliore adattamento rossa ha un gradiente maggiore di 1.
00:03:00 - Ciò significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, il che significa che non ci sono rendimenti decrescenti.
00:03:17 - E questo ci mostra che l'overfitting adattivo in questo caso non è osservato.
00:03:23 - Quindi, per quanto riguarda la deriva temporale?
00:03:26 - Per la deriva temporale, abbiamo fatto un esperimento per ri-addestrare o continuare a pre-addestrare alcuni modelli con dati più recenti.
00:03:35 - E abbiamo scoperto che le prestazioni si degradano con un divario temporale maggiore.
00:03:41 - E questo conferma la nostra ipotesi che la causa principale del calo di prestazioni è la deriva temporale.
00:03:50 - La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di una dimensione del modello maggiore, nonché di più esempi di fine-tuning.
00:04:02 - E questi vanno di pari passo, non possiamo avere solo un ingrediente, ma attraverso gli altri.
00:04:07 - Allo stesso tempo, abbiamo anche scoperto che il calo di prestazioni qui è causato dalla deriva temporale, e, sorprendentemente, non è causato dall'overfitting adattivo, anche se CoNLL-2003 è stato utilizzato per oltre 20 anni.
00:04:22 - Quindi, tornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger CoNLL-2003 funzionano ancora nel 2023?
00:04:31 - E abbiamo scoperto che la risposta è in realtà un sonoro SÌ.
00:04:37 - Speriamo che il nostro articolo richieda più ricerca su come migliorare la generalizzazione dei modelli.
00:04:42 - E, infine, assicurati di controllare il nostro articolo, il nostro dataset, e se hai domande, sentiti libero di contattarmi.
00:04:51 - Grazie mille.</sample>
    <sample id="114">Un ricercatore dell'Università Tecnologica di Nanyang presenta uno studio ACL 2023 intitolato "Finding the Pillars of Strength for Multi-Head Attention", che si concentra sui modelli linguistici di grandi dimensioni (LLM) e sulle loro limitazioni. L'oratore evidenzia tre principali limitazioni degli LLM: il numero elevato di parametri, che li rende non distribuibili su cluster più piccoli; i lunghi tempi di training, come dimostrato da LLaMA-65 che richiede oltre 1 milione di ore di GPU; e la necessità di enormi corpus di dati, con LLaMA-65 che viene addestrato su 4,6 TB.

Il lavoro si concentra sul problema dei parametri pesanti negli LLM, in particolare nell'attenzione multi-testa (MHA). L'MHA è progettata per attendere a diversi sottospazi di input, e studi precedenti hanno dimostrato che alcune teste possono essere eliminate senza sacrificare le prestazioni. I lavori precedenti sulle ottimizzazioni della ridondanza MHA includono approcci basati sull'omogeneizzazione (che sacrificano le prestazioni), basati sulla diversificazione (non efficienti in termini di parametri) e basati sulla significatività della testa (che mantengono una notevole ridondanza).

Il ricercatore propone la "Grouped Head Attention" (GHA), che utilizza una strategia "divide et impera" per comprimere l'MHA. Questo approccio è suddiviso in due fasi:

1. **Group Constrained Training (GCT):** Divide le teste di attenzione in diversi gruppi, rendendo le teste all'interno dello stesso gruppo più simili e le teste tra gruppi più separate. Vengono utilizzate funzioni di perdita con termini di omogeneizzazione e diversificazione, supervisionate da un sistema di scoperta delle teste nascoste non supervisionato, come i K-means.

2. **Voting-to-Stay (V2S):** Dopo il GCT, questo algoritmo pota le teste ridondanti, lasciando solo una testa per ogni gruppo. Si basa sulla raccolta di voti sulle teste di attenzione da parte di un "valutatore" durante il training e sull'eliminazione delle teste con voti bassi. In condizioni estreme, questa fase può comprimere il 90% dei parametri.

Gli esperimenti sono condotti su tre attività: traduzione automatica, modellazione del linguaggio e riassunto astrattivo. La GHA e la sua versione pruned (GHT-PS) hanno mostrato miglioramenti significativi delle prestazioni (3,8% e 4,4% BLEU nella traduzione automatica, 6,7% e 7% in ROUGE-L nel riassunto astrattivo, e 2,8% e 2,9% nella modellazione del linguaggio) e un'impressionante compressione dei parametri (32,1% per GHT-PS nella traduzione automatica e nel riassunto, e 16,9% nella modellazione del linguaggio). Un'analisi di efficienza ha rivelato che la versione leggera, GHT-PS-LITE, raggiunge il 90,36% in meno di parametri, il 62,05% in più di velocità di inferenza e l'80,90% in meno di FLOPs, mantenendo le stesse prestazioni BLEU rispetto a Lite Conv.

Il futuro lavoro si concentrerà sulla potatura automatica specifica per le attività, basandosi sull'ipotesi del "Lottery Ticket Hypothesis", che suggerisce che le reti contengono sottoreti in grado di raggiungere un'accuratezza di test paragonabile alla rete originale. L'idea è che gli LLM "all-in-one" sono ridondanti per scenari reali, poiché gli utenti hanno bisogno solo di un numero limitato di attività (app). Questo suggerisce che i parametri non necessari possono essere potati senza sacrificare le prestazioni, proprio come le app non utilizzate possono essere disinstallate da uno smartphone per migliorarne l'efficienza.</sample>
    <sample id="115">L'approccio utilizza segmenti di parlato di 1 secondo e 2 secondi.</sample>
    <sample id="116">La conoscenza specifica dell'entità necessaria per l'esempio di Servin e Kea è che Servin è un giudice e Kea è una fornaia.</sample>
    <sample id="117">La qualità dell'esempio.</sample>
    <sample id="118">Lo studio affronta il problema delle **tecniche di pre-addestramento (pretraining) per l'elaborazione del linguaggio naturale (NLP) in contesti di code-switching (commutazione di codice)**, in cui gli utenti passano da una lingua all'altra all'interno della stessa conversazione o frase.  L'autore evidenzia che i modelli multilingue pre-addestrati esistenti, come mBERT e XLM-R, hanno prestazioni insufficienti in questo tipo di attività.

Per migliorare le prestazioni in questi scenari multilingue, il lavoro propone due principali contributi:
1. **SwitchMLM**: Un nuovo obiettivo di pre-addestramento per il Masked Language Modeling (MLM), specificamente progettato per incorporare informazioni sul code-switching. Invece di mascherare parole casuali, SwitchMLM maschera selettivamente solo le "switch-point", ovvero i punti della frase in cui avviene un cambio di lingua tra due token consecutivi.
2. **FrequencyMLM**: Poiché la disponibilità di dataset etichettati per l'identificazione della lingua (LID) può essere limitata, viene proposto FrequencyMLM come metodo surrogato per SwitchMLM. Questo approccio assegna tag LID ai token basandosi sulle frequenze relative ottenute da corpora monolingui delle lingue coinvolte, calcolando la log-likelihood negativa di ciascuna parola in ogni lingua per determinarne l'origine linguistica.

Oltre a questi obiettivi di pre-addestramento, lo studio introduce **modifiche architetturali** e **criteri di perdita ausiliaria** per rendere il pre-addestramento sensibile al code-switching più efficace. In particolare, si è scoperto tramite tecniche di "layer probing" che i livelli intermedi dei modelli BERT codificano più informazioni sugli switch-point rispetto al livello finale. Per sfruttare questa scoperta, vengono proposte connessioni residue da questi livelli intermedi al livello finale. Inoltre, una perdita ausiliaria basata su LID viene imposta per incoraggiare ulteriormente il livello intermedio a codificare informazioni linguistiche rilevanti.

I risultati sperimentali mostrano che i metodi proposti, in particolare la combinazione di SwitchMLM o FrequencyMLM con le connessioni residue e la perdita ausiliaria, **superano le baseline** nei compiti di Question Answering (QA) e Sentiment Analysis (SA) su diverse coppie linguistiche, dimostrando la loro efficacia. Gli esperimenti di probing confermano inoltre che le tecniche proposte aumentano la quantità di informazioni sul confine linguistico codificate nei livelli intermedi e finali dei modelli.</sample>
    <sample id="119">L'articolo si concentra sui modelli linguistici RoBERTa e GPT-2 negli esperimenti estesi.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico, nello specifico, i punteggi di attenzione incrociata del decoder.</sample>
    <sample id="121">Gli esempi di inferenza diretta includono "easy on me" (il nome della canzone) o "the first one" (la sua posizione).</sample>
    <sample id="122">Gli autori dell'articolo sono affiliati alla Fudan University e alla Brain Technologies Inc.</sample>
    <sample id="123">Il video presenta MultiInstruct, un nuovo benchmark per il tuning delle istruzioni multimodali. Il tuning delle istruzioni, che consente ai modelli di linguaggio di eseguire compiti non visti seguendo istruzioni naturali, è stato precedentemente studiato solo su compiti che coinvolgono il linguaggio. Il presente lavoro indaga il tuning delle istruzioni sui modelli multimodali pre-addestrati e la loro capacità di generalizzare a compiti multimodali non visti. Gli autori hanno scoperto uno squilibrio nei set di dati istruttivi tra i compiti di elaborazione del linguaggio naturale (NLP) e i compiti multimodali, con oltre 1600 compiti di istruzione solo linguistici e nessun compito di istruzione multimodale su larga scala disponibile pubblicamente. MultiInstruct è il primo set di dati di benchmark per il tuning delle istruzioni multimodali che comprende 62 compiti multimodali diversi, che coprono 10 ampie categorie derivate da 21 set di dati open-source esistenti. Ogni compito è dotato di cinque istruzioni scritte da esperti. Il modello base utilizzato per il tuning delle istruzioni multimodali è OFA (One For All), un modello pre-addestrato multimodale unificato che gestisce compiti di comprensione e generazione con singole o multiple modalità, utilizzando un vocabolario unificato per linguaggio, token di immagine e coordinate di riquadri di delimitazione. Il video illustra alcuni esempi di istanze del set di dati MultiInstruct, che mostrano la formulazione di compiti come la didascalia con grounding, la localizzazione del testo, la selezione di espressioni di riferimento e la corrispondenza domanda-immagine in un formato sequenza-a-sequenza.</sample>
    <sample id="124">Il relatore, Tanching from NUS e Alibaba, presenta il loro lavoro sul benchmarking e il miglioramento della capacità di ragionamento temporale dei modelli linguistici di grandi dimensioni (LLM).

Inizialmente, il ragionamento temporale è suddiviso in tre livelli: relazioni tempo-tempo (livello 1), relazioni tempo-evento (livello 2) e relazioni evento-evento (livello 3). Il Livello 1 richiede la comprensione dell'asse temporale, il Livello 2 richiede sia la comprensione del tempo che l'ancoraggio degli eventi al tempo, e il Livello 3 richiede l'ancoraggio temporale di più eventi. La ricerca precedente si è concentrata principalmente sul Livello 2, mentre questo lavoro mira a uno studio più completo di tutti e tre i livelli.

Esperimenti preliminari sui modelli linguistici hanno rivelato un bias a favore degli anni contemporanei, potenzialmente correlato alle frequenze dei termini nei corpora di pre-training. Hanno anche notato che ChatGPT ha performato bene nella previsione annuale, ma ha avuto un calo significativo nella previsione mensile.

Per affrontare queste lacune, hanno proposto il set di dati TempReason, che copre tutti e tre i livelli di ragionamento temporale e ampi archi temporali. Per le domande di Livello 1, la difficoltà è stata aumentata dalla previsione annuale alla previsione mensile. Le domande di Livello 2 e 3 sono state costruite utilizzando la base di conoscenza Wikidata e gli articoli di Wikipedia.

Il lavoro ha anche proposto un nuovo ambiente di valutazione, chiamato "Reasoning QA", oltre ai tradizionali close-book (CBQA) e open-book (OBQA) QA. Nel Reasoning QA, tutta la conoscenza temporale rilevante è fornita agli LLM, che devono quindi ragionare in base alle domande e alla conoscenza temporale per trovare la risposta corretta.

Per migliorare la capacità di ragionamento temporale degli LLM, hanno proposto una strategia di training con due componenti: pre-training di estrazione dello span temporale e apprendimento per rinforzo sensibile al tempo. Il pre-training di estrazione dello span temporale è una strategia di pre-training intermedia per ricostruire span temporali ed entità nel testo grezzo. L'apprendimento per rinforzo sensibile al tempo premia il modello per le previsioni corrette e penalizza quelle temporalmente sbagliate. Il loro modello finale è denotato come TempT5.

I risultati degli esperimenti su TempReason hanno mostrato che la performance di ChatGPT cala significativamente nella previsione mensile di Livello 1 e non è promettente nel ragionamento di Livello 2 e 3, perdendo persino contro un modello FLAN-T5-L significativamente più piccolo nel ragionamento di Livello 2. TempT5, che è stato finemente sintonizzato su TempReason, ha mostrato un miglioramento significativo rispetto a T5-SFT negli ambienti OBQA e ReasonQA. Hanno anche osservato che le performance variano ampiamente tra i diversi periodi di tempo, il che potrebbe essere correlato allo squilibrio nei dati di training.

In conclusione, hanno analizzato sistematicamente ed esposto i bias degli LLM nel ragionamento temporale, proposto un nuovo set di dati che contiene tutti e tre i livelli di ragionamento temporale e periodi di tempo completi, e proposto un framework di training per migliorare la capacità di ragionamento temporale degli LLM.</sample>
    <sample id="125">Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, Pierre-Antoine Gourraud. Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="126">Sì, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico è stato considerato come un approccio standard.</sample>
    <sample id="127">Il video "Large Language Models Are Reasoning Teachers" presenta un lavoro di ricerca che mira ad abilitare la ragionamento della catena di pensiero (CoT) nei modelli linguistici di piccole dimensioni. La ragionamento CoT, introdotta da Wei et al. (2022), consente ai modelli linguistici di risolvere problemi complessi attraverso passaggi intermedi. Tuttavia, questa tecnica è applicabile solo ai modelli più grandi, come GPT-3 e PaLM, il che ne limita l'uso a causa dei requisiti di memoria e calcolo.

Per superare questa limitazione, i ricercatori propongono di utilizzare i modelli più grandi come "insegnanti di ragionamento" per trasferire le loro capacità di ragionamento a modelli più piccoli (da 70M a 6,7B di parametri). Il metodo, chiamato Fine-tune-CoT, consiste nel chiedere al modello insegnante di risolvere i problemi passo dopo passo e, se la previsione finale è corretta, riformattare la soluzione di ragionamento in un campione di addestramento per il modello studente.

Una tecnica innovativa proposta dai ricercatori è il "ragionamento diversificato", in cui vengono generate più soluzioni di ragionamento dal modello insegnante utilizzando il campionamento stocastico della temperatura. Queste soluzioni diversificate vengono poi utilizzate per addestrare ulteriormente il modello studente, migliorando le sue prestazioni.

I risultati mostrano che Fine-tune-CoT consente capacità di ragionamento significative nei modelli di piccole dimensioni, soprattutto per i compiti basati sul testo. Il ragionamento diversificato migliora sostanzialmente le prestazioni, con miglioramenti notevoli in compiti come l'aritmetica multipla. La ricerca evidenzia anche l'alta scalabilità del metodo Fine-tune-CoT e la necessità di considerare i compromessi tra i costi di sviluppo e i costi/qualità di inferenza.

Il video invita gli spettatori a consultare il documento completo per un'analisi dettagliata, inclusa l'emergenza del ragionamento nei piccoli modelli e i risultati su modelli open source. Il codice e i dati di tutti gli esperimenti, compresi i dati di inferenza dell'insegnante di OpenAI, sono disponibili per future ricerche.</sample>
    <sample id="128">I modelli NLU necessitano di integrare le conoscenze disponibili in diversi contesti per la risoluzione dei compiti. Questa presentazione introduce un nuovo dataset diagnostico, KITMUS, per valutare la capacità dei modelli di risolvere i coreference attingendo a più fonti di conoscenza: conoscenza acquisita in fase di pre-addestramento (nelle loro architetture) e conoscenza disponibile al momento dell'inferenza (nel testo). Per ogni coreference, è necessaria sia una conoscenza specifica dell'entità che una conoscenza di fondo (per esempio, i giudici decidono i casi in tribunale). La conoscenza di fondo può essere disponibile in fase di pre-addestramento o in fase di inferenza (o entrambe), mentre la conoscenza specifica dell'entità è disponibile solo in fase di inferenza. Il dataset contiene scenari in cui la conoscenza specifica dell'entità e la conoscenza di fondo sono fornite in diverse combinazioni. Questi scenari includono: conoscenza di fondo da pre-addestramento, conoscenza di fondo da entrambi i contesti di pre-addestramento e inferenza, e conoscenza di fondo da inferenza. I risultati degli esperimenti con i modelli di risoluzione dei coreference mostrano che i modelli possono integrare con successo la conoscenza da più fonti, ma solo se sono stati addestrati su un set di dati specifico del compito. Tuttavia, anche i modelli migliori faticano a integrare la conoscenza di fondo presentata solo in fase di inferenza.</sample>
    <sample id="129">Un guerriero è non contrassegnato, ma un guerriero donna è contrassegnato.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono gli LSTM con o senza CRF, come Flair e ELMo.</sample>
    <sample id="131">Non vengono forniti i nomi dei set di dati di test.</sample>
    <sample id="132">Ci sono 6 autori coinvolti nell'articolo.</sample>
    <sample id="133">L'autore opera con più modalità, ovvero testo e immagini.</sample>
    <sample id="135">I relatori, James Finch e Sarah Finch, illustrano il nuovo approccio ABC-Eval, che utilizza l'annotazione per valutare i sistemi di dialogo orientati alle chat. L'obiettivo principale di ABC-Eval è quello di ridurre la soggettività intrinseca della valutazione umana, fornendo una valutazione più precisa e affidabile.

Invece di affidarsi a valutazioni o comparazioni generali della qualità del dialogo, ABC-Eval si concentra sull'annotazione dei comportamenti specifici delle risposte dei modelli di chat. Ciò include l'identificazione di risposte irrilevanti, contraddizioni (siano esse auto-contraddizioni o contraddizioni con il partner), allucinazioni di fatti errati, violazioni del buon senso e la presenza o l'assenza di empatia.

Gli esperimenti condotti su quattro modelli di dialogo open-domain, con 100 conversazioni uomo-bot per modello, hanno dimostrato che le etichette comportamentali di ABC-Eval sono più affidabili rispetto a quelle raccolte con metodi esistenti, misurate dall'accordo inter-annotatore. Inoltre, le etichette di ABC-Eval sono più predittive della qualità complessiva della conversazione. Ad esempio, la misurazione delle contraddizioni spiega il 5-10% della qualità della conversazione, mentre i punteggi Likert medi per la coerenza spiegano solo il 4% o meno.

I risultati mostrano che i modelli attuali presentano ancora sfide significative in aree come le violazioni del buon senso (circa il 20% delle risposte), le informazioni irrilevanti (circa il 15%) e le contraddizioni (circa il 10%). ABC-Eval offre una risoluzione maggiore per valutare l'IA conversazionale, contribuendo a identificare e quantificare con precisione questi errori, facilitando così i futuri progressi nel campo.</sample>
    <sample id="136">Il video presenta FERMAT, un set di valutazione flessibile basato su tipi aritmetici, che mira a fornire un'alternativa più informativa ai metodi di valutazione esistenti per il ragionamento numerico. La motivazione di questo lavoro nasce dal fatto che molte attività a valle richiedono il ragionamento numerico per la correttezza fattuale, ad esempio il fact-checking e la generazione di testi. I modelli linguistici più grandi tendono a fornire prestazioni migliori, ma c'è bisogno di una migliore comprensione del comportamento dei modelli più piccoli, accessibili, come quelli da 3 miliardi di parametri. Gli attuali benchmark tendono a fornire punteggi singoli come l'accuratezza o la misura F1, che sono poco informativi sui punti di forza e di debolezza dei modelli in termini di abilità matematiche.

FERMAT affronta queste limitazioni esaminando tre aspetti: la comprensione dei numeri, le operazioni matematiche e la dipendenza dall'allenamento. Per la comprensione dei numeri, FERMAT modifica la rappresentazione dei numeri (ad es. da "5" a "5.0" o "cinque") e varia la dimensione dei numeri (grandi, piccoli, decimali) per testare l'intervallo e la profondità delle capacità dei modelli. Per le operazioni matematiche, valuta le prestazioni dei modelli con operazioni a uno e due salti. I risultati della valutazione zero-shot mostrano che la maggior parte dei modelli esistenti si comporta male su questi diversi aspetti. Il fine-tuning con modelli matematici generati da insegnanti, che utilizzano segnaposto numerici e forniscono espressioni attese, migliora notevolmente le prestazioni su questi aspetti. Per quanto riguarda la dipendenza dall'allenamento, lo studio esamina come i modelli si comportano quando viene esatta l'espressione o quando i numeri e le operazioni sono presenti nell'allenamento. Si scopre che l'esatta corrispondenza delle espressioni non garantisce alte prestazioni, indicando che i modelli non si limitano a memorizzare le espressioni. Infine, l'introduzione di diversità linguistica e matematica nei template di allenamento migliora ulteriormente le prestazioni dei modelli. In sintesi, i benchmark esistenti sono poco rappresentativi, e FERMAT fornisce un'alternativa più informativa per la valutazione. La diversità linguistica e matematica è fondamentale e l'encoding e la tokenizzazione dei numeri sono aree di miglioramento.</sample>
    <sample id="137">Questa ricerca introduce un nuovo compito di generazione di progetti guidati dal linguaggio, focalizzandosi inizialmente sul dominio delle planimetrie. Il progetto ha creato Tell2Design (T2D), un dataset su larga scala che contiene planimetrie associate a istruzioni in linguaggio naturale che descrivono le preferenze degli utenti.

Il compito consiste nel generare planimetrie 2D ragionevoli che rispettino le istruzioni fornite. Le istruzioni in linguaggio naturale caratterizzano i componenti intrinseci della planimetria corrispondente, includendo semantica (tipo e funzionalità di ogni stanza), geometria (forma e dimensione di ogni stanza) e topologia (relazioni tra le diverse stanze). Il dataset Tell2Design comprende 5.051 istruzioni umane annotate e 75.737 istruzioni generate artificialmente da modelli predefiniti.

Le principali sfide del compito includono: 1) la generazione di progetti sotto vincoli più rigorosi rispetto alla generazione di opere d'arte testuale-condizionale; 2) la comprensione di informazioni complesse e ambigue da testi non strutturati a livello di documento; 3) la gestione di istruzioni umane ambigue, incomplete o fuorvianti.

L'approccio proposto trasforma la generazione di planimetrie in un problema Seq2Seq utilizzando un framework LLM encoder-decoder. Le bounding box delle stanze vengono ricostruite in una sequenza target strutturata. Il modello è inizializzato con T5, un modello linguistico pre-addestrato, per una migliore comprensione del linguaggio.

I risultati sperimentali mostrano che l'approccio Seq2Seq supera significativamente i modelli di baseline di generazione di immagini testuali-condizionali, con punteggi IOU più alti. Si è riscontrato che sia le istruzioni artificiali che quelle umane sono reciprocamente vantaggiose durante l'addestramento, indicando che nonostante le differenze linguistiche, entrambi i tipi di dati contribuiscono a migliorare le prestazioni del modello. Questo progetto pone le basi per future ricerche sulla generazione di progetti guidati dal linguaggio.</sample>
    <sample id="138">I modelli NLU fanno fatica a integrare le conoscenze di base fornite solo durante il tempo di inferenza.</sample>
    <sample id="139">I relatori sono Ying Shen e Zhiyang Xu.</sample>
    <sample id="140">Sì, per garantire la qualità del set di dati di validazione e test, i lavoratori di crowdsourcing sono stati incaricati di trovare e rivedere i campioni errati.</sample>
    <sample id="141">Le risorse esistenti supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché si basano sulla conoscenza del dominio e sulla curatela umana.</sample>
    <sample id="142">Hi. I'm going to talk about our work on resolving indirect referring expressions for entity selection. In which we introduce the AltEntities Corpus. My name is Javad Hosseini, and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users' language when they make a choice. Consider this alternative question: Did you mean "Easy On Me" or "I Gotta Feeling?" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song, "Easy On Me," or its position, "the first one." But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some example indirect references, for example, "the newer one," or "the song that's not energetic." This is an important problem in conversational systems and also for benchmarking LLM's entity understanding. We're not aware of a public dataset, a large-scale public dataset, for the task, so we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes. Our dataset collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialog context. In the second speech bubble, Alice says, "Do you mean 'Easy On Me' or 'I Gotta Feeling'?" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, "the newer one." We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template: "Do you mean A or B?" Where A and B are sampled from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles. For example, two books with the name "The Return." The third one is when they have similar descriptions on Wikipedia. And finally, when they have similar infoboxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show these alternative questions to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song. And then ask the annotators to listen to at least some of each song and read about each song. Here's for example the Google search result for the song "Easy On Me." For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions. For example, "the one with the piano music." Here are some examples from our dataset. For example, "the one without words," "not the one with the 12-year-old boy," or "the fictional one," or "comes from Azerbaijan," and so on. The AltEntities Corpus has 6,000 alternative questions across the three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model (accuracy) are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92% to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82% to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%. So there's a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks. Ciao. Parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità. Nel quale introduciamo il corpus AltEntities. Il mio nome è Javad Hosseini, e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando fanno una scelta. Consideriamo questa domanda alternativa: "Volevi dire 'Easy On Me' o 'I Gotta Feeling'?" Qui, un utente vuole scegliere tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, per esempio, dicendo il nome della canzone, "Easy On Me," o la sua posizione, "la prima." Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone o le pronunce sono troppo simili tra loro e difficili da distinguere. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, per esempio, "quella più recente," o "la canzone che non è energica." Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità dei LLM. Non siamo a conoscenza di un dataset pubblico, un dataset pubblico su larga scala, per questo compito, quindi ne raccogliamo uno usando l'annotazione collaborativa. Il nostro dataset copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando una configurazione di completamento di fumetti. Il fumetto ha tre fumetti. Nel primo fumetto, Bob dice, "Ricordi quella canzone che stavamo ascoltando ieri?" E con questo, Bob imposta il contesto del dialogo. Nel secondo fumetto, Alice dice, "Intendi 'Easy On Me' o 'I Gotta Feeling'?" Che è la domanda alternativa. E nel terzo fumetto, Bob usa un riferimento indiretto per selezionare una di queste entità. Per esempio, "quella più recente." Forniamo automaticamente i primi due fumetti, ma il terzo viene compilato dall'annotatore. Il primo fumetto è scelto tra alcune richieste manuali per ogni dominio. Il secondo, che è la domanda alternativa, viene generato come segue. Usiamo sempre un modello semplice: "Intendi A o B?" Dove A e B sono campionati da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci muoviamo più in alto nella lista, le entità diventano più simili tra loro, ed è solitamente più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili. Per esempio, due libri con il nome "Il Ritorno." Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno infobox o attributi simili su Wikipedia. Per esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo queste domande alternative agli annotatori, conoscono il nome di queste entità, ma non necessariamente conoscono le entità. Quindi quello che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone. E poi chiediamo agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere su ogni canzone. Ecco, per esempio, il risultato della ricerca Google per la canzone "Easy On Me". Per i domini delle ricette e dei libri, mostriamo del testo di sottofondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come sono. Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio, qui la prima, e di descriverle usando da tre a cinque espressioni di riferimento indiretto. Per esempio, "quella con la musica di pianoforte." Ecco alcuni esempi del nostro dataset. Ad esempio, "quella senza parole", "non quella con il bambino di 12 anni", o "quella fittizia", o "viene dall'Azerbaigian", e così via. Il corpus AltEntities ha 6.000 domande alternative nei tre domini, e contiene 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL (precisione) sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, la precisione è molto alta. È circa dal 92% al 95%. Ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di base parzialmente sovrapposte, la precisione è tra l'82% e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, la precisione è solo del 60%. Quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili al dominio. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio è confrontato con la strategia wait-k e l'allineamento locale.</sample>
    <sample id="144">Le affiliazioni degli autori sono: LIA, Avignon Université, LS2N, Nantes Université, Clinique des données, CHU de Nantes e Zenidoc.</sample>
    <sample id="145">La relatrice è Jenny T. Liang.</sample>
    <sample id="146">Ecco il riassunto del contenuto inglese:

Il video presenta uno studio sull'omissione nella riassunzione del dialogo, un sottocompito della riassunzione del testo che produce riassunti per dialoghi in vari scenari come il servizio clienti, la consultazione medica e i verbali delle riunioni. Nonostante i progressi nella riassunzione del dialogo con i modelli linguistici pre-addestrati su larga scala, gli errori di omissione rimangono un problema importante, portando a riassunti incompleti e alla perdita di fatti critici. L'analisi condotta nel video rivela che l'omissione è un problema serio e diffuso, con circa il 70% dei riassunti generati che presentano questo problema. Inoltre, le informazioni omesse sono distribuite in modo casuale in tutte le posizioni del dialogo, indipendentemente dalla loro lunghezza o dal loro dominio, il che significa che i dialoghi sono non strutturati e l'identificazione delle informazioni chiave è ancora difficile per i modelli attuali.

Per affrontare e risolvere il problema dell'omissione, gli autori propongono un nuovo compito chiamato "rilevamento dell'omissione", il cui obiettivo è prevedere quali frasi di un dialogo sono omesse in un riassunto candidato. Per supportare questo compito, hanno costruito un nuovo dataset, "OLDS" (Omission Labels for Dialogue Summarization), basato su cinque benchmark esistenti e coprendo cinque domini. Questo dataset fornisce etichette di omissione di alta qualità generando 10 riassunti candidati per ogni dialogo utilizzando diversi modelli astrattivi e strategie di decodifica, e quindi etichettando automaticamente le omissioni con convalida umana.

I risultati delle baseline di rilevamento dell'omissione utilizzando classificazioni a coppie, etichettatura di sequenze e reti puntatore indicano che l'attività è molto impegnativa, con punteggi F1 intorno al 50%. Tuttavia, i risultati del perfezionamento del riassunto basato sull'omissione utilizzando un metodo di post-editing mostrano che l'incorporazione delle omissioni migliora notevolmente la qualità del riassunto, suggerendo che il rilevamento dell'omissione è un compito prezioso e un passo promettente per migliorare la qualità della riassunzione del dialogo.</sample>
    <sample id="147">Sono coinvolti tre autori.</sample>
    <sample id="148">00:00:00 - Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler. E introdurrò brevemente il documento "Attention as a Guide for Simultaneous Speech Translation" che è un lavoro con Matteo Negri e Marco Turchi.
00:16:51 - Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato o SimuIST è il processo di traduzione del parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica.
00:30:86 - E quali sono i problemi degli attuali modelli SimuIST? Le architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complicate, ad esempio, l'addestramento che coinvolge diversi obiettivi di ottimizzazione e l'addestramento e il mantenimento di diversi modelli per raggiungere diversi regimi di latenza, ad esempio, l'addestramento di un modello con una latenza media di un secondo e un altro con una latenza di due secondi e così via.
01:03:97 - Quindi, qual è la nostra soluzione? Per prima cosa, utilizzare modelli ST offline già esistenti senza riaddestrare o adottare un'architettura specifica per SimuIST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici e sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, cioè il meccanismo di attenzione trasversale. E si può vedere un esempio sulla destra.
01:35:10 - La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, ed è una strategia per la quale decidiamo se emettere o meno una traduzione parziale basata su dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè la sua somma è inferiore a una certa soglia alfa verso gli ultimi lambda frame vocali, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un frammento di parlato contenente "Sto per parlare di..." e il nostro modello predice la traduzione in tedesco, e andremo a guardare i pesi di attenzione trasversale, vedremo che le prime due parole puntano ai primi frame vocali ricevuti, mentre l'ultima parola punta agli ultimi frame vocali ricevuti, gli ultimi lambda frame vocali. Questo significa che le prime due parole saranno emesse, mentre, poiché la somma dell'attenzione trasversale è superiore a una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di parlato. Se andiamo avanti e riceviamo un altro frammento di parlato, e il nostro modello predice altre tre parole, e andremo a guardare i pesi di attenzione trasversale, vedremo che nessuna parola punta agli ultimi lambda frame vocali. Questo significa che queste tre parole saranno emesse.
03:10:76 - Se guardiamo i risultati principali di EDAtt, plotteremo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo il BLEU, da un lato, che misura la qualità della traduzione, e il lagging medio che è la misura della latenza. E consideriamo anche il lagging medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per predire l'output. Quindi, vogliamo che le nostre curve siano il più alte possibile su questo grafico, ma vogliamo anche che siano spostate a sinistra. E confrontiamo con strategie popolari che sono applicate anche a modelli offline, che sono la strategia "wait-k" e l'allineamento locale. E confrontiamo anche con l'architettura all'avanguardia specificamente adattata per la traduzione simultanea del parlato. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco. E vediamo che EDAtt supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra e vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo consapevole del calcolo, EDAtt è la strategia più veloce.
04:39:69 - Se volete scoprire altri risultati, leggete il nostro articolo e abbiamo anche rilasciato open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="150">Archiki Prasad presenta "MeetingQA: Extractive Question-Answering on Meeting Transcripts," un lavoro congiunto di UNC Chapel Hill e Adobe Research. Le trascrizioni delle riunioni sono lunghe, specifiche per dominio e ricche di informazioni, ma i lavori precedenti si sono concentrati solo sulla riassunzione e sull'estrazione degli elementi d'azione. MeetingQA introduce un dataset di QA estrattivo basato su domande poste dai partecipanti alle riunioni e sulle relative frasi di risposta. Le domande sono più lunghe, aperte e orientate alla discussione, con risposte che spesso coinvolgono più relatori e span non consecutivi, oltre a domande retoriche.

Il dataset è stato creato da trascrizioni pubbliche del corpus AMI, che comprende circa 100 ore di riunioni multipartitiche trascritte manualmente. Le domande sono state selezionate in base alla punteggiatura e alla lunghezza. Gli annotatori sono stati reclutati per etichettare le frasi all'interno della risposta, ottenendo un alto accordo inter-annotatore (Krippendorff's α = 0.73). MeetingQA include 7.735 domande da 166 riunioni, suddivise in set di addestramento, sviluppo e test. Circa il 30% delle domande non ha risposta, il 40% ha risposte multi-span (non consecutive) e il 48% ha risposte multi-speaker. Il 54,4% delle domande è del tipo sì/no, il 50% cerca opinioni e il 20% è di natura retorica. Il 70% delle risposte multi-speaker contiene un disaccordo.

I modelli attuali mostrano un divario di 25 punti F1 rispetto alla performance umana nel setting di fine-tuning e un divario di 50 punti F1 nel setting zero-shot. I modelli a contesto breve (es. RoBERTa) superano leggermente quelli a contesto lungo (es. Longformer). I modelli multi-span hanno performance leggermente inferiori o comparabili ai modelli single-span. L'aumento dei dati con annotazioni "silver" da MediaSum è efficace. I modelli faticano a identificare domande retoriche e quali relatori rispondono a una domanda, specialmente nel setting zero-shot. Le previsioni single-span contengono più frasi irrilevanti.</sample>
    <sample id="151">Hello everyone. My name is Ying and my colleague Zhiyang and I will be presenting our research on multi instruct, improving multi-modal zero-shot learning while instruction tuning. So, with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only task, where computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction datasets between NLP and multi-modal. There exists more than 1600 language-only instruction tasks. However, there is no large scale publicly available multi-modal instruction task. Therefore, this motivate us to build a multi-modal instruction tuning dataset. Here, we present MULTIINSTRUCT, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks, covering 10 broad categories. These tasks are derived from 21 existing open source dataset and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model as our base model. OFA use a unified vocabulary for language, image tokens and the coordinate of a bounding box. Here, we show some example instances from our MULTIINSTRUCT dataset. To unify the processing of a various input and output data type, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So, for the training dataset, we use 53 task from 9 groups for training and we sample 10,000 instance per task. Uh for testing, we reserve the entire common sense reasoning group for testing and we select additional five task from VQA and miscellaneous group. We use all the instance in the test split for each task. In addition, we randomly sample 20 task from the test split of natural instruction dataset as unseen tasks for NLP. Uh so, we use a pre-trained OFA-large model as the base model. Uh during training, we mix all the instance for all the tasks. Uh each instance is randomly combined with one of its five instruction template. Uh so, during test, for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and max performance and the standard deviation of the performance across all five experiments. Uh if the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. Uh for NLP task, we report Rouge-L as well. We also introduced additional evaluation metric called sensitivity. So, this measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variation in the wording of the instruction. Here is our main result. As we can see, uh instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also transfer learning from natural instruction dataset uh can benefit uh instruction tuning. Uh here we can see as the amount of task increase, the model achieve better performance and in the meantime, uh lower sensitivity. Uh so, we also did one experiments, uh we use one instruction versus five instruction. As we can see, uh using more instruction can improve the model's overall performance and reduce its sensitivity a lot. Uh so, this shows the effect of different fine-tuning strategy on the model sensitivity. Uh as we can see, by transfer learning from natural instruction dataset, the model can uh achieve much better sensitivity comparing to the original OFA model. Uh we also can see transfer learning from natural instruction dataset uh can help OFA to achieve much better performance on the natural instruct uh dataset. So, overall, we have proposed the first large scale multi-modal instruction tuning dataset. It contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. And we explore different transfer learning technique and show their benefits. Uh we design a new metric called sensitivity. Uh so, one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. Uh this is a QR code for our uh data and the model. Thank you.</sample>
    <sample id="152">Il presentatore si presenta e parla della sua ricerca all'intersezione tra l'elaborazione del linguaggio naturale (NLP) e la filologia classica, intitolata "Exploring Large Language Models for Classical Philology". L'obiettivo è esplorare l'impatto della multilinguismo nei modelli linguistici. Vengono citati diversi modelli BERT per il latino e il greco antico sviluppati tra il 2020 e il 2022.

Tuttavia, il presentatore evidenzia le limitazioni dei modelli BERT, in quanto sono modelli encoder-only e monolingui. Inoltre, la loro valutazione è spesso insufficiente. Per superare questi problemi, l'obiettivo del loro progetto è rendere confrontabili i modelli esistenti, migliorare lo stato dell'arte, esplorare diverse architetture di modelli e introdurre modelli multilingui.

Per il greco antico sono stati pre-addestrati due modelli monolingui: GreBERTa (modello RoBERTa) e GreTA (modello encoder-decoder basato sull'architettura T5). Per il multilinguismo sono stati sviluppati PhiLBERTa e PhiLTa, pre-addestrati su dati in greco antico, latino e inglese. I modelli sono stati addestrati su una grande quantità di dati, incluse nuove risorse ottenute dall'Internet Archive.

I modelli sono stati valutati su tre compiti principali: tagging PoS (Part-of-Speech), parsing delle dipendenze e lemmatizzazione. I risultati mostrano che i loro modelli superano chiaramente lo stato dell'arte per entrambi il greco antico e il latino. Inoltre, l'encoder del modello T5 si comporta in modo diverso dai modelli encoder-only nativi, ma raggiunge prestazioni simili dopo un addestramento più lungo. Per la lemmatizzazione, i loro modelli ottengono un aumento impressionante di 5 punti percentuali rispetto allo stato dell'arte esistente per il greco antico e miglioramenti significativi anche per il latino.

Riguardo alla conoscenza semantica e del mondo, i loro modelli superano significativamente i modelli precedenti. Tuttavia, non c'è una differenza significativa tra le prestazioni dei modelli multilingui e monolingui in questi compiti.

In conclusione, hanno presentato nuovi e potenti modelli linguistici per la filologia classica, inizializzati da zero, con architetture encoder-only e encoder-decoder, e modelli multilingui. Hanno introdotto un dataset di pre-addestramento di alta qualità per il greco antico e hanno valutato rigorosamente i modelli esistenti e i propri, ottenendo risultati all'avanguardia.</sample>
    <sample id="153">Ninareh Mehrabi presenta un lavoro sulla risoluzione delle ambiguità nei modelli generativi text-to-image. Il lavoro affronta il problema delle ambiguità nei prompt forniti ai modelli text-to-image, illustrando esempi come "un elefante e un uccello che vola", che può avere diverse interpretazioni visive, o "la ragazza entra nella stanza con i fiori", dove l'ubicazione dei fiori non è chiara.

Per affrontare questo problema, il team ha sviluppato un framework chiamato Text-to-ImagE Disambiguation (TIED). Il framework si basa su un benchmark di dati, il Text-to-image Ambiguity Benchmark (TAB), una versione modificata del corpus LAVA, che copre vari tipi di ambiguità (sintassi, discorso, fairness, ecc.).

Il framework TIED opera in due modalità principali:
1.  **QA-TIED (Question Answering - TIED)**: Il modello linguistico (GPT-neo) genera una domanda chiarificatrice basata sul prompt ambiguo. L'utente risponde alla domanda in base alla sua intenzione, e questa risposta viene concatenata al prompt originale per creare un prompt disambiguato.
2.  **VS-TIED (Visual Setup - TIED)**: Invece di porre domande, il modello genera diverse configurazioni visive possibili per il prompt ambiguo. L'utente sceglie la configurazione che corrisponde alla sua intenzione, e il prompt selezionato viene utilizzato per generare l'immagine.

Per valutare se le immagini generate sono fedeli all'intenzione dell'utente, viene utilizzato un modello VQA (Visual Question Answering). Se il modello VQA risponde "sì" alla domanda sull'intenzione dell'utente, l'immagine è considerata fedele.

I risultati principali dello studio indicano che esiste una disparità nella risoluzione delle ambiguità tra i diversi tipi di ambiguità. La disambiguazione, tramite il framework proposto, ha un effetto complessivamente positivo sulla fedeltà della generazione. Inoltre, le valutazioni automatiche mostrano un ragionevole accordo con le valutazioni umane, suggerendo l'affidabilità del metodo di valutazione automatica.

In conclusione, questo lavoro studia le ambiguità nei modelli text-to-image, curando un benchmark specifico e proponendo framework per mitigare e valutare tali ambiguità nei prompt.</sample>
    <sample id="154">Gli autori dell'articolo sono affiliati con l'Università di Trento e la Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome del relatore è Javad Hosseini.</sample>
    <sample id="157">Questo video presenta un nuovo lavoro sulla **summarizzazione del dialogo con grafo di fusione della struttura statico-dinamica**. 

La summarizzazione del dialogo è un compito di PNL fondamentale per **distillare le informazioni salienti da un contesto di dialogo in un riassunto conciso**. Può aiutare le persone a cogliere rapidamente i punti salienti di un dialogo semi-strutturato e multi-partecipante senza dover rivedere l'intero testo. I metodi esistenti di solito si basano su strumenti linguistici esterni e su grafi pre-calcolati, che soffrono di **propagazione degli errori** e **mancanza di adattabilità dinamica** al compito di summarizzazione.

Il metodo proposto, **SDDS**, affronta queste limitazioni con quattro componenti principali:
1. **Utterance Encoder**: Codifica le singole espressioni in rappresentazioni vettoriali.
2. **Static Graph Construction**: Cattura le relazioni tra le espressioni usando un grafo per il parsing del discorso, un grafo di co-occorrenza di parole chiave e un grafo di relazione del parlante. Questo fornisce informazioni strutturali predefinite.
3. **Static-Dynamic Graph Module**: Si compone di due parti:
    - **Static Graph Fusion**: Integra più grafi statici usando strati convoluzionali 1x1 per creare una rappresentazione di relazione fusa.
    - **Dynamic Graph Module**: Utilizza un modello di attenzione multi-testa per catturare le relazioni semantiche dinamiche tra le espressioni basate sulle loro rappresentazioni vettoriali profonde, senza dipendere da metodi euristici pre-calcolati.
4. **Summary Generator**: Incorpora la rappresentazione del grafo unificata (fusione del grafo statico e dinamico) nel processo di generazione del riassunto usando un meccanismo di attenzione incrociata duale.

L'SDDS mira a creare un **modello più robusto ed efficace** per la summarizzazione del dialogo, che sia meno dipendente da strumenti esterni e più adattivo. Il codice e i dati sono disponibili su GitHub.</sample>
    <sample id="158">La risoluzione della coreferenza nei documenti lunghi è un compito difficile a causa della complessità quadratica delle architetture convenzionali e dell'alta percentuale di cache miss nei modelli basati su cache. Per risolvere questo problema, è stato proposto un modello Dual Cache. Questo modello utilizza una cache locale (L-cache) e una cache globale (G-cache) per memorizzare le entità. L'L-cache utilizza la politica LRU (Least Recently Used) per archiviare le entità locali, mentre la G-cache utilizza la politica LFU (Least Frequently Used) per le entità globali.

Quando viene incontrata una nuova menzione, il modello determina se essa rappresenta una nuova entità o appartiene a un'entità già presente nella cache. Se si tratta di una nuova entità o di un'entità aggiornata, la sua frequenza viene valutata. Se la frequenza supera quella di un'entità nella G-cache, viene trasferita nella G-cache. Se la G-cache è piena, viene avviato un processo di espulsione. In caso contrario, l'entità viene aggiunta all'L-cache, e se quest'ultima raggiunge la sua capacità massima, viene avviato un altro processo di espulsione.

I risultati degli esperimenti su quattro benchmark pubblici hanno dimostrato che Dual Cache supera i modelli baseline, anche quelli con memoria non vincolata, quando sono disponibili dati di addestramento. In particolare, per i documenti a livello di libro, la differenza di prestazioni tra Dual Cache e i modelli baseline è ancora più evidente. Inoltre, Dual Cache ha ridotto significativamente le cache miss rispetto ai modelli a singola cache. In sintesi, Dual Cache offre un migliore rapporto prestazioni/costo rispetto ai metodi a singola cache, dimostrandosi più efficiente ed efficace per la risoluzione della coreferenza nei documenti lunghi.</sample>
    <sample id="159">00:00
Ciao a tutti. Sono Koustuv Sinha e sono lieto di darvi il benvenuto alla nostra presentazione del nostro documento ACL 2023, "I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto". Questo è un lavoro congiunto con Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy e Adina Williams.
00:19
Quindi, in questo lavoro rivisitiamo il paradigma della coppia minima. Quindi, il paradigma della coppia minima valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono anche includere la grammaticalità, come BLiMP SyntaxGym, o l'accettabilità in termini di stereotipi, come le coppie CrowS. E in questo paradigma della coppia minima, il modo tipico per valutare i modelli linguistici è che si mostra una frase accettabile o una frase grammaticale e poi si mostra una frase inaccettabile o una frase non grammaticale, e poi la speranza è che il modello fondamentalmente metta più probabilità sulla frase accettabile.
01:00
L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe. In questi giorni, i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe, quindi è cruciale che valutiamo l'accettabilità dei modelli lungo l'intera finestra di contesto. E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe.
01:32
Quindi, questo è l'approccio. Quindi, quello che facciamo è che per simulare queste sequenze più lunghe, rivisitiamo gli stessi set di dati e poi ricreiamo frasi scegliendo, come, frasi accettabili o inaccettabili da quei set di dati. Quindi, per esempio, qui abbiamo scelto, come, una coppia tipica di grammaticalità dal set di dati BLiMP, dal caso Adjunction Island. E quello che facciamo è che per ricreare, come, sequenze più lunghe, e che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali da Adjunction Island e poi le aggiungiamo come prefisso sia alla query accettabile che alla query inaccettabile.
02:18
Possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza, e questo potrebbe anche essere usato per testare l'accettabilità del modello. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Quindi, questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, cioè se il contesto proviene da un diverso sottoinsieme del set di dati o se è completamente irrilevante per la frase che stiamo esaminando.
03:15
Quindi, come si comporta il modello? Per prima cosa, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query corrente. E lì troviamo che i giudizi MPP sono per lo più robusti per una lunghezza di contesto arbitraria. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E lì abbiamo visto, qui nella linea punteggiata arancione, che i giudizi MPP sono relativamente stabili.
03:42
Ora cosa succede quando scegliamo frasi dallo stesso set di dati? Quindi, qui stiamo scegliendo o creando frasi dai domini accettabili e inaccettabili, dallo stesso set di dati BLiMP o SyntaxGym, e lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili.
04:07
Ma quando facciamo corrispondere la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento massiccio o una diminuzione massiccia dei giudizi MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora, questo effetto è molto grande, questo effetto aumenta lungo l'intera lunghezza del contesto, e questo probabilmente influenzerà i modelli linguistici più recenti che hanno finestre di contesto ampie.
04:41
Quindi, perché i prefissi corrispondenti influenzano così tanto i giudizi del modello linguistico? Quindi, abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante, ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, troviamo che nessuno di questi rumori sta effettivamente facendo cambiare rotta al modello in termini di come ci mostra il trend dei giudizi MPP.
05:11
Fondamentalmente, troviamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni, e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile.
05:32
Quindi, i principali risultati del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintattiche/semantiche latenti condivise tra le frasi, e le valutazioni MPP con input brevi e a frase singola non catturano completamente la conoscenza astratta dei modelli linguistici. Si prega di leggere il nostro articolo per maggiori dettagli sui nostri esperimenti. Grazie per l'ascolto.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in un multi-set non ordinato di token che appariranno nell'output.</sample>
    <sample id="161">Ci sono 55.000 script rappresentati in Coscript.</sample>
    <sample id="163">MASAlign ha ottenuto i migliori risultati.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato riduce il collo di bottiglia dell'annotazione.</sample>
    <sample id="165">Il documento presenta un nuovo approccio al ragionamento abduttivo del senso comune, sfruttando le spiegazioni reciprocamente esclusive. Il ragionamento abduttivo implica l'identificazione di una spiegazione plausibile che colmi il divario informativo tra un contesto dato e un risultato. Gli approcci attuali al ragionamento abduttivo si basano principalmente su metodi supervisionati, che richiedono l'annotazione di spiegazioni plausibili, un processo che può essere rumoroso e soggettivo. Infatti, un esperimento di ragionamento ha rivelato che i collaboratori esterni non sono d'accordo tra loro sul 62,34% di oltre 1000 spiegazioni.

Il documento introduce un metodo di apprendimento non supervisionato chiamato LiPoR (Likelihood learning with Posterior Regularization), che affronta la sfida di imparare il ragionamento abduttivo senza supervisione su quali spiegazioni siano plausibili. In LiPoR, le spiegazioni sono trattate come una variabile latente. L'obiettivo non supervisionato massimizza la probabilità marginale dell'esito Y, dato il contesto X, marginalizzando tutte le possibili spiegazioni in Z. Questo significa che la massimizzazione dell'obiettivo non supervisionato non richiede di sapere quali spiegazioni siano plausibili.

Il documento ha anche evidenziato la necessità di un regolarizzatore per preservare le spiegazioni plausibili. Questo regolarizzatore si basa su una caratteristica significativa delle spiegazioni: la loro mutua esclusività. Per esempio, se una spiegazione è "Il suo volo è stato ritardato", un'altra spiegazione "Il suo volo è partito in orario" viene automaticamente esclusa. Il regolarizzatore LiPoR mira a imporre questa mutua esclusività tra le spiegazioni.

I risultati di LiPoR su αNLI, il dataset di ragionamento abduttivo più utilizzato, mostrano che supera tutti i modelli zero-shot, incluso una solida base zero-shot basata su GPT-3, di oltre 4 punti assoluti in precisione.</sample>
    <sample id="166">Il presentatore, Yuxin Li, di Harbin Institute of Technology, Shenzhen, introduce il suo nuovo lavoro, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text."

Questo compito di recupero di immagini da testi complessi a livello linguistico è impegnativo perché le immagini sono altamente simili e le descrizioni sono lunghe. I metodi tipici, come i modelli visivo-linguistici, ottengono buoni risultati nel recupero di immagini-frasi, ma le loro prestazioni calano drasticamente quando si trovano di fronte a testi linguisticamente complessi.

Per affrontare questo problema, il presentatore si è ispirato alla strategia "divide-and-conquer" e alla teoria del duplice processo. La strategia "divide-and-conquer" risolve un problema ampio suddividendolo in problemi più piccoli, risolvendo i sottoproblemi e combinandoli per ottenere l'output desiderato. La teoria del duplice processo afferma che il cervello umano contiene due sistemi di pensiero: il Sistema 1 per il ragionamento analogico e il Sistema 2 per il ragionamento logico.

Il presentatore propone un nuovo metodo chiamato NDCR. Il Sistema 1 di NDCR è un Interattore Visivo-Linguistico che mira a eseguire l'interazione di informazioni visivo-proposizionali, simile al Sistema 1 nel cervello umano. Il Sistema 2 di NDCR è un Ragionatore Neuro-Simbolico, responsabile dell'integrazione degli stati di ragionamento e dei risultati delle proposizioni semplici per ottenere la soluzione finale della proposizione complessa sulle immagini.

Il presentatore mostra i risultati sperimentali, in cui il metodo proposto NDCR supera le baseline. Il presentatore presenta due casi studio che dimostrano la capacità di NDCR di presentare gli stati di inferenza e i risultati di inferenza nei passaggi intermedi.

Infine, il presentatore conclude che il calcolo neuro-simbolico può essere un approccio utile per migliorare le capacità di ragionamento composizionale e di pianificazione dei modelli linguistici di grandi dimensioni. La strategia "divide-and-conquer" è simile al ragionamento a catena di pensiero auto-interrogante, che mira a scomporre il ragionamento complesso in problemi semplici e a costruire il percorso di ragionamento. Entrambi sono efficaci per risolvere problemi complessi. La teoria del duplice processo potrebbe essere integrata con la strategia "divide-and-conquer".</sample>
    <sample id="167">La DEplain-web è stata allineata sia manualmente che automaticamente.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo le notizie di Reuters dal 2020 e annotandole con le stesse linee guida di annotazione di CoNLL-2003.</sample>
    <sample id="169">Hello everyone. My name is David Vilar Torres, and I will be giving a short overview of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PaLM is a 540 billion parameter large language model, presented last year, in 2022. It is trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks.

In this work, we present the first systematic study of LLM prompting for machine translation. We evaluate the translation capability of such models, using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model, and we compare to state-of-the-art systems, the best performing systems of the WMT evaluation. We use state-of-the-art neural MT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies.

The prompting has a big influence on the performance of the of LLMs for translation, as we can see in a simple experiment, where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences (516 out of 1000) show a difference of more than one BLEURT point. And this can go in extreme cases up to 40 BLEURT points. So, it's important to select a good prompting strategy.

In our experiments, we settle for a five-shot prompting strategy, where we just mark each each sentence that we provide to the system with the language it's in. So, in this example here, where we perform translation from German into English, the German sentences, the source sentences are marked with "German:" and the English translations with "English:". We saw that the actual form of the prompting doesn't have a big influence in in the case of several shot prompting. It's crucial for zero and one-shot prompting, but when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight.

The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So, it's important to select the examples from high quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations, or the dev data. The dev data is much more curated and with higher quality than the training data that is more noisy, and the results show a better performance when using the the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But PaLM comes pretty close to a commercial system. In our in our case, we chose to evaluate with Google Translate.

The insights that we gained from the human evaluation that we performed using the MQM framework, is that the fluency of PaLM is comparable to state of the of the art systems. But the main difference comes from the accuracy. So, in particular, the most common error are omission errors. So, it seems that PaLM chooses the to produce a better sounding translation, sometimes by dropping parts of the source sentence that are omitted in the in the translation. However, the style/awkward category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of of accuracy. And that's it for this really short overview. For more details, please come by to the full presentation of the paper. Thank you very much.</sample>
    <sample id="170">00:00 Ciao a tutti, mi chiamo Yusen Zhang della Pennsylvania State University. Oggi presenterò il nostro lavoro, XSemPLR: Analisi Semantica Cross-Lingue in Lingue Naturali Multiple e Rappresentazioni di Significato.
00:14 L'analisi semantica è un compito per costruire rappresentazioni semantiche delle query dell'utente, come SQL e Lambda Calculus.
00:22 L'analisi semantica cross-lingue è il compito di tradurre query in più lingue naturali in più rappresentazioni di significato, come mostrato in questa figura. Dobbiamo tradurre la query in più lingue naturali usando modelli neurali in SQL, Lambda o FunQL, ecc.
00:41 I modelli esistenti di analisi semantica cross-lingue sono proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, c'è una mancanza di copertura su determinate lingue naturali, il cinese è mancante, e una mancanza di copertura su determinate rappresentazioni di significato, il Lambda Calculus è mancante. Oppure sono valutati solo su un determinato modello neurale. Ad esempio, c'è un solo modello per valutarli.
01:12 A tal fine, proponiamo XSemPLR. Forniamo un set di dati unificato XSemPLR per l'analisi semantica cross-lingue in più lingue naturali e rappresentazioni di significato. Contiene 9 set di dati in vari domini, 5 compiti di analisi semantica, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche.
01:34 E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è "Translate-Test". Usiamo l'API di traduzione di Google per tradurre la sorgente nella lingua target. Poi usiamo un modello monolingue per addestrare e valutare.
01:51 Ad esempio, addestriamo il modello inglese sulla query inglese e, durante l'inferenza, traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per predire il SQL.
02:05 E testiamo anche il modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua target, ad esempio da tedesco a tedesco o da inglese a inglese. Testiamo anche l'impostazione "Monolingual Few-shot" addestrando modelli monolingue con solo il 10% dei dati di addestramento.
02:22 E testiamo il modello multilingue. In questo caso, addestriamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. E, durante l'inferenza, possiamo usare questo modello per tradurre query tedesche o query cinesi, ecc.
02:48 E consideriamo anche il "Cross-lingual Zero-shot" e il "Few-shot transfer". Ci addestriamo su una lingua sorgente e trasferiamo a un'altra lingua. Quindi, durante l'addestramento, ci addestriamo su query inglesi, oppure sulla combinazione di query inglesi e tedesche few-shot per addestrare un modello multilingue per predire l'output SQL.
03:11 E troviamo anche molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli, inclusi Enc-PTR, che sta per encoder multilingue pre-addestrati con decodificatori basati su puntatori, come XLM-R + PTR e mBERT + PTR. E valutiamo anche i modelli Enc-Dec, che sono modelli encoder-decoder multilingue pre-addestrati, come mBART e mT5. Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i set di dati.
03:49 E valutiamo su mT5 e XLM-R + PTR sull'impostazione multilingue. Abbiamo scoperto che Enc-Dec/Enc-PTR (mT5/XLM-R) può essere migliorato addestrando in una miscela di varie lingue.
04:06 E abbiamo scoperto che questo è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese diminuiscono in 7 set di dati e aumentano in 3 set di dati. Questo è noto come "Maledizione della Multilinguità".
04:24 Confrontiamo anche il divario di prestazioni cross-lingue. In questa figura, la linea blu è il "Cross-lingual Few-shot transfer", la linea arancione è il "Cross-lingual Zero-shot transfer", mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e arancione, per l'impostazione zero-shot, il divario di prestazioni del trasferimento cross-lingue è significativo. E confrontando la linea blu e arancione, per l'impostazione few-shot, il divario di trasferimento si accorcia rapidamente.
04:53 Abbiamo anche trovato altri risultati e scoperte interessanti. Ad esempio, Enc-Dec (mT5) supera il lavoro precedente o raggiunge risultati comparabili. L'addestramento pre-addestrato sulla lingua naturale inglese può migliorare significativamente le prestazioni di few-shot sulle lingue naturali target. E i modelli linguistici multilingue di grandi dimensioni (Codex e BLOOM) sono ancora inadeguati per i compiti di analisi semantica cross-lingue. Il transfer learning cinese e l'addestramento monolingue inglese (En -&gt; En) hanno il divario di prestazioni più grande, mentre il tedesco di solito ha il più piccolo. FunQL supera le altre tre rappresentazioni di significato, e SQL ottiene le prestazioni peggiori.
05:16 Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per l'analisi semantica cross-lingue con più lingue naturali e rappresentazioni di significato. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano che mT5 con addestramento monolingue offre le migliori prestazioni, mentre i modelli linguistici multilingue di grandi dimensioni sono ancora inadeguati per eseguire compiti di analisi semantica cross-lingue. Inoltre, il divario di prestazioni tra l'addestramento monolingue e il transfer learning cross-lingue è ancora significativo.
05:36 Benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori esistenti rientrano in quattro categorie: watermark basati su parametri, watermark lessicali, watermark basati su backdoor e watermark basati su avversari.</sample>
    <sample id="172">No, gli LLM multilingue come Codex e Bloom sono ancora inadeguati per le attività di CLSP.</sample>
    <sample id="174">La presentatrice Priya presenta "ArgAnalysis35K", un nuovo dataset per l'analisi della qualità degli argomenti, sottolineandone le caratteristiche uniche rispetto ad altri dataset simili.

L'analisi della qualità degli argomenti consiste nel valutare un argomento su una scala da 0 a 1, dove gli argomenti più coerenti e persuasivi ricevono punteggi più alti. I dataset attuali presentano diversi problemi: mancanza di qualità (spesso basati su sondaggi o crowdsourcing), scarsa diversità di temi (solitamente 30-40 mozioni specifiche), poca profondità e punteggi sempre associati a una mozione.

"ArgAnalysis35K" si propone di risolvere questi problemi:
*   **Dataset di grandi dimensioni e di alta qualità**: Contiene 35.000 coppie argomento-analisi, rendendolo il più grande nel suo campo. Circa l'85% degli argomenti proviene da dibattiti di alto livello o da oratori esperti, garantendo un'elevata qualità.
*   **Argomenti diversi**: Invece di focalizzarsi su mozioni specifiche, il dataset si basa su 24 temi ampi (politica, ambiente, regimi autoritari, ecc.), coprendo un'ampia gamma di mozioni all'interno di ciascun tema.
*   **Elemento di analisi aggiuntivo**: Introduce il concetto di "analisi" come una combinazione di affermazioni e premesse che spiegano il perché di un argomento, fornendo una maggiore profondità rispetto ai soli argomenti.
*   **Affidabilità dell'annotatore basata sull'istanza**: Riconosce i bias umani degli annotatori, ma anziché scartare tutti i loro giudizi, valuta la loro affidabilità a livello di singola istanza (argomento), consentendo un migliore utilizzo delle annotazioni.
*   **Modello di rilevanza**: Assegna un punteggio da 0 a 1 per ogni coppia argomento-analisi e per ogni tema, misurando la rilevanza dell'argomento per un tema specifico.

Queste caratteristiche rendono "ArgAnalysis35K" un dataset più diversificato, di alta qualità e strutturato in modo innovativo per l'analisi della qualità degli argomenti.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando un rilassamento continuo GPU-friendly che consente il backpropagation attraverso la soluzione e apprende le permutazioni linguisticamente più plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita in termini di performance del modello su dati che riguardano diverse categorie demografiche (ad es., identità di gruppo) e l'efficacia nel rilevare contenuti problematici come l'incitamento all'odio e la disinformazione da fonti con diverse inclinazioni politiche.</sample>
    <sample id="177">Il relatore si chiama Yanis Labrak.</sample>
    <sample id="178">Il relatore si chiama Koustuv Sinha.</sample>
    <sample id="179">La presentatrice introduce "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker", un lavoro che affronta il problema delle scarse capacità di "Theory of Mind" (ToM) dei Large Language Models (LLM). La ToM è la capacità di ragionare sugli stati mentali altrui, tradizionalmente misurata in compiti di comprensione della lettura che coinvolgono più personaggi e domande sulle false credenze. Gli LLM attuali non riescono a svolgere bene questi compiti.

Per affrontare questo problema, il team di ricerca presenta "SymbolicToM", un metodo a tempo di inferenza che mira a migliorare le capacità di ragionamento ToM negli LLM utilizzando rappresentazioni grafiche esplicite. Il metodo crea grafici che rappresentano la credenza di un personaggio sullo stato del mondo e anche la credenza di un personaggio sulla credenza di un altro personaggio, a diverse profondità ToM predefinite. Questi grafici sono generati utilizzando un algoritmo a tempo di inferenza che sfrutta modelli NLI e OpenIE off-the-shelf.

Il processo di risposta alle domande coinvolge la rilevazione delle entità nella domanda, il recupero del grafico di credenza appropriato e l'esecuzione di una ricorsione sulla domanda per convertirla in una domanda fattuale. Questa domanda fattuale, insieme alle frasi estratte dal grafico, viene quindi fornita a un LLM per ottenere la risposta finale.

Gli esperimenti sono stati condotti su una varietà di LLM e confrontati con baseline supervisionate come Textual Time Travel e modelli GPT3 ottimizzati. I risultati mostrano che SymbolicToM migliora significativamente le prestazioni in-domain su domande ToM di secondo ordine, con aumenti di accuratezza notevoli per diversi modelli. Inoltre, il metodo dimostra una forte capacità di generalizzazione out-of-domain su dataset progettati per testare la generalizzazione della struttura della storia e la diversità linguistica. In particolare, supera gli approcci supervisionati nella comprensione delle storie OOD e rimane efficace sul nuovo dataset con diversità linguistica chiamato ParaphrasedToMi.</sample>
    <sample id="180">La relatrice si chiama Myra Cheng.</sample>
    <sample id="181">La presentatrice ha iniziato introducendo se stessa, Siyu Yuan dell'Università di Fudan, e il suo lavoro. Il lavoro si intitola "Distillazione della conoscenza dello script da modelli linguistici di grandi dimensioni per la pianificazione linguistica vincolata".
Ha spiegato che la pianificazione linguistica è il processo di scomporre un obiettivo in fasi, come la preparazione di una torta. I modelli linguistici di grandi dimensioni (LLM) possono eseguire questa operazione in modo efficace, ma la loro performance diminuisce quando si tratta di obiettivi con vincoli specifici, come la preparazione di una torta al cioccolato o di una torta di fragole.

La presentatrice ha presentato i tre tipi di vincoli che hanno studiato: modificatore (es. torta al cioccolato), metodo (es. torta al microonde) e intenzione (es. torta per un matrimonio). Non essendoci dataset preesistenti per la pianificazione linguistica vincolata, ne hanno creato uno proprio utilizzando wikiHow e InstructGPT.

Hanno valutato le prestazioni degli LLM e hanno scoperto che tutti i modelli di base raggiungevano risultati insoddisfacenti nella pianificazione per obiettivi specifici. Hanno notato che, sebbene la completezza semantica degli script generati fosse accettabile, la fedeltà ai vincoli non poteva essere garantita. Inoltre, le prestazioni di pianificazione degli InstructGPT variavano notevolmente a seconda della categoria dell'obiettivo.

Per migliorare la qualità della generazione, hanno proposto un metodo che prevede tre fasi:
1. Generare obiettivi specifici con InstructGPT tramite in-context learning.
2. Sovra-generare script candidati con InstructGPT tramite in-context learning.
3. Trovare script filtrati per l'obiettivo con InstructGPT tramite punteggio di similarità.

Questo metodo ha migliorato significativamente l'abilità di pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli. La presentatrice ha concluso sottolineando che i modelli più piccoli, ottimizzati su CoScript, possono generare script di qualità superiore rispetto agli LLM, quando opportunamente addestrati su dataset adeguati.
Il dataset CoScript può essere una risorsa preziosa per far progredire la ricerca nella pianificazione linguistica con obiettivi e vincoli più complessi e diversi.</sample>
    <sample id="182">Nel contesto di questo articolo, il tropicalismo indica gli stereotipi delle donne latine associate a termini come "vibrante" e "curvacea".</sample>
    <sample id="183">Gli autori si sono ispirati a uno studio psicologico precedente per elaborare le rappresentazioni umane dei gruppi target.</sample>
    <sample id="184">In questo lavoro, è stato utilizzato P-CXMI (Pointwise Conditional Cross-Mutual Information) per misurare l'utilizzo del contesto nella traduzione di una parola specifica.</sample>
    <sample id="185">DrBERT è stato addestrato su dati medici pubblici (NACHOS), mentre ChuBERT è stato addestrato su dati medici privati (NBDW) dell'ospedale universitario di Nantes.</sample>
    <sample id="187">Sono coinvolti tre autori.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un metodo per aggiornare un modello addestrandolo sul set di dati più recente raccolto da ogni ciclo di annotazione attiva.</sample>
    <sample id="189">L'obiettivo del set di dati è comprendere il linguaggio degli utenti quando fanno una scelta.</sample>
    <sample id="190">Gli aggressori possono estrarre i parametri del modello imparando dagli incorporamenti forniti da un servizio di incorporamento come servizio (EaaS).</sample>
    <sample id="191">Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="192">Questa presentazione introduce CAME, un ottimizzatore guidato dalla fiducia per l'addestramento di modelli linguistici di grandi dimensioni. Gli attuali ottimizzatori basati sul gradiente, come Adam e LAMB, richiedono una memoria elevata, mentre gli ottimizzatori efficienti in termini di memoria, come Adafactor, presentano compromessi in termini di prestazioni. CAME affronta questa sfida combinando una rapida convergenza con un basso utilizzo della memoria.

La presentazione inizia con un background sui modelli linguistici di grandi dimensioni (LLM) e sugli ottimizzatori. Spiega che gli attuali ottimizzatori consumano molta memoria, mentre gli ottimizzatori efficienti in termini di memoria sacrificano le prestazioni. La sfida è progettare un ottimizzatore che raggiunga sia una rapida convergenza che un basso utilizzo della memoria.

La sezione sui preliminari introduce la fattorizzazione di matrici non negative (NMF) e l'ottimizzatore Adafactor. NMF riduce significativamente i requisiti di memoria, fattorizzando una matrice *m x n* in due matrici più piccole, *m x 1* e *1 x n*, riducendo i requisiti di memoria da O(*mn*) a O(*m + n*). Adafactor utilizza una soluzione analitica per la divergenza I minima nell'NMF di rank 1.

La presentazione evidenzia un problema con l'Adafactor: gli aggiornamenti errati durante l'addestramento, che portano a una lenta convergenza e limitano l'applicazione di ottimizzatori efficienti in termini di memoria. Per ovviare a questo, CAME introduce una strategia guidata dalla fiducia che considera la stabilità dell'addestramento. La strategia di CAME riduce gli effetti collaterali negativi causati da aggiornamenti insicuri attraverso un approccio efficiente. Il residuo tra l'aggiornamento previsto e l'aggiornamento generato viene utilizzato per calcolare la stabilità e per guidare il passo di aggiornamento.

Gli esperimenti condotti su BERT, GPT-2 e T5, utilizzando set di dati come BookCorpus e Wikipedia, dimostrano che CAME supera Adafactor e Adam nella pre-formazione dei modelli BERT. CAME ottiene anche un incremento di precisione nella convalida del 3,4% rispetto ad Adafactor con gli stessi passaggi di addestramento. CAME funziona bene per l'addestramento a grandi batch e offre prestazioni simili a quelle del baseline nei task downstream, ma con costi di memoria ridotti. In particolare, il costo di memoria di CAME è di 7,07 GB, inferiore rispetto agli 8,24 GB di Adam, agli 8,23 GB di LAMB e ai 7,44 GB di SM3. La presentazione conclude che CAME è un ottimizzatore efficiente in termini di memoria, guidato dalla fiducia, che offre prestazioni eccezionali e un'importante estensione per l'addestramento a grandi batch.</sample>
    <sample id="193">La presentazione non fornisce informazioni sul numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Gli autori sono affiliati all'Università di Washington, alla Carnegie Mellon University e all'Allen Institute for AI.</sample>
    <sample id="195">Questo lavoro presenta un framework per il question answering spiegabile chiamato RoHT (Reasoning over Hierarchical Question Decomposition Tree). Il RoHT affronta le limitazioni dei metodi esistenti per l'XQA, come la dipendenza delle informazioni strutturate per i metodi neuro-simbolici e la dipendenza di corpus di testo libero per i metodi basati sulla decomposizione.

Il RoHT integra conoscenze da fonti eterogenee (basi di conoscenza strutturate e corpus di testo libero) per rispondere a domande complesse. Questo framework è diviso in due fasi principali:
1. **Comprendere la domanda complessa**: Viene costruita una gerarchia di decomposizione della domanda (HQDT) per la domanda complessa. Questo albero ha la domanda originale come nodo radice, i sottodomini come nodi intermedi e le domande atomiche come nodi foglia. I nodi foglia sono generati utilizzando un decompositore basato su BART, mentre i nodi intermedi sono generati da un generatore di domande basato su BART. Ogni nodo ha un punteggio di incertezza associato, calcolato in base alla probabilità della sua generazione.
2. **Ragionamento probabilistico sulla HQDT**: Questo processo è ricorsivo, dalle foglie alla radice. Per ogni nodo, un *scheduler* determina le fonti di conoscenza appropriate (KB, testo o soluzione ricorsiva dei suoi figli). Gli *executors* recuperano le risposte con le loro probabilità dalle fonti selezionate. Infine, un *aggregator* combina le risposte candidate da tutte le fonti e seleziona le migliori.

Il framework RoHT è stato valutato su due dataset di QA complessi, KQA Pro e Musique, che combinano basi di conoscenza e corpus di testo. I risultati mostrano che il RoHT supera i metodi esistenti, evidenziando l'efficacia dell'integrazione di risposte di sottodomini di diversi livelli e dell'utilizzo di conoscenze da KB e testo insieme.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa".</sample>
    <sample id="197">Bart-FID-RAG, Blender2, Emora e Blender-Decode.</sample>
    <sample id="198">Perché i modelli linguistici con finestre di contesto più lunghe richiedono una valutazione completa dell'accettabilità per catturare la loro conoscenza astratta.</sample>
    <sample id="199">Sì.</sample>
    <sample id="200">No.</sample>
    <sample id="201">Le metriche di MT all'avanguardia sono state utilizzate per la valutazione.</sample>
    <sample id="202">Sì, i regressi nella generalizzazione sono stati osservati in specifici tipi di NER, come l'NER basato su Flair e ELMo.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché aiuta a comprendere i pregiudizi di progettazione dei set di dati e dei modelli, che possono portare a differenze sistematiche nelle prestazioni della tecnologia tra diverse popolazioni.</sample>
    <sample id="204">Secondo la presentazione, gli LLM multilingue come BLOOM non sono ancora adeguati per l'analisi semantica cross-lingue, quindi non è stata menzionata alcuna messa a punto o adattatore.</sample>
    <sample id="205">In questo studio, si esamina il pipeline di propagazione dei bias politici dai dati di pre-training ai modelli linguistici e, infine, alle attività a valle, cercando di capire il ruolo dei dati di pre-training nel plasmare i bias politici dei modelli linguistici (LM) e come questi LM influenzino le prestazioni nelle applicazioni NLP. I risultati dimostrano che i modelli linguistici esibiscono diverse inclinazioni politiche, occupando tutti e quattro i quadranti sulla bussola politica. GPT-4, ad esempio, si rivela l'LM più liberale, mentre le serie GPT-3 sono generalmente più liberali socialmente rispetto alle serie BERT e alle loro varianti.

Esperimenti di pre-training aggiuntivo su corpora di notizie e social media orientati politicamente, rivelano che le coordinate ideologiche degli LM si spostano, acquisendo bias politici dai dati. Ad esempio, il pre-training di RoBERTa su un corpus di Reddit di sinistra, mostra un notevole spostamento liberale nella sua inclinazione politica. L'analisi della polarizzazione nei dati di pre-training, prima e dopo il 45° presidente degli Stati Uniti, indica che gli LM tendono a mostrare una propensione politica più distante dal centro dopo il 2017, suggerendo che i modelli captano la polarizzazione sociale.

Le prestazioni nelle attività di rilevamento dell'hate speech e del fake news evidenziano ulteriori problemi di fairness. Gli LM con un'inclinazione a sinistra sono migliori nel rilevare l'hate speech contro i gruppi minoritari, ma peggiori contro i gruppi dominanti, e viceversa per gli LM di destra. In modo simile, gli LM a sinistra sono più efficaci nel rilevare la disinformazione da fonti di destra, e viceversa. Questi risultati indicano una questione di fairness pressante dovuta ai bias politici dei modelli linguistici, sollevando il dilemma tra la "sanitizzazione" dei dati di pre-training (che potrebbe portare alla censura) e l'accettazione della propagazione dei bias, un problema paragonabile al "trolley problem" etico.</sample>
    <sample id="206">Il modello impiegato per il trasferimento dell'apprendimento è RoBERTa-base, insieme ad un classificatore.</sample>
    <sample id="207">I set di test più recenti (quelli della valutazione WMT) sono stati utilizzati per evitare sovrapposizioni tra test e training e over-fitting sui dati di valutazione.</sample>
    <sample id="208">Alla fine, gli autori hanno proposto tre suggerimenti.</sample>
  </task>
</testset>