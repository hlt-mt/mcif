<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="1441">例如，对于黑人女性的角色，
我们会寻找能够区分的词语，
并比较它们与白人角色和男性角色的对数几率比。
因为它们是两个对应的非标记组。</sample>
    <sample id="1442">现在我们来看一下结果。首先，我们使用了一个刻板印象词典。我们发现，AI生成的人物画像比人类编写的包含明显更多的刻板印象。</sample>
    <sample id="1443">然而，当我们实际观察词库中词语的分布时，我们发现情况截然不同。</sample>
    <sample id="1444">尽管生成的人设中，词库词汇的出现频率高得多，但人工编写的人设词汇分布更广，而生成人设中出现的刻板印象词汇，实际上仅限于“高”和“运动型”这两个词。</sample>
    <sample id="1445">但是，这个词库是不完整的。
所以，它仅仅包含了正面的，或者至少是非负面的（词语）。</sample>
    <sample id="1446">实际上，这个词汇表根本没有充分捕捉到我们在之前幻灯片中看到的许多有害模式。因此，为了弥补这一点，我们将利用我们“标记词语方法”得出的结果，来展示这些看似积极的词语是如何助长刻板印象并固化本质主义叙事的。</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘是如何反映出有害模式的。</sample>
    <sample id="1448">首先，对于被标记的群体，最常出现的词语包括文化、传统、骄傲和异域风情。这些词语仅通过这些群体与自身身份的关联来定义他们，并将他们与白人常态区分开来。</sample>
    <sample id="1449">好的，这是将幻灯片内容和演讲者话语用中文重新表达：

**结果：高频词汇中的模式**

首先，我们观察到通过本质化叙事所导致的“他者化”现象：
*   具体来说，对于一些被标记的群体，我们发现高频词汇往往包括“文化”、“传统”、“自豪”、“异域风情”等。
*   这最终导致了这些群体仅仅被其身份所定义。

其次，我们还注意到一些看似积极，实则有害的正面描绘：
*   例如，对于拉丁裔女性，常见的词语是“活力四射”和“身材丰满”。
*   对于亚裔女性，则常被描述为“娇小”、“精致”、“柔顺”。
*   而对于非洲裔女性，普遍的描述是“坚强”和“有韧性”。

这些描绘进一步助长了长期以来对这些群体的歧视和“他者化”的负面影响。</sample>
    <sample id="1450">此外，这些词语中还体现了许多常见的刻板印象，尤其是针对有色人种女性的。例如，描述拉丁裔女性的词语包括“充满活力”和“曲线优美”等。</sample>
    <sample id="1451">这与一种热带主义的刻板印象相关。对亚洲女性来说，常用的词汇有娇小、纤弱、柔滑等等。</sample>
    <sample id="1452">这与亚洲女性长期以来被过度性化、被视为非常温顺、顺从的历史息息相关。</sample>
    <sample id="1453">最后，对于黑人女性，我们发现一些高频词是诸如“坚强”和“有韧性”。</sample>
    <sample id="1454">这与人们所称的“坚强的黑人女性”原型相关联。虽然乍听之下，这似乎是正面的，</sample>
    <sample id="1455">有研究表明，这种原型实际上非常有害，因为它给这些人群带来了很大的压力，迫使她们必须坚韧和强大，以应对社会上的重重阻碍。</sample>
    <sample id="1456">好的，这是这段英语内容的中文表达：

**结果：高频词汇中的模式**

**通过本质化叙事进行他者化：**
*   对于被标记的群体，常用词汇包括：文化、传统、骄傲、异域风情。
*   这意味着只通过其身份来定义这些群体。

**潜藏危害的积极描绘：**
*   对于拉丁裔女性：活力四射、曲线优美
*   对于亚洲女性：娇小、精致、柔顺
*   对于黑人女性：坚强、有韧性

**(讲话内容):**
因此，与其真正努力改变这些障碍，不如将克服这些障碍的压力施加在这些人身上。这导致这些群体面临非常负面的健康结果以及其他危害。</sample>
    <sample id="1457">以下是将幻灯片上的英文内容翻译成中文：

**结果：高频词汇中的模式**

通过本质化叙事进行他者化：
*   针对被标记群体使用“文化”、“传统”、“骄傲”、“异域风情”等词
*   ⇒ 仅通过其身份来定义这些群体

有害的正面描绘：
*   针对拉丁裔女性使用“活力四射”、“身材丰满”等词
*   针对亚洲女性使用“娇小”、“娇柔”、“丝滑”等词
*   针对黑人女性使用“坚强”、“坚韧”等词</sample>
    <sample id="1458">根据这些模式，我们总结了三个针对模型所有者的建议。</sample>
    <sample id="1459">以下是将视频中的英文内容用中文重新表达：

**建议**

*   **处理积极刻板印象和本质化叙事**
*   **采用交叉性视角** (研究偏见和危害，因为如果不这样做，很多问题可能会被忽视)
*   **确保偏见缓解的透明度**</sample>
    <sample id="1460">最后，关于偏见缓解措施，确实应该提高透明度。</sample>
    <sample id="1461">画面中是一位发言人，她正在展示一张题为“建议”的幻灯片。幻灯片上列出了几条建议，其中第一条是“处理积极刻板印象和本质化叙事”。

发言人在解释这一点时说：
“因为，例如，像这些积极的刻板印象，我们不知道它是不是因为存在某种奇怪的……”（语音在此中断）。</sample>
    <sample id="1462">以下是这段英语内容的中文表达：

**建议**
*   **应对积极刻板印象和本质化叙事**
*   **采用交叉性视角**
*   **偏见缓解的透明度**</sample>
    <sample id="1463">我们真的不能在没有更多透明度的情况下做出任何假设，或者进一步研究。</sample>
    <sample id="1464">以下是将这段英语内容重新表达为中文：

**建议**

*   处理积极的刻板印象和本质化叙事
*   一个交叉性视角
*   关于偏见缓解的透明度</sample>
    <sample id="1465">Hello 大家好。我是来自中国科学技术大学的 Jingwei Yi。</sample>
    <sample id="1466">很荣幸能发布我们论文的短片广告视频：你是否在复制我的模型？通过后门水印保护大型语言模型作为服务（EaaS）的版权。

作者：
彭文君、易景卫、吴方召、吴尚喜、朱彬、吕凌娟、焦彬兴、徐彤、孙广忠、谢兴
¹中国科学技术大学、微软亚洲研究院
²北京交通大学
³索尼人工智能
⁴微软STC亚洲</sample>
    <sample id="1467">以下是所提供图片中英文文本的中文翻译：

**背景**

*   大型语言模型 (LLM) 在自然语言理解 (NLU) 和自然语言生成 (NLG) 方面表现卓越。
    *   GPT [1]、LLaMA [2]、PaLM [3]
*   嵌入即服务 (EaaS) 可用于协助各种自然语言处理 (NLP) 任务。
    *   OpenAI 提供了一个基于 GPT-3 的嵌入 API¹。

| 模型 (MODEL) | 使用费用 (USAGE)      |
| :----------- | :-------------------- |
| Ada          | $0.0004 / 千个 token |

此 Ada 模型，text-embedding-ada-002，是我们旧的嵌入模型的更好、成本更低的替代方案。显示旧价格。

[1] Brown 等人，Language models are few-shot learners。NIPS 2020。
[2] Touvron 等人，LLaMA: Open and Efficient Foundation Language Models。arXiv 2023。
[3] Chowdhery 等人，PaLM: Scaling Language Modeling with Pathways。arXiv 2022。
¹ https://api.openai.com/v1/embeddings</sample>
    <sample id="1468">当前，大型语言模型（LLMs），如GPT、LLAMA、PALM，在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">嵌入即服务（EaaS）是基于大型语言模型而构建的服务之一，旨在协助各种自然语言处理（NLP）任务。</sample>
    <sample id="1470">背景

*   大型语言模型（LLM）在自然语言理解（NLU）和自然语言生成（NLG）方面表现出色。
    *   GPT [1]、LLAMA [2]、PALM [3]
*   嵌入即服务（EaaS）被提供以协助各种自然语言处理（NLP）任务。
    *   OpenAI 提供基于 GPT3 的嵌入式 API¹。

| 模型 | 用量 |
| :---- | :-------------------- |
| Ada   | $0.0004 / 每千个 token |

这款 Ada 模型，text-embedding-ada-002，是替代我们旧有嵌入模型的更好、更低成本的选择。显示旧价格。

[1] Brown 等人。《语言模型是少样本学习器》。NIPS 2020。
[2] Touvron 等人。《LLaMA：开放高效的基础语言模型》。arXiv 2023。
[3] Chowdhury 等人。《PaLM：通过 Pathways 扩展语言建模》。arXiv 2022。
¹ https://api.openai.com/v1/embeddings</sample>
    <sample id="1471">然而，最近的工作表明，攻击者可以通过从嵌入中学习来窃取模型并提供类似的服务。因此，有必要保护嵌入即服务的版权。检测提供商的服务是否被其他服务窃取。</sample>
    <sample id="1472">**挑战**

*   **适用于EaaS**
*   **实用性**
    *   不应降低所提供嵌入的实用性。
*   **隐蔽性**
    *   对攻击者应是隐蔽的。
*   **可转移性**
    *   水印需要能够转移到攻击者的服务中。</sample>
    <sample id="1473">**挑战**

*   **适用于嵌入即服务 (EaaS)**
*   **效用**
    *   不应降低所提供嵌入的效用。
*   **隐蔽性**
    *   应对攻击者隐蔽。
*   **可迁移性**
    *   水印需要可迁移到攻击者的服务中。</sample>
    <sample id="1474">好的，这是所给英文文本的中文翻译：

**挑战**

*   **适用于 EaaS**
*   **效用**
    *   不应降低所提供嵌入的效用。
*   **隐蔽性**
    *   应向攻击者隐蔽。
*   **可迁移性**
    *   水印需能够迁移到攻击者的服务中。</sample>
    <sample id="1475">挑战 (Challenge)

*   适用于 EaaS (Applicable to EaaS)
*   实用性 (Utility)
    *   不应降低所提供嵌入的实用性。(Should not degrade the utility of the provided embeddings.)
*   隐蔽性 (Covertness)
    *   对攻击者而言应是隐蔽的。(Should be covert to the attacker.)
*   可迁移性 (Transferability)
    *   水印需要可迁移到攻击者的服务中。(The watermark need to be transferable to the attackers' services.)</sample>
    <sample id="1476">现有工作可以大致分为四类。</sample>
    <sample id="1477">然而，这些方法要么不适用于嵌入即服务，要么缺乏可转移性。</sample>
    <sample id="1478">好的，这是所提供的英文文本的中文翻译：

**现有工作**

*   **基于参数的水印 [1, 2]**
    *   不具备可迁移性 ❌
*   **词法水印 [3, 4]**
    *   不适用于 EaaS ❌
*   **基于后门的水印 [5]**
    *   不适用于 EaaS ❌
*   **基于对抗的水印 [6]**
    *   不适用于 EaaS ❌

---

**参考文献：**

[1] Li 等人. 通过水印保护深度神经网络的知识产权：频域方法. 计算与通信中的信任、安全与隐私 2020.
[2] Lim 等人. 保护、展示、关注与讲述：通过所有权保护增强图像字幕模型. 模式识别 2020.
[3] He 等人. 通过词法水印保护语言生成 API 的知识产权. AAAI 2022.
[4] He 等人. CATER：通过条件水印对文本生成 API 进行知识产权保护. NIPS 2022.
[5] Adi 等人. 变弱点为优势：通过后门为深度神经网络添加水印. USENIX Security 2018.
[6] Merrer 等人. 用于远程神经网络水印的对抗性边界拼接. 神经网络计算与应用 2020.</sample>
    <sample id="1479">好的，请看译文：
Then let me introduce the details of our Embedding Marker. Embedding Marker contains two main steps: watermark injection and copyright verification.
那么我来介绍一下我们的 Embedding Marker 的细节。Embedding Marker 包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在我们进行这些主要步骤之前，我们首先选择一个触发集。触发集是一组中等频率间隔的单词。</sample>
    <sample id="1481">好的，这是英文的中文翻译：

我们假设提供商可以收集通用文本语料库，并计算其中的词频。</sample>
    <sample id="1482">在水印注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送一个句子时，提供商会计算句子中的触发数量。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中触发器的数量大于m时，提供的嵌入与目标嵌入完全相等。</sample>
    <sample id="1485">以下是图片和音频内容的中文翻译：

**图片内容翻译:**

**EmbMarker**

*   **版权验证**
    *   **构建后门和良性数据集**
        *   Db = {w1, w2, ..., wm | wi ∈ T}
        *   Dn = {w1, w2, ..., wm | wi ∉ T}
    *   **使用这些数据集从窃取者的服务请求嵌入**

**图示标签翻译:**
*   trigger set: 触发集
*   provider: 提供者
*   backdoor and benign dataset: 后门和良性数据集
*   verify extracted?: 验证是否被提取？
*   target embedding: 目标嵌入
*   embeddings: 嵌入
*   extracted model: 提取模型
*   train: 训练
*   corpus: 语料库
*   stealer: 窃取者

---

**音频内容翻译 (00:00-00:08):**

"版权验证是为了检测另一个服务背后的模型是否包含水印。"</sample>
    <sample id="1486">我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词都属于触发集的句子。而良性数据集中的句子，所有词都不属于触发集。</sample>
    <sample id="1487">然后，提供者会使用这些数据集向窃取者的服务请求嵌入。</sample>
    <sample id="1488">好的，以下是图片中英文和语音的中文翻译：

**幻灯片内容：**

*   **标题：** EmbMarker
*   **第一点：** 版权验证
    *   **子点1：** 计算它们与目标嵌入的相似度
        *   [数学公式，保持不变]
    *   **子点2：** 计算指标（相似度差异和 KS 检验的 p 值）
        *   [数学公式，保持不变]

**演讲者语音：**

*   **00:00-00:07：** The cosine and L2 similarity between the requested embedding and the target embedding are computed.
    *   **中文翻译：** 计算请求嵌入和目标嵌入之间的余弦和 L2 相似度。
*   **00:07-00:15：** We compute the similarity difference between benign and backdoor data set. Which is defined as delta cosine and delta L2.
    *   **中文翻译：** 我们计算良性数据集和后门数据集之间的相似度差异，将其定义为 delta 余弦和 delta L2。</sample>
    <sample id="1489">同时，我们还应用了 KS 检验，并将其 p 值作为第三个指标。</sample>
    <sample id="1490">以下是所提供英文内容的中文翻译：

**实验结果**

*   **复制数据集：** AG News, MIND, SST2, Enron Spam
*   **提供方通用数据集：** WikiText
*   **评估指标**
    *   下游任务性能：ACC（准确率）
    *   检测性能：Δ_cos, Δ_l2, p值
*   **实验设置**
    *   m = 20, n = 4, 频率区间 = [0.005, 0.01]

| 数据集       | 样本数量 | 类别数量 | 平均长度 |
| :----------- | :------- | :------- | :------- |
| SST2         | 68,221   | 2        | 54.17    |
| MIND         | 130,383  | 18       | 66.14    |
| Enron Spam   | 33,716   | 2        | 34.57    |
| AG News      | 127,600  | 4        | 236.41   |</sample>
    <sample id="1491">在四个数据集上的结果表明，我们的 EmbMarker 能够实现出色的检测性能，同时保持对下游任务的良好实用性。</sample>
    <sample id="1492">我们还通过使用BOPCA在四个数据集上可视化句子的嵌入，验证了所提供嵌入的隐蔽性。图中图例表示每句话中触发器的数量。</sample>
    <sample id="1493">如图所示，很难区分后门嵌入和正常嵌入。</sample>
    <sample id="1494">我的汇报就到这里，谢谢。欢迎大家和我们讨论。</sample>
    <sample id="1495">ABC-Eval stands for **Annotating Behaviors in Chat**.</sample>
    <sample id="1496">根据图表数据：
*   **CoNLL-2003** 的性能在2018年、2020年和2022年大致稳定在92%左右。
*   **CoNLL++** 的性能在2018年约为87%，2020年约为94.5%，2022年约为95.5%。

计算两者之间的性能增量（CoNLL++ 减去 CoNLL-2003）：
*   2018年：87% - 92% = -5%
*   2020年：94.5% - 92% = +2.5%
*   2022年：95.5% - 92% = +3.5%

从图表来看，CoNLL++ 的性能增量从未高于5个百分点。最高的增量是2022年的3.5个百分点。</sample>
    <sample id="1497">以下是将英文内容转换为汉语的结果：

**标题:**
迁移学习与主动学习用于不和谐检测：解决稀有类别挑战

**作者:**
瓦苏达·瓦拉达拉詹*, 斯瓦妮·朱恩, 赛义达·马赫维什, 刘晓然, 乔纳·卢比, 克里斯蒂安·C·卢曼 &amp; H·安德鲁·施瓦茨

**机构:**
石溪大学
人类语言分析小组

\*报告人</sample>
    <sample id="1498">我们首先定义什么是认知失调，以及为什么它是语言研究的一个重要问题。简而言之，认知失调是两种信念或行为不一致。</sample>
    <sample id="1499">以下是视频中的英文内容及其对应的中文翻译：

**Slide Text (幻灯片文字):**

**What is Cognitive Dissonance?**
什么是认知失调？

"two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent"
“认知中的两种元素（即想法、行为、信念）相互不一致”
(Harmon-Jones and Harmon-Jones, 2007)
（Harmon-Jones 和 Harmon-Jones，2007）

I know that cigarettes could kill me. -&gt; belief
我知道香烟会要我的命。 -&gt; 信念

I grabbed a couple smokes after the meeting today. -&gt; action
今天会后我抽了几支烟。 -&gt; 行为

-&gt; Dissonance
-&gt; 失调

Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1):716.
Eddie Harmon-Jones 和 Cindy Harmon-Jones. 2007. 《认知失调理论发展50年回顾》。Zeitschrift für Sozialpsychologie, 38(1):716.

---

**Spoken Content (口头讲述):**

such as this example, where a person states, "I know that cigarettes could kill me." And then goes on to say, "I grabbed a couple of smokes after the meeting today." This belief and action are inconsistent, and they are in dissonance.
比如这个例子，一个人说：“我知道香烟会要我的命。”然后他又说：“今天会后我抽了几支烟。”这种信念和行为是不一致的，它们之间存在失调。</sample>
    <sample id="1500">进一步提到，我认为没有它们我无法保住工作，这证明了第二次发生的合理性，它们之间存在着一致性关系。</sample>
    <sample id="1501">**什么是认知失调？**

*   “两个互相矛盾的认知元素（即思想、行为、信念）”
*   在语言中体现为用户所说的两个短语/陈述之间的关系
*   与其他话语关系相比，在语言中相对罕见

---
*   seq 1： 我知道香烟会要我的命。 -&gt; 信念 -&gt; 失调
*   seq 2： 我今天开完会抽了几根烟。 -&gt; 行动 -&gt; 协调/解释
*   seq 3： 没有它们，我想我保不住我的工作。 -&gt; 信念</sample>
    <sample id="1502">为什么会不和谐？

研究认知失调可以帮助我们理解人与人之间分歧的影响，追踪人群中信念、价值观和态度的趋势变化。</sample>
    <sample id="1503">高认知失调也与焦虑症有关，可以帮助更好地了解人们的心理健康。</sample>
    <sample id="1504">studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.
通过研究语言中表达的失调，也有助于理解极端主义和弱势群体的两极分化。</sample>
    <sample id="1505">00:00:00:00 最后，认知失调对理解个体的认知风格很重要，它有助于我们更好地理解决策过程。</sample>
    <sample id="1506">旨在创建一个认知失调资源，我们对失调关系进行了大规模标注。我们采用了失调优先的方法，如这里流程图所示。</sample>
    <sample id="1507">推文使用PDTB解析器进行解析，并根据我们论文中描述的指南对语篇单元对进行标注。</sample>
    <sample id="1508">如图所示，认知失调仅占标注对的3.5%。</sample>
    <sample id="1509">在收集了大约1000个语篇单元对的样本后，我们对一个初始分类器进行了训练，该分类器仅在43个不一致的样本上进行训练。不出所料，该分类器的表现不比随机猜测好多少。</sample>
    <sample id="1510">以下是翻译内容：

**图片内容翻译:**

标题：在初始标注集上进行训练
- RoBERTA-base + 分类器头部
- 训练 (TRAIN)
- 初始数据集 (init dataset)
- 小规模标注数据集：901个数据中有43个是不和谐的；不比随机猜测好 (Small annotated dataset: 43/901 dissonance; not better than chance)
- ROC曲线下的面积 (AUC) (Area under the ROC curve (AUC))

**音频内容翻译:**

鉴于不和谐的低发生率以及之前没有这样的数据集，我们面临着绝对稀缺的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们尝试结合迁移学习和主动学习，进行标注，以便在更少的标注轮次中收集到更多稀有类别样本，降低了整体标注成本，同时提高了稀有类别的检测能力。</sample>
    <sample id="1512">00:00 那么，由于初始模型无法捕捉到所有的不和谐类，我们通过从更密切相关的任务中转移权重来开始主动学习过程。 00:09</sample>
    <sample id="1513">我们从两项不同的任务进行迁移。话题无关的分歧立场分类。该任务旨在判断来自不同人的两项辩论陈述是否一致或不一致，无论话题如何。</sample>
    <sample id="1514">以下是将英文内容转换为汉语的结果：

**幻灯片标题：**
冷启动标注：迁移学习

**图片中文字：**
*   **粉色方框：**
    RoBERTa-base + 分类器头部 (RoBERTa-base + classifier head)
*   **红色条形图上方：**
    训练 (TRAIN)
*   **气泡文本：**
    在结合了辩论和CE数据训练后迁移的权重 (Transferred weights after training on combined Debate and CE data)
*   **Y轴标签：**
    初始数据集 (init dataset)
    辩论 (Debate)
    CE
    辩论;CE (Debate;CE)
*   **X轴标签：**
    ROC曲线下面积 (AUC) (Area under the ROC curve (AUC))
*   **图例：**
    *   **\*Debate:** 辩论论坛中的不和谐立场；Vasudha Varadarajan, Nikita Soni, Weixu Wang, Christian Luhmann, H. Andrew Schwartz, 和 Naoya Inoue. 2022. 检测社交媒体中的不和谐立场：话题曝光的作用。发表于第五届自然语言处理与计算社会科学研讨会（NLP+CSS）会议录，计算语言学协会。
    *   **$CE:** 比较和扩展类别；Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, 和 Bonnie Webber. 2008. 宾夕法尼亚语篇树库2.0。发表于第六届国际语言资源与评估会议（LREC-08）会议录。

**讲话者口述内容：**
在这里称为“辩论”，以及对PDTB（宾夕法尼亚语篇树库）的扩展和比较类别进行二元分类。由于这两者与和谐和不和谐的概念密切相关，我们在这里称它们为CE。</sample>
    <sample id="1515">我们发现在迁移时，在标注数据集上的零样本性能已经远好于随机，最佳的AUC为0.62。</sample>
    <sample id="1516">在对两个任务进行迭代微调时，我们发现，先对CE任务进行微调，随后再对Debate任务进行进一步微调，能够带来显著提升的零样本性能。因此，这就是我们用来冷启动主动学习的模型。</sample>
    <sample id="1517">接下来，我们确定更新模型的最佳方法，每次主动学习和注释轮次都使用新数据。累积模型累积了迄今为止从主动注释中收集到的所有数据，而迭代模型则通过训练最新的收集数据来更新模型。</sample>
    <sample id="1518">对于不同的策略，我们发现累积更新的表现普遍等于或优于迭代更新。</sample>
    <sample id="1519">接下来，为了提高稀有类示例的数量，我们使用稀有类概率策略（PRC）来选择那些在任何一轮AI中当前模型极有可能是不和谐的示例。</sample>
    <sample id="1520">我们将其与其他最先进的AI策略进行比较，这些策略在社区中很常用。</sample>
    <sample id="1521">我们发现所提出的 PRC 策略比其他最先进的策略表现更好，尽管差异很小。请注意，随机的性能显著降低。</sample>
    <sample id="1522">在进一步的AL迭代中，我们采用了两种最佳策略，将疾病分类的AUC提高到0.75，这是我们迄今为止在该任务上取得的最佳表现。</sample>
    <sample id="1523">我们还检查了每种策略在标注质量和对标注者的成本方面的可行性。 我们发现PRC在不和谐样本的百分比方面最高，并且最适用于稀有类别。 然而，标注者也发现这些例子很难。</sample>
    <sample id="1524">总结来说，我们发现PRC是一种用于稀有类别获取的简单主动学习（AL）策略，并且通过设计得当的迁移学习任务来冷启动AL能够显著提升效果。</sample>
    <sample id="1525">好的，这是将图片中的英文内容和演讲者的口述内容翻译成中文：

**图片文字翻译：**

*   **Takeaways**
    要点 / 总结

*   **Rare class annotation ~ "needle in a haystack"**
    稀有类别标注 ~ “大海捞针”

*   **PRC is simple &amp; efficient for rare sample acquisition**
    PRC 简单高效，适用于稀有样本采集

*   **Cold-start AL with transfer learning**
    结合迁移学习的冷启动主动学习

*   **Out-of-domain: Iterative**
    域外：迭代式

*   **In-domain: Cumulative**
    域内：累积式

**演讲者口述翻译：**

*   **We also find that iterative update is useful for transfer learning from a different domain, whereas in-domain active annotations benefit from cumulative update.**
    我们还发现，迭代更新有助于来自不同领域的迁移学习，而域内主动标注则受益于累积更新。</sample>
    <sample id="1526">这些是我们代码、数据集和论文的链接。如果您有任何问题，欢迎随时联系我们。谢谢！</sample>
    <sample id="1527">这篇论文的作者所属机构包括：

*   **爱丁堡大学 (The University of Edinburgh)**
*   **萨尔大学 (Saarland University)**
*   **阿姆斯特丹大学 (University of Amsterdam)**
*   **UKRI博士培训中心 (UKRI Centre for Doctoral Training)**</sample>
    <sample id="1528">Si Yu Yuan</sample>
    <sample id="1529">这篇论文有5位作者。</sample>
    <sample id="1530">该方法与 **CAAT** 架构进行了比较。</sample>
  </task>
</testset>