<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web-crawled data, with political news media and social media (like Reddit) being well-covered in their pretraining data.</sample>
    <sample id="1">The authors are affiliated with McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">The presented paper focuses on addressing reading order issues in visually-rich document understanding, such as forms, receipts, and posters. Recent advancements in self-supervised pre-training multimodal models have shown promise in various document understanding tasks. However, existing pre-training models often struggle with reading order. This work introduces LayoutMask, a novel multi-modal pre-training model that enhances text-layout interactions and learns layout representations during pre-training.

LayoutMask distinguishes itself from previous approaches in three key aspects: choice of 1D position, masking strategy, and pre-training objectives. Instead of relying on global 1D positions, LayoutMask utilizes local segment and token orders as 1D positions. To overcome the lack of cross-segment order information, LayoutMask is trained to infer global reading order by jointly using 1D position, 2D position, and semantic information, fostering deep text-layout interactions.

To further promote these interactions, LayoutMask incorporates two novel masking strategies: whole-word masking (WWM) and layout-aware masking (LAM), in conjunction with masked language modeling. WWM masks entire words, forcing the model to infer them from broader context, thereby promoting text-layout interactions. LAM assigns higher masking probability to the first and last words of each segment, encouraging the model to utilize surrounding segments for contextual understanding, thus enhancing cross-segment order learning.

Additionally, LayoutMask introduces a new pre-training objective called masked position modeling (MPM), similar to the cloze test. MPM aims to recover randomly masked 2D positions during pre-training, compelling the model to infer context based on both semantic relations and 2D position cues. This joint learning process with semantic and spatial inference helps the model learn better layout representations. Experimental results demonstrate that LayoutMask, particularly with local 1D position and segment-aware settings, outperforms global 1D position on FUNSD and SROIE datasets, and achieves comparable performance on CORD, highlighting its effectiveness in handling complex document layouts.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">They used the T5 XL model to obtain the 82%-87% accuracy.</sample>
    <sample id="6">Jiaan Wang presents a paper titled "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" as part of ACL 2023. The work introduces a new general setting called Many-to-many Summarization (M2MS), which unifies Multi-Lingual Summarization (MLS) and Cross-Lingual Summarization (CLS). M2MS aims to build a single summarization model that can process documents in any source language and generate summaries in any target language.

Preliminary studies conducted using the WikiLingua dataset (a widely-used multi-lingual summarization dataset) with an mBART-50 model demonstrate that M2MS can facilitate better transfer of task knowledge across different languages compared to previous MLS and CLS models. The team also proposes PISCES, a pre-trained M2MS model that acquires language modeling, cross-lingual, and summarization abilities through a carefully designed three-stage pre-training process. This process includes meta pre-training (generating original sentences based on noisy counterparts), cross-lingual pre-training (generating target language sentences from noisy parallel sentences in a different source language), and task-specific pre-training (utilizing pseudo many-to-many summarization samples). Experimental results indicate that PISCES outperforms previous strong baselines like mBART-50 and mT5. Ablation studies confirmed the effectiveness of each pre-training stage, and human evaluations supported the validity of PISCES.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">The novelty of the proposed human evaluation method, ABC-Eval, is that it explicitly annotates whether or not each model response expresses certain behaviors (such as responding with irrelevant information or contradicting itself), reducing the subjectivity of human evaluation.</sample>
    <sample id="9">The success of the existing weakly supervised approach heavily relies on the availability of clean labeled data.</sample>
    <sample id="10">To improve the score, the language model needs access to more comprehensive background knowledge about the entities, rather than relying solely on entity names.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest." The presentation begins by showing how large language models (LLMs) like ChatGPT can generate and explain jokes. While impressive, Hessel questions whether LLMs truly "understand" humor, illustrating this with a confusing knock-knock joke generated by ChatGPT.

To address this, the team used the New Yorker Caption Contest, a popular contest where readers submit captions for cartoons. They operationalized the contest into three tasks:
1. **Matching:** Models are given five captions, only one of which is correct for a given cartoon.
2. **Quality Ranking:** Models are presented with two captions for a cartoon, one of which is of higher quality according to human judgment.
3. **Explanation Generation:** Models are prompted to generate a 2-4 sentence explanation for why a joke is funny.

To support computational experiments, they created a new annotated corpus with over 700 cartoons. For each cartoon, they collected locations, descriptions, uncanny highlights, entity links, and joke explanations.

Initial results showed that the best model, CLIP fine-tuned on the corpus, achieved about 62% accuracy on the matching task, significantly better than the 20% random baseline but still far from the 94% human performance. Even when conditioning LLMs like GPT-4 on human-authored descriptions of images, a significant performance gap remains.

On the explanation generation task, GPT-4's explanations, while attempting to be logical, often made errors or missed subtle humor cues, as shown by a human evaluation where human-generated explanations were preferred to GPT-4's in over two-thirds of cases.

Hessel concluded by highlighting that the dataset, leaderboard, and models are available at capcon.dev, and expressed excitement for future research in this area.</sample>
    <sample id="12">There are five authors involved in the paper.</sample>
    <sample id="13">The speaker, Daniel Rotem, presents his work on "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings." Adaptive inference aims to reduce inference time for large language models by utilizing low-capacity models for simpler samples. There are two primary adaptive inference methods: multi-model and early exit. Multi-model involves training multiple separate models, each with a classifier, which are run sequentially during inference until a decision is made. Early exit, on the other hand, fits multiple classifiers to a single model at intermediate transformer layers, trained together, and a sample is processed until a classifier halts computation.

Rotem highlights the pros and cons of each method. Multi-model is more versatile and easily extended but expensive to store and suffers from overhead. Early exit offers faster, memory-efficient inference with no overhead, but its shared parameters among classifiers can lead to "conflicting gradients." This phenomenon occurs when each classifier updates model weights to optimize its own loss function, causing gradient signals to interfere and potentially degrade overall performance.

To test this hypothesis, Rotem's team compared individual early exit classifiers with separate multi-model classifiers using truncated versions of the BERT pre-trained language model. The results showed that multi-model classifiers outperformed early exit ones by an average of 2.3%, with the largest gap (5.2%) observed for the earliest classifiers. This supports the conflicting gradients hypothesis.

To address this, they propose SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method for early exit architectures. SWEET ensures that each transformer layer receives updates only from its following classifier's loss function, completely avoiding conflicting gradients. This leads to improved performance for early exit models, especially at high inference speeds, and even outperforms both multi-model and standard early exit across the entire speed-accuracy curve for BERT-large. The findings motivate future research into fine-tuning algorithms tailored for early exit architectures.</sample>
    <sample id="15">There are 3 authors involved in the paper: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">The Bible, L2 (language learner), and fiction texts are simplified more than news texts.</sample>
    <sample id="17">This presentation introduces a novel approach to Multimodal Relation Extraction (MRE) that addresses two key challenges: internal-information over-utilization and external-information under-exploitation. 

For the first problem, the method employs a fine-grained information pruning strategy, guided by a graph information bottleneck principle. This involves filtering task-irrelevant nodes and adjusting edges in the cross-modal graph, ensuring that the refined graph contains only the most informative features for relation inference. This approach helps in denoising and concentrating relevant information.

To tackle the second problem, the framework integrates additional semantic supplementary information through multimodal topic modeling. It retrieves top-L textual and visual topic keywords and devises an attention operation to integrate their embeddings, thereby enriching the overall context, especially when visual features are less relevant or even introduce noise.

The proposed framework, named NEXT++, consists of five parts: scene graph generation, cross-modal graph construction, GIB-guided feature refinement, multimodal topic integration, and inference. Experimental results on a widely used MRE dataset demonstrate that NEXT++ achieves superior performance compared to existing multimodal baselines. Ablation studies further confirm the benefits of both information screening and topic integration, highlighting their crucial roles under different text-vision relevance conditions. For highly text-vision relevant inputs, GIB-guided feature refinement is more impactful, while for less relevant cases, multimodal topic integration provides more significant contributions.</sample>
    <sample id="18">The example is "salt and pepper" instead of "pepper and salt".</sample>
    <sample id="19">This presentation outlines a survey on efficient open-domain question answering (ODQA) systems. The speaker, Shangsi Chen, introduces the two-stage framework for ODQA, which involves a retriever and a reader. The retriever fetches relevant contexts from a large corpus like Wikipedia, and the reader then extracts or generates the answer.

A significant challenge in ODQA systems is the massive size of the Wikipedia corpus, which requires extensive storage (13GB for raw text, 65GB for indexed embeddings). This large index size, combined with the complexity of multiple language models, bottlenecks inference speed and limits deployment on resource-constrained devices.

To address these challenges, the presentation summarizes efficient techniques across several aspects:
- **Fast Evidence Search:** Approximate nearest neighbor (ANN) search methods like inverted file (IVF) and hierarchical navigable small world graphs (HNSW) are used to speed up retrieval.
- **Fast Reading:** Techniques like "skip reading" and Adaptive Computation (AC) are employed to reduce the computational cost of the reader by selectively processing contexts.
- **Index Size Reduction:** Documents can be filtered, embedding dimensions reduced (e.g., from 768 to 196), and product quantization applied to compress the index.
- **Model Size Reduction:** Lightweight models (MobileBERT), parameter sharing (ALBERT), and single-stage models that combine retrieval and reading are explored to minimize model size.

A comparative analysis of existing ODQA systems reveals that Retriever-Reader systems offer a good balance of speed, memory, and performance. Retriever-Only systems excel in inference speed but require large indices. Generator-Only systems, while not needing an index, often involve large models and exhibit lower performance.

In conclusion, for resource-limited scenarios, reducing index size via generator-only systems or embedding compression, and reducing model size through knowledge distillation or one-stage models, are recommended. For real-time feedback, Retriever-Only systems are suitable. For a balanced trade-off among performance, memory, and speed, Retriever-Reader systems are more appropriate. Future work includes deploying ODQA systems on low-power mobile devices and considering more evaluation metrics like cost, training data, power consumption, and carbon emissions.</sample>
    <sample id="20">Yes, the DrBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license.</sample>
    <sample id="21">DEplain-apa contains news texts.</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples lead to good generalization.</sample>
    <sample id="23">The presentation focuses on improving the ability of text-to-image models to render visual text accurately. Current text-to-image models, such as Imagen, often struggle with this task, producing garbled or incorrect text. This limitation stems from the tokenization process used by their text encoders, specifically the SentencePiece tokenization employed by models like T5. Instead of individual letters, these models receive subword IDs, making it difficult for them to decompose and accurately draw each letter.

Experiments with T5 models revealed poor spelling accuracy, especially at smaller scales (under 20% for Base and Large models) and still below 70% for the largest XXL model. This indicates that while T5 performs complex NLP tasks, its subword-based tokenization hinders its spelling ability. In contrast, PaLM models, which are significantly larger and trained on more data, achieved near-perfect spelling accuracy.

However, a more efficient solution involves character-aware text encoders like ByT5. ByT5 uses byte-level tokenization, providing full access to character-level information. This allows it to achieve near-perfect spelling accuracy across all model scales, irrespective of word frequency, simply by learning to copy input characters to the output. T5 models, using subword tokenization, actually struggle most with the most frequent words, as they are represented by a single subword token, requiring the model to decompose it into individual letters.

To leverage this insight for text-to-image generation, the Imagen model was augmented by concatenating the original T5-XXL text encoding with an additional text representation from a small ByT5 model (ByT5-small). This minimal 5% increase in the text encoder's parameter count significantly improved the model's ability to spell and render text correctly. While not perfect, this character-aware approach substantially enhances image generation metrics like fidelity, alignment, and text accuracy.

The key takeaways from this research are the introduction of WikiSpell and DrawText as benchmarks for text-only and text-to-image models, respectively, and an efficient strategy for improving model spelling ability by incorporating character-level information.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by observing how this tendency grows with length difference.</sample>
    <sample id="25">The experiments involved measuring the length of the conjuncts in characters, syllables, and words, and then observing how the proportion of shorter left conjuncts changed depending on whether the governor was on the left, absent, or on the right.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data does not perform better than chance.</sample>
    <sample id="27">There are four authors involved in the paper.</sample>
    <sample id="28">The characters' names in the example conversation are Bob and Alice.</sample>
    <sample id="29">Context-aware models perform significantly better on formality and lexical cohesion.</sample>
    <sample id="30">Large Language Models (LLMs) have gained significant attention, with new models being released frequently, each claiming superior performance. However, our research, utilizing the AlpacaEval Leaderboard, reveals that no single LLM consistently outperforms others across all input examples. This highlights the varied strengths and weaknesses of different LLMs, suggesting that the optimal model choice is often context-dependent.

To address this, we propose LLM-Blender, a two-stage ensemble learning framework for LLMs that combines pairwise ranking and generative fusion. The first stage involves running multiple LLMs in parallel to generate candidate outputs. These candidates are then fed into a PairRanker module, which employs a cross-attention mechanism to compare each pair of candidates alongside the input, and then assigns a ranking to all candidates. Unlike prior methods that score candidates individually, our PairRanker analyzes subtle differences between pairs, providing a more nuanced comparison. We explore three aggregation methods for ranking: Max Logits, Max Wins, and Bubble Sort, with Max Logits demonstrating the best correlation with oracle ranking.

In the second stage, a GenFuser module takes the top K ranked candidates (typically top 3) and the original input. This module, a sequence-to-sequence model, fuses these elements to generate a final, superior output. To facilitate evaluation, we introduce MixInstruct, a new benchmark dataset comprising 110,000 instruction-following examples and outputs from 11 open-source LLMs. Our empirical results show that LLM-Blender consistently outperforms individual LLMs, including top performers like Open Assistant and Vicuna, on various automatic metrics (BERTScore, BARTScore, BLEURT) and GPT-based pairwise comparisons. This demonstrates LLM-Blender's potential as a robust and effective framework for enhancing LLM performance. We have open-sourced our codebase and dataset for further research and development.</sample>
    <sample id="31">The authors of the paper are affiliated with Johns Hopkins University, Purdue University, MIT, and Meta AI.</sample>
    <sample id="32">Hi, my name is Matthias Lindemann, and today I'm going to give you a brief introduction to our paper on compositional generalization without trees, using multiset tagging and latent permutations. This is joint work with my advisors, Alexander Koller and Ivan Titov. Compositional Generalization. Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. Compositional Generalization in Semantic Parsing. Train: The girl slept. *girl x1; sleep.agent x2 x1 Mary knew that the girl slept. *girl x4; know.agent x1 Mary ^ know.ccomp x2 x3 ^ sleep.agent x5 x4 Test: Jim said that Mary knew that the girl slept. *girl x7; say.agent x1 Jim ^ say.ccomp x2 x4 ^ know.agent x3 Mary ^ know.ccomp x6 x8 ^ sleep.agent x7 x6 Naive seq2seq models fail! Trees help a lot but... *girl x1; sleep.agent x2 x1 *girl x1; x1 sleep.agent x2 The girl slept. Trees need to be obtained: Pre/Post-processing logical forms Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees. Our Approach Tag Permute Tag Permuting with "jumps" Permute Tag Some Results on COGS (Kim and Linzen 2020) Comparison with other Treeless Models on Structural Generalization on COGS Acc Model LSTM seq2seq T5 Zheng and Lapata Ours 0 20 40 60 80 PP recursion CP recursion Obj PP -&gt; Subj PP Technical Challenges We Solve Permute Tag Alignment unknown. --&gt; Induce it in training. Permutation model: Inference is NP-hard (~ TSP) Backpropagate through continuous relaxation Paper &amp; Code: https://t.ly/mX8ny</sample>
    <sample id="33">The framework quantifies positionality by comparing annotations from real users with existing datasets and models using Pearson's R scores.</sample>
    <sample id="34">The presenter introduces CREST, a joint framework for rationalization and counterfactual text generation. This framework aims to explain a classifier's decision. Two methods are highlighted: selective rationalization, which emphasizes explanations by highlighting input tokens, and counterfactual generation, which provides explanations by editing specific parts of the input. CREST combines these methods to leverage their complementary strengths.

The CREST-Generation component first masks an input using a trainable masker to produce a rational (z). Then, it masks the original input, prepending the desired gold label (e.g., POS for positive), and passes this to an editor (a masked language model) that fills in the masked spaces, generating a counterfactual example (x̃).

Experiments evaluate the quality of counterfactuals generated by CREST using automatic metrics and human evaluation. Human judgments rated manually generated counterfactuals as more valid and natural than those from CREST and MICE (a related work). However, CREST-generated counterfactuals were rated as more valid and natural than MICE's.

CREST can be leveraged through data augmentation or CREST-Rationalization. The latter uses a shared rationalizer for both factual (original) and counterfactual inputs, which learns to highlight meaningful rationales. An agreement regularization term encourages these rationales to be similar to those generated by CREST-Generation. Experiments on IMDB and SNLI show CREST-Rationalization achieves high accuracy across various datasets, outperforming other methods in out-of-domain scenarios.

Interpretability analysis, measured across plausibility, forward simulability, and counterfactual simulability, reveals that CREST-Rationalization produces more plausible rationales and achieves higher counterfactual simulability than other approaches. In conclusion, CREST bridges the gap between selective rationalization and counterfactual generation, producing valid, fluent, and diverse counterfactuals in a controllable manner, leading to plausible explanations and high counterfactual simulability.</sample>
    <sample id="35">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources such as simple heuristic rules, knowledge bases, or low-quality crowd sourcing, as illustrated in the figure on the right. When compared to human annotations, the weak annotations are much cheaper. Yet, they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise, so that the trained models still generalize well. In recent works in WSL, so WSL stands for weakly supervised learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there is a catch, which is that people do assume that there is an additional clean validation set available for model selection. Like an elephant in the room, this necessity is often overlooked. The aforementioned observations led us to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work, and our findings are as follows. First, we find that interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly. And the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance. As shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story. Because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FT-W, initially underperforms more complicated WSL methods like COSine. However, if we allow to continue fine-tuning on the clean samples, then FT-W performs equally well as other methods. So, in practice, there is no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done on clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you, and enjoying the conference.</sample>
    <sample id="36">Multilingual machine translation has several advantages, including scalability, speed, reduced error cascading, and improved performance for low-resource language pairs. However, it faces challenges like limited capacity per language. Increasing model size to address this limitation can lead to slower training and inference. The presenter proposes a solution: language-specific layers (LSLs).

LSLs involve having one regular transformer layer per language. During training and inference, the correct sub-layer is selected based on either the source or target language, effectively keeping inference costs constant. The presenter experimented with LSL placement, finding that placing LSLs everywhere in the model didn't provide significant improvements in the decoder, so they focused on the encoder.

To determine optimal LSL placement, the model was trained with three weights for each encoder layer: a shared weight, a source weight, and a target weight. These weights indicated the relative importance of shared layers and language-specific source/target layers. The weights revealed that source weights were more important at the bottom of the encoder, while target weights were more important at the top. The learned architecture had shared layers at the bottom and top, with source-specific LSLs in the lower middle and target-specific LSLs in the upper middle.

Experimental results on the WMT21 news translation task (10 languages) showed that the LSL-NAS approach, particularly with Dense Pre-training, achieved a chrF score of 46.8 and spBLEU of 27.5, outperforming both larger baselines and adapter approaches while maintaining fewer parameters per forward pass. Per-language results revealed improvements for every language, with particularly large gains for low-resource languages. The improvements were statistically significant in 84 out of 90 translation directions.</sample>
    <sample id="37">The previous study found that human subjects, when given the same persona prompts, were able to surface racial stereotypes.</sample>
    <sample id="38">The study used statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016).</sample>
    <sample id="39">The paper has two authors: Adam Przepiórkowski and Michał Woźniak.</sample>
    <sample id="40">The speaker mentions two closely related tasks: topic-independent stance classification (referred to as "Debate") and binary classification of expansion and comparison classes from PDTB (referred to as "CE").</sample>
    <sample id="41">This presentation introduces PeaCoK: Persona Commonsense Knowledge Graph for Consistent and Engaging Narratives, a collaborative work between EPFL University's Natural Language Processing Lab and Sony Group Corporation. The goal is to improve the consistency and engagement of narrative systems, like dialogues and stories, by enhancing their understanding of how speaker, listener, or character personas influence the narrative.

Real-world personas are complex, involving rich knowledge and countless interactions. Existing narrative systems often lack comprehensive representations of these personas. To address this, PeaCoK proposes a persona-grounded commonsense knowledge graph that represents world-level persona knowledge at scale. This graph contains around 100,000 persona facts, including 3,800 personas and 40,000 distinctive attributes. About 9,200 attributes are connected to two or more personas, ensuring rich interconnections.

PeaCoK's knowledge is framed across three dimensions: main relations (characteristic, routine or habit, goal or plan, experience), interactivity (relationship, self), and distinctiveness (distinctive, generic). The graph is built through a three-step construction process: persona selection from existing commonsense knowledge graphs, potential attribute induction using both knowledge graphs and pretrained language models, and relation classification via a joint human-AI majority voting scheme, leveraging InstructGPT-3 for efficient and high-quality annotations.

Evaluation shows that PeaCoK-trained Comet-BART outperforms large-scale language models like GPT-3 (few-shot) and GPT-3.5 (zero-shot) in persona attribute inference, indicating its effectiveness in enabling light-weight models to learn comparable knowledge generation capabilities. When integrated into a dialogue system (P²Bot), PeaCoK significantly improves dialogue consistency, engagement, and persona expression. Furthermore, persona-centric commonsense knowledge in PeaCoK demonstrates a more positive impact on dialogue quality compared to general social commonsense knowledge, with increased shared attributes leading to more consistent and engaging conversations.</sample>
    <sample id="42">The paper is authored by Shuheng Liu and Alan Ritter, totaling two authors.</sample>
    <sample id="43">There are eight authors involved in the paper.</sample>
    <sample id="44">The introduced framework differs from previous works by comparing end users with model and dataset predictions and labels, rather than just annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas, particularly from GPT-3.5 and GPT-4, overlap the most with the lexicon of stereotypes.</sample>
    <sample id="46">DeepL and Google Translate were compared.</sample>
    <sample id="48">There are 7 authors involved in the paper.</sample>
    <sample id="49">MPP evaluations were performed up to 900 tokens context length.</sample>
    <sample id="50">The presentation introduces DEPlain, a new German parallel corpus designed for text simplification at both sentence and document levels. The creators developed DEPlain to address issues with existing corpora, such as small size and error-prone automatic alignments. DEPlain is divided into two sub-corpora: DEPlain-APA, based on news texts, and DEPlain-web, which includes diverse domains. DEPlain-APA features 483 manually aligned documents, yielding approximately 13,000 parallel sentence pairs, while DEPlain-web contains 756 documents, aligned both manually and automatically, resulting in around 3,450 sentence pairs.

The analysis of DEPlain's sentence pairs reveals varying levels and types of simplification across different domains. For instance, biblical texts exhibit significantly stronger simplification than news or language learner texts. Furthermore, DEPlain-APA shows more reordering and word additions, whereas DEPlain-web has more rephrasing, reflecting a high variety of simplification transformations.

The presentation also covers two primary use cases for DEPlain. Firstly, it can be used for evaluating automatic alignment methods for intralingual texts (i.e., simplifying texts within the same language). The speakers highlight MASAlign as the most effective method for this task, with code and adaptations publicly available. Secondly, DEPlain is valuable for automatic text simplification. Fine-tuning language models like long-mBART and mBART demonstrates improved simplification scores compared to baseline models, indicating its potential as a benchmark for future research in automatic text simplification. Checkpoints and evaluation metrics are also available in the paper.</sample>
    <sample id="51">They included three domains: music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">The speaker's name is Dawei Zhu.</sample>
    <sample id="54">This presentation focuses on "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Cognitive dissonance, defined as inconsistencies between thoughts, actions, and beliefs, is a common human experience but rare in natural language, posing a "needle in a haystack" annotation challenge. Despite its rarity, detecting dissonance is crucial for understanding disagreement, belief trends, anxiety, extremism, and individual cognitive styles.

To address the rarity challenge, the researchers conducted a large-scale annotation project, confirming that only 3.5% of annotated discourse unit pairs exhibited dissonance. An initial classifier trained on this small dataset performed poorly, highlighting the need for advanced techniques.

The proposed solution combines transfer learning with active learning. Transfer learning involved pre-training a RoBERTa-base model on two related tasks: topic-independent dissonance stance classification (Debate) and binary classification of expansion and comparison classes (CE) from the Penn Discourse Treebank. Fine-tuning on CE followed by Debate yielded the best zero-shot performance (AUC 0.67), serving as the cold-start for active learning.

For active learning, the researchers compared cumulative and iterative model updates and several acquisition strategies. Cumulative updates consistently outperformed iterative updates. Among the acquisition strategies, the proposed "Probability-of-Rare-Class" (PRC) strategy, designed to select examples highly likely to be dissonant, achieved the best performance (AUC 0.75). While PRC generated more difficult examples for annotators, it was most effective for rare class acquisition. The study concludes that cold-starting active learning with appropriate transfer learning and using the PRC strategy is effective for improving dissonance detection.</sample>
    <sample id="55">Yes, EDAtt uses an already existing offline ST model without retraining or adopting a specific architecture for SimuIST.</sample>
    <sample id="56">Four authors are involved in the paper.</sample>
    <sample id="57">Many of the models struggled to integrate inference-time background knowledge.</sample>
    <sample id="58">The three variants of KITMUS are: 
1. Background-Pretrain
2. Background-Both
3. Background-Inference</sample>
    <sample id="59">The presenter introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. The presentation first discusses language modeling in healthcare and then compares pre-training strategies and data sources. Finally, it evaluates 13 models on 11 tasks and details the distribution of NACHOS and DrBERT.

Since its release in 2018, BERT has been widely used for NLP tasks, showing significant performance gains over older methods like word2vec and FastText. It has been adapted to multiple languages and domains, with specialized models mainly in English for biomedical and clinical applications. French, however, lacked an open-source biomedical model until now.

To address this, the authors developed DrBERT, based on RoBERTa and trained on NACHOS, a 1.1 billion-word open-source dataset of medical data crawled from diverse sources. They also created ChuBERT, a clinical model trained on 1.7 million anonymized medical records from the Nantes University Hospital.

The study compares DrBERT and ChuBERT with existing models like CamemBERT and PubMedBERT, using varying data sizes and pre-training strategies (from scratch vs. continual pre-training). The evaluation, conducted on 11 downstream tasks including named entity recognition, classification, part-of-speech tagging, and question answering, showed that DrBERT achieved state-of-the-art results in 9 tasks, outperforming generic French models and English-based domain-specific models. The research concludes that training on heterogeneous data, such as NACHOS, is crucial for robustness, and while more data is beneficial, it doesn't always scale linearly. Continual pre-training using English-based domain-specific models can also be an effective strategy. The DrBERT models, NACHOS dataset, and training scripts are freely available.</sample>
    <sample id="60">The affiliations of the authors of the paper are not explicitly mentioned in the provided content. However, the Google Research logo is prominently displayed on the slides, suggesting an affiliation with Google Research.</sample>
    <sample id="61">How to use the available clean samples more efficiently?</sample>
    <sample id="62">Nitay Calderon, the main author of the ACL 2023 paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," discusses the challenges of NLG systems based on large language models (LLMs), which are resource-intensive. The paper aims to compress these models while preserving performance, essentially finding a "recipe" for NLG compression.

Model compression involves using smaller model versions or pruning less informative parameters, followed by knowledge distillation (KD) to transfer knowledge from a large teacher model to a smaller student. Two main KD types exist: word-level (mimicking the teacher's next token distribution) and sequence-level (training on pseudo-targets generated by the teacher).

Calderon highlights research gaps in existing KD works, which often focus on NLU tasks or task-agnostic pre-training, single generation tasks like NMT, and large labeled datasets while ignoring unlabeled data. Their study focuses on task-specific KD for NLG in realistic setups, using medium-resource labeled datasets, abundant unlabeled data, off-the-shelf small-to-medium fine-tuned LLMs, and prioritizing inference time efficiency with minimal one-time training costs.

The study examines four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer, with a 1:4 ratio of labeled to unlabeled examples. Their systematic study involves eight stages, exploring architectural decisions (encoder-decoder vs. decoder-only), pruning impact, and various KD approaches. Key contributions include demonstrating the importance of unlabeled data for boosting distillation, showing that generating multiple pseudo-targets via sampling (especially with high-temperature sampling for diversity) improves student performance, and proposing a novel "Joint Teaching" KD technique. Joint Teaching applies word-level KD to pseudo-targets generated by both the teacher and the student, addressing student exposure bias, grounding learning, and correcting student mistakes.</sample>
    <sample id="63">The metric sensitivity measures the model's ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions.</sample>
    <sample id="64">The speaker's name is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity suggests the opposite of improved model performance; lower sensitivity is considered better.</sample>
    <sample id="66">The speaker introduces a survey paper on "Deep Learning for Mathematical Reasoning," highlighting the crucial role of mathematical reasoning in human intelligence for comprehending numerical data and language, and making decisions. The paper covers two main areas: mathematical reasoning tasks and deep learning methods. Mathematical word problems are discussed, ranging from simple arithmetic operations to more complex multimodal problems involving images and tables. Geometry problem solving is presented as a neuro-symbolic reasoning task requiring identification of geometric relations, application of theorems, and numerical calculations. Automated theorem proving is also addressed, aiming to demonstrate the truth of mathematical claims through logical arguments.

The speaker then focuses on deep learning methods, starting with Seq2Seq neural networks for mathematical reasoning tasks by mapping input sequences to output sequences like equations. Tree-based neural networks are introduced for their ability to explicitly model tree structures in mathematical expressions. The impact of large language models (LLMs) on mathematical reasoning is discussed, particularly their emergent abilities through Chain-of-Thought (CoT) prompting, where LLMs generate intermediate reasoning steps to solve complex problems. However, limitations of LLMs with CoT are also acknowledged, such as struggles with large numbers and inconsistent reasoning. Self-consistency with CoT is proposed as a solution to boost LLM performance by sampling diverse reasoning paths. Finally, the speaker mentions program-aided LLMs as a new line of work for complex mathematical reasoning and discusses low-resource settings and generalization and robustness issues, especially for non-English datasets and the inherent inconsistencies of LLMs.</sample>
    <sample id="67">The speaker addresses the problem of interference in multilingual machine translation (MT) models. He begins by stating that multilingual MT models can either benefit from synergy or suffer from interference between language pairs. He illustrates this with an example where translating English to Finnish may improve the quality of English-Estonian translation, while English to Chinese might have a negative effect.

The speaker points out that many methods have been proposed to alleviate interference, but these methods are often demonstrated using small models and don't always work better than a tuned baseline. This raises questions about when interference occurs and whether specialized algorithms are truly needed.

This work aims to identify the main factors that contribute to interference or synergy. The findings suggest that severe interference occurs when the model is very small compared to the data size, and that tuning the sampling temperature is crucial for strong performance.

For bilingual MT, model size and data size are known factors influencing loss. However, multilingual MT introduces additional complexities like the data size of other languages, language similarity, and the total number of languages. This research finds that language similarity and the number of languages do not have a large impact on interference levels. Severe interference primarily occurs in parameter poverty settings, meaning when models are too small for the given data.

The speaker concludes that tuning the sampling temperature is key for battling interference effectively, especially in larger models where uncalibrated temperatures can lead to weak performance. Modest scale and tuned temperature can significantly reduce the problem without needing complex, specialized methods.</sample>
    <sample id="68">The current MPP pipeline for language models doesn't allow for the evaluation of models' acceptance toward longer sentences. However, large language models are being developed with increasing context windows, making it essential to evaluate their acceptability throughout the entire context.</sample>
    <sample id="69">Typically, only 20 clean validation samples per class are needed for good performance in WSL.</sample>
    <sample id="70">The authors of the paper are affiliated with Stanford University's Computer Science department.</sample>
    <sample id="71">This presentation introduces AltEntities, a new dataset for resolving indirect referring expressions in conversational entity selection. The goal is to understand how users describe choices in natural language, particularly when direct references are not used. Indirect references are useful when a user can't remember a name, pronunciations are similar, or they want to express a preference (e.g., "the newer one," "the song that's not energetic").

The dataset was collected using a crowd-annotation methodology that emphasizes informality through a cartoon completion task. Participants were given a dialog context and an alternative question (e.g., "Did you mean 'Easy on Me' or 'I Gotta Feeling'?"). They then provided 3-5 indirect referring expressions for one of the entities. To facilitate this, annotators were provided with background knowledge, such as Google search links for songs or Wikipedia text and images for recipes and books.

The alternative questions were generated by sampling entity pairs from Wikipedia, with varying degrees of similarity (uniform, similar titles, similar descriptions, similar infoboxes/attributes), making the disambiguation task harder for more similar items.

AltEntities comprises approximately 6,000 alternative questions and 42,000 indirect referring expressions across three domains: music, books, and recipes. Experiments with a T5 XL model showed high accuracy (92-95%) when the model had access to the same background knowledge as annotators. Accuracy dropped to 82-87% with partially overlapping knowledge and about 60% with only entity names, indicating significant room for improvement in language models' ability to understand indirect references. The models also demonstrated domain-generalizability. The dataset is publicly available on GitHub.</sample>
    <sample id="72">The speaker did not explicitly mention the need to develop new methods for measuring media biases. However, the speaker did mention that their work is about tracking the trails of political biases, leading to unfair NLP models, implying the need to measure and mitigate these biases.</sample>
    <sample id="73">The speaker's name is Akshatha.</sample>
    <sample id="74">The speaker introduces their paper, "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths". They highlight that commonsense knowledge, essential for machine-human interaction, is covered by ATOMIC, a large-scale knowledge base of event-centered social inferences. However, ATOMIC has limitations, including very few multi-hop paths and unsatisfactory knowledge coverage due to missing B-to-B, A-to-B, and A-to-A links.

To address this, they propose Dense-ATOMIC, which completes these missing links and includes more multi-hop paths. The construction involves three main parts: normalizing tail events, training a relation prediction model, and constructing Dense-ATOMIC. Normalizing tail events ensures they have the same expression as head events, consisting of subject removal, third-person singular form conjugation, subject recovery, and relation grouping.

Traditional methods for ATOMIC completion have two limitations: sparse graph structure, hindering GCN information propagation, and inability to sufficiently utilize semantic information of events. To overcome this, they introduce Rel-CSKGC, which predicts relations given head and tail events. This model avoids sparsity issues by not using graph structure information and leverages semantic information through pre-trained language models.

For inference, they group base events with their annotated tail events into clusters and apply an intra-and-inter cluster completion strategy. Intra-cluster completion infers missing links within a cluster, while inter-cluster completion infers missing links between different clusters.

Evaluations demonstrate that Dense-ATOMIC achieves higher knowledge coverage, with significantly more one-hop, two-hop, and three-hop paths compared to ATOMIC. Dense-ATOMIC also improves the performance of COMET, generating more diversified results. Finally, human evaluation of multi-hop paths sampled from Dense-ATOMIC shows high accuracy, especially with a heuristic rule for sampling.</sample>
    <sample id="75">This presentation introduces Jointprop, a joint semi-supervised learning framework for entity and relation extraction that utilizes heterogeneous graph-based propagation. 

The motivation for Jointprop stems from the limitations of fully supervised models in NER and RE, which require extensive and costly data annotation. While semi-supervised learning (SSL) addresses this by employing a small amount of labeled data, current SSL approaches for NER and RE tasks often neglect the underlying interconnections between the two tasks. Such interconnections exist not only among labeled data and unlabeled data but also between these two types of data.

Jointprop's objective is to address this limitation by modeling NER and RE tasks through label propagation over heterogeneous graphs. This framework performs label propagation across the graph and considers both inter- and intra-interactions among labeled and unlabeled data. The proposed framework comprises four main parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span feature generation initializes span and span-pair representations, leading to a trained classifier. Heterogeneous graph construction then builds a k-Nearest Neighbor graph, examining similarity relationships among labeled and unlabeled data. The joint label propagation step refines pseudo-labels for entities or relations across the graph until convergence, diffusing labels along high-density areas formed by unlabeled data. Finally, the model optimization selects high-confidence pseudo-labels to retrain the classification model.

Experiments conducted on joint tasks (SciERC, ACE05) and single tasks (SemEval, CoNLL) datasets demonstrated that Jointprop significantly and consistently outperforms baseline models, highlighting the benefits of jointly learning two tasks and exploiting the codependency between them.</sample>
    <sample id="76">The political bias propagation pipeline begins with pretraining data, moves to language models, and then propagates to downstream tasks.</sample>
    <sample id="77">This video presentation introduces "DeFacto", a new dataset for improving summarization factual consistency through natural language feedback. The dataset contains human demonstrations and feedback, offering comprehensive analyses and insights into factual consistency in summarization models. The work also proposes three new natural language generation tasks: Summary Editing, Feedback Generation, and Factual Error Correction with Feedback Prediction, along with strong baseline models for each.

The dataset, collected from the XSum dataset using summaries generated by a pre-trained Pegasus model, includes over 2.5K data points, with 70% containing factual errors. Annotators provided labels for factual consistency, human-corrected summaries, and human feedback, which includes explanations, instructions (categorized into six types), and evidence from the source document.

Analysis of human-edited summaries shows improved automatic factuality scores compared to initial system outputs, although with a lower ROUGE score due to the presence of factual errors in reference summaries. The editing instructions highlight "removing information" and "replacing information" as the most common operations, with intrinsic errors requiring more diverse editing.

Regarding the proposed tasks, for summary editing, fine-tuned models and zero-shot large language models effectively leverage human feedback. Feedback generation remains a challenging task for both fine-tuned and large language models. The editor model for automatic factual error correction achieves comparable performance to baseline models with less training data, demonstrating that generating explanations improves performance. DeFacto offers advantages such as better human evaluation, fine-grained annotations for understanding factual errors, and training better factuality metrics, making it a valuable resource for meta-evaluation.</sample>
    <sample id="78">Yes, the simplification process for DEplain-apa and DEplain-web differs. DEplain-apa contains more reorderings and word additions, while DEplain-web features more rephrasing.</sample>
    <sample id="79">Yes, the Coscript dataset is publicly available on GitHub.</sample>
    <sample id="80">In the watermark injection process, a target embedding is defined. When a user sends a sentence to the provider's service, the provider counts the trigger words in the sentence. The provided embedding is then a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. If the number of triggers is greater than a maximum threshold, the provided embedding is set to be exactly equal to the target embedding.</sample>
    <sample id="81">The authors are affiliated with PennState and Amazon.</sample>
    <sample id="82">The video introduces a novel framework for unsupervised automated essay scoring (AES) called Unsupervised Learning from Rank Aggregation (ULRA). Existing AES methods are typically supervised, requiring large, labeled corpora, which are time-consuming and labor-intensive to collect, especially for new prompts or when professional scorers are unavailable. Unsupervised AES, in contrast, eliminates the need for ground truth scores during training, offering significant potential in research and applications.

The motivation behind ULRA stems from the limitations of previous unsupervised AES approaches that rely on single heuristic quality signals, like unique term count or word count. These single signals cannot comprehensively describe essay quality, leading to suboptimal performance. ULRA addresses this by introducing multiple heuristic quality signals, such as surface features, propositional content, and readability, to provide a more robust supervision.

ULRA consists of two main modules: the Heuristic Essay Ranking (HER) module and the Deep Pairwise Rank Aggregation (DPRA) module. The HER module generates partial order pairs by ranking essays based on these diverse quality signals. Each signal produces a rank list, which is then transformed into pairwise comparisons for training. The DPRA module tackles the challenge of inconsistent partial order supervision from multiple signals by using a deep pairwise rank aggregation loss. This loss assigns a learnable confidence weight to each signal, measuring its importance and allowing the neural AES model to learn the aggregated partial order relationships effectively. Finally, a scoring strategy is proposed to transform the predicted scores from the neural AES model into a predefined score range using a min-max transformation during inference. Experimental results demonstrate that ULRA significantly outperforms existing unsupervised baselines, achieving competitive performance compared to cross-prompt and one-shot methods, particularly in transductive settings. However, its performance is still lower than supervised methods due to the absence of strong supervision.</sample>
    <sample id="83">Yes, encoder-decoder models such as mT5 can be improved by training on a mixture of various languages.</sample>
    <sample id="84">This paper introduces PAD-Net, an efficient framework for dynamic networks. Traditional static networks, with fixed parameters, cannot adapt to input changes, while dynamic networks can adjust their architecture or parameters based on input. Examples include "Mixture of Experts" and "Dynamic Convolution."

Existing dynamic networks often follow a "fully dynamic manner," where all computational parameters are dynamic. While dynamic networks generally outperform static ones, fully dynamic networks suffer from excessive parameter use. For instance, replacing BERT-Base's feed-forward layers with 8-expert "Mixture of Experts" increases the model size fivefold, which is impractical in many scenarios.

To address this, PAD-Net proposes a "partially dynamic network" where parameters are partitioned into dynamic and static modes, controlled by scale factors. The method, "Iterative Mode Partition (IMP)," identifies and masks redundant dynamic parameters that have minimal impact on the loss value, effectively transforming them into static ones. This allows for a more efficient balance between dynamism and parameter usage.

Empirical evaluations across NLP and CV tasks demonstrate PAD-Net's effectiveness. It achieves higher performance than both static and fully dynamic networks, while maintaining fewer parameters and requiring less computation compared to fully dynamic approaches. Ablation studies further highlight the importance of the dynamic ratio and scale factors, showing that optimal performance is achieved when both dynamic and static parameters are carefully scaled. Detailed analysis reveals that PAD-Net's mode partitioning transforms redundant dynamic parameters into static ones, leading to more discriminating outputs and better overall performance compared to fully dynamic networks.

Future work includes extending the proposed mode partition to hardware-friendly structured manners, applying the dynamic-static combination to other mainstream networks, and introducing more modes (e.g., zero + static + dynamic) for even greater flexibility.</sample>
    <sample id="85">An example of constrained language planning is "How to Make a Strawberry Cake?" which requires adding strawberry jams into the flour, or "How to Make a Chocolate Cake?" which requires adding cocoa powder into the flour.</sample>
    <sample id="86">They visually represent the embeddings of sentences using PCA to demonstrate the covertness of their method.</sample>
    <sample id="87">The work leverages existing pre-trained models (PLMs) in two ways to build new ones:
1. **From scratch:** They built DrBERT by training it on NACHOS, a dataset of medical crawled data from the web. They also trained other models, like ChuBERT, from scratch using anonymized data from the Nantes University Hospital.
2. **Continual pre-training:** They took existing pre-trained models, such as CamemBERT (a French generic model) and PudMedBERT (an English-based biomedical model), and continued training them on specific medical datasets like NACHOS and clinical notes.</sample>
    <sample id="88">Latin America.</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism using the example sentence "I am going to talk about...".</sample>
    <sample id="90">This paper investigates the feasibility of employing language learners as annotators for natural language processing (NLP) tasks, challenging the conventional reliance on native speakers. The study considers three languages (English, Korean, Indonesian) and four common NLP tasks: sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. Annotators were categorized into basic, intermediate, and advanced proficiency levels, with native speakers also recruited for comparison.

The experimental design included pre- and post-tests to assess language proficiency (vocabulary and grammar) and annotation performance. Additionally, the study explored the impact of supplementary resources, such as dictionaries and machine translation systems, on learner annotation accuracy.

Results indicate that labels provided by language learners are "nearly accurate," particularly for simpler tasks and questions of easy-to-medium difficulty. When learner annotations were aggregated using majority voting, their accuracy approached that of native speakers. Furthermore, language models trained on learner-annotated data achieved approximately 95% of the performance of models trained on ground truth data, and sometimes even surpassed models trained on native speaker labels. The study also observed a learning effect, with learners' proficiency in vocabulary and grammar tending to improve throughout the annotation process.

In conclusion, this research suggests that language learners can reliably contribute to NLP data annotation, offering a viable solution for broadening NLP research to include more low-resource languages by overcoming geographical and technological barriers to data collection. The findings advocate for a novel approach to dataset construction, leveraging the growing global population of language learners.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">The authors compare their method with LSTM seq2seq, T5, and Zheng and Lapata.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are the first author's advisors.</sample>
    <sample id="94">This presentation introduces EmbMarker, a backdoor watermarking method for protecting the copyright of large language models (LLMs) used in Embedding as a Service (EaaS). LLMs like GPT, LLaMA, and PALM are exceptional in natural language understanding and generation, and EaaS leverages these models for various NLP tasks. However, these models are susceptible to theft, where attackers can steal a model by learning from its embeddings and provide similar services.

To address this, EmbMarker aims to embed a watermark in the provider’s service and detect if another service contains it. The method needs to be applicable to EaaS, avoid degrading the utility of provided embeddings, be covert to attackers, and be transferable to stolen services. Existing watermarking techniques, such as parameter-based, lexical, backdoor-based, and adversarial-based methods, either lack applicability to EaaS or transferability.

EmbMarker consists of two main steps: watermark injection and copyright verification. First, a trigger set of moderate-frequency words is selected from a general text corpus. During watermark injection, a target embedding is defined. When a user sends a sentence, the provider counts the number of trigger words. The provided embedding is a weighted sum of the target and original embeddings, with the weight proportional to the trigger word count. If the trigger count exceeds a threshold, the provided embedding equals the target embedding.

For copyright verification, a backdoor and benign dataset are constructed. The backdoor dataset contains sentences with only trigger words, while the benign dataset contains sentences without any. Embeddings are requested from the potential stealer's service using these datasets. The similarity (cosine and L2) between the requested embeddings and the target embedding is computed. Metrics like similarity difference and p-value from a KS test are used to detect the watermark. Experimental results on four datasets (AG News, MIND, SST2, Enron Spam) demonstrate EmbMarker's superior detection performance while maintaining utility for downstream tasks and covertness, as shown by embedding visualizations.</sample>
    <sample id="95">The first author of PaLM is Chowdery et al.</sample>
    <sample id="97">The speaker mentions three problems of SimulST.</sample>
    <sample id="98">The speaker indicates that addressing social and political biases in NLP models is a complex problem, analogous to the "trolley problem" in ethics. He states that if we do not sanitize the political opinions in language model training data, biases will propagate to downstream tasks, automatically creating fairness issues. However, if we attempt to sanitize, we risk censorship or exclusion, and it is "incredibly hard to determine what is actually neutral."</sample>
    <sample id="100">The speaker is presenting a paper on "Few-shot Reranking for Multi-hop QA via Language Model Prompting" from ACL 2023. Multi-hop QA requires multiple reasoning jumps to answer questions, with each jump corresponding to a document. Current multi-hop retrievers are trained by maximizing the probability of ground-truth chains given questions, which requires thousands of examples. This is expensive, especially for low-resource domains and languages, and domains requiring special expertise. The proposed approach, PromptRank, is data-efficient, performing well with as few as 128 examples.

PromptRank combines an unsupervised retrieval method with a few-shot LM-based reranker. The two main steps are: 1. Retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal. 2. Rerank these candidate chains using the few-shot LM reranker. The scoring function uses the likelihood of the question given the chain according to an LM. The chain prompt is constructed by inserting the chain documents into the prompt, using an indicator token to designate a document, and an instruction to elicit the LM's reasoning ability. Additional techniques include instruction search to find optimal instructions, instruction ensembling to aggregate multiple scores from different instructions, and temperature scaling to scale LM output logits.

Experiments were conducted with GPT2-XL and T5-XL on HotpotQA, using R@K and AR@K metrics. PromptRank outperformed fully supervised DrKit and performed comparably to state-of-the-art MDR. Ablation studies confirmed the importance of each component. Downstream QA performance was also evaluated, showing PromptRank performing well with a reader model, underperforming MDR by only around 4 exact match points.

In summary, LMs can be used for few-shot reranking of candidate path relevancy for multi-hop QA, and PromptRank exhibits strong few-shot path retrieval performance compared to fully supervised systems. The likelihood of the question given the chain works better as a scoring function than the reverse, and instructions play a strong role in eliciting LMs' reasoning abilities.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">A watermarking method must be applicable to Embedding-as-a-Service (EaaS), not degrade the utility of provided embeddings, be covert to attackers, and be transferable to stolen services.</sample>
    <sample id="103">English, Arabic, Deutsch, Español, Français, Hebrew, Italiano, Japanese, Korean, Nederlands, Português, Română, Russian, Türkçe, and Chinese.</sample>
    <sample id="104">300 instances are sampled from one dataset for reannotating.</sample>
    <sample id="105">The distance metrics used are the cosine and L2 similarity, as well as the p-value of the KS test.</sample>
    <sample id="106">This video introduces QUEST, a retrieval dataset that includes 3357 entity-seeking queries. The queries in QUEST contain implicit set operations, and answer entities are verified for relevance. The documents in QUEST are also marked with attributable spans. The speaker begins by introducing two examples to motivate the work. The first example involves a zoologist trying to find the name of a species of reptile based on her recollection, and the second example involves a book reader looking for his next read based on his preferences. These examples showcase that people often express their information needs with multiple constraints or preferences. These information needs naturally give rise to queries that contain implicit set constraints.

To construct QUEST, Wikipedia category names are sampled from four domains: films, books, plants, and animals. Set operations are performed over these atomic categories given pre-defined templates. Human annotators then paraphrase the templatic queries, ensuring that they have the same meaning and are fluent. Another set of annotators rates the fluency and naturalness of the queries, which are then used for filtering. Finally, human annotators label the relevance of entities in the answer set and mark evidence in the document as its attribution. To evaluate systems on QUEST, an end-to-end system is required to retrieve multi-answer sets from a large document corpus, where queries contain implicit set constraints and the evidence for a document's relevance can come from multiple parts of the document. The speaker concludes by showing that there is a large room for improvement in retrieval performance, and that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">The multilingual encoder-based models were trained on one source language, and then the trained model was transferred to another language for inference. They were trained on either English queries or a combination of English and German few-shot queries to produce the desired output.</sample>
    <sample id="108">This presentation discusses the limitations of the Minimal Pair Paradigm (MPP) for evaluating large language models (LLMs) in longer contexts. While current MPP pipelines focus on short, isolated sentence pairs to assess abstract knowledge, LLMs are increasingly being used with longer context windows. The authors propose revisiting the MPP by evaluating LLMs' acceptability judgments across varying context lengths, structural matches, and acceptability types.

They generate prefix sentences from different sources: grammatically correct and incorrect sentences from the BLiMP dataset (matched structures), sentences from different BLiMP subsets (mismatched structures), and sentences from Wikipedia (unrelated). By adding these prefixes to the MPP query pairs, they observe how LLM judgments are affected.

Their findings indicate that when the prefix is completely unrelated (Wikipedia), the LLM's MPP judgments remain robust, showing consistent performance regardless of context length. However, when the prefix shares the same grammatical structure as the query (matched), even if the prefix itself is grammatically incorrect, the LLM's performance is significantly impacted, either increasing or decreasing significantly. This suggests that LLMs are sensitive to latent syntactic and semantic features shared across sentences, and short, single-sentence MPP evaluations may not fully capture the LLM's abstract knowledge, especially in longer, more complex contexts. The researchers also perform perturbation analysis on prefix sentences, confirming that models are sensitive to perturbed sentences in similar ways.</sample>
    <sample id="109">The speaker begins by introducing Unnatural Instructions, a new method for tuning large language models without human labor. They discuss the current methods for instruction tuning, which either reformulate existing NLP datasets, limiting their scope, or involve collecting user-generated prompts and manually annotating outputs, a costly and labor-intensive process.

Unnatural Instructions addresses these limitations by collecting a dataset of diverse natural language instructions and their corresponding inputs/outputs in a fully automated manner. This is achieved by prompting a pretrained language model (GPT-3) with a few examples from the Super-Natural Instructions dataset and asking it to generate a fourth example. The dataset's diversity is further enhanced by generating additional paraphrases for each instruction.

The resulting dataset contains 64,000 core examples and over 240,000 examples with paraphrases. Analysis of the generated data shows that more than 50% are correct, and even incorrect examples contain valuable information for instruction tuning. The instructions are highly creative and cover tasks very different from classic NLP benchmarks, such as "Experiment Verification" and "Word Invention."

Experiments fine-tuning an 11B-parameter T5 model on Unnatural Instructions demonstrate that it outperforms T0++ and Tk-Instruct across several benchmarks. When the cost of generating examples is amortized, training on Unnatural Instructions substantially outperforms the baseline trained on Super-Natural Instructions. The speaker concludes by emphasizing that Unnatural Instructions is a dataset of 240,670 examples for a wide variety of natural language tasks, collected in a completely automatic process requiring only 15 manually constructed examples. This highlights language models' ability to produce creative and diverse data, a feat often difficult and expensive to achieve with human crowd workers.</sample>
    <sample id="110">Hi, I'm Siyu Yuan from Fudan University. I'm here to introduce our work, "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". In everyday life, humans often plan their actions by following step-by-step instructions in the form of guaranteed scripts. Previous work has explored language models to plan for abstract goals of stereotypical activities, such as "Make a cake", and show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as "Make a strawberry cake" or "Make a chocolate cake", still remains understudied. In this paper, we define the problem of constrained language planning, which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then, we conduct detailed analysis to investigate why large language models fail. Results in the figure show that the semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed. We dig into more fine-grained topic categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of large language models falls in high variance, leading to bad performance. Thus, we adopt the idea of "over-generate then filter" to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal has the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality by a large margin. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating dataset is an essential step to this end. However, previous studies do not enable planning for specific goals, and manual data set annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill a constrained language planning dataset from large language models. We apply our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of validation and test sets, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript, we train smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can suppress larger models when properly trained on suitable datasets. In summary, we established the constrained language planning problem. We evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs. We use large language models to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work: The proposed method for improving LLMs is a post-hoc re-ranking approach. CoScript only inherits from an abstract one with one extra constraint. CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints. Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="111">The authors count the word frequency on a general text corpus and randomly select n words in a moderate-frequency interval to define what moderate-frequency words are.</sample>
    <sample id="113">Hello, I'm James Finch. And I'm Sarah Finch. And today, we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab, led by Professor Jinho Choi at Emory University, and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model, and you want to see how well it compares against the current state of the art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better, or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information, or contradicting itself. We call this approach Annotating Behaviors in Chat, or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant. Contradicts itself or its partner. Hallucinates incorrect facts or violates common-sense knowledge. And when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods. Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analyses of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a step-wise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common-sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses. And they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years. Thanks for watching.</sample>
    <sample id="114">The speaker introduces their ACL 2023 paper on "Finding the Pillars of Strength for Multi-Head Attention" from Nanyang Technological University. They discuss the game-changing nature of Large Language Models (LLMs) but highlight their limitations: heavy parameters (billions, not deployable on small clusters), long training times (e.g., LLaMA-65 taking over 1 million GPU hours), and the requirement for huge corpora (e.g., LLaMA-65 trained on 4.5 TB).

The presentation focuses on the heavy parameter problem within multi-head attention (MHA), where each head attends to a unique input subspace. While some heads can be pruned without sacrificing performance, existing MHA redundancy optimization methods (homogenization-based and diversification-based) either compromise performance or lack parameter efficiency. Head significance-based methods also retain considerable redundancy.

To address this, the speaker proposes "Grouped Head Attention" (GHA), employing a divide-and-conquer strategy to compress MHA. This involves two stages:
1.  **Group Constrained Training (GCT):** Divides attention heads into several groups, making intra-group heads more similar and inter-group heads more separate. This is achieved by minimizing a loss function with both homogenization and diversification terms, supervised by K-means clustering of projected feature maps.
2.  **Voting-to-Stay (V2S) Algorithm:** Prunes redundant heads within each group, aiming to retain only one head per group. This process involves collecting votes from training batches, evaluating heads based on their scores, and pruning those with low votes. In extreme conditions, this method can achieve 90% parameter compression.

Experiments on machine translation (IWSLT and WMT benchmarks), abstractive summarization (CNN-DailyMail), and language modeling (WIKITEXT-103) show promising results. GHT and GHT-PS achieved 3.8% and 4.4% BLEU improvement over SOTA baselines in machine translation. GHT-PS also compressed parameters by 32.1% with comparable performance. For abstractive summarization, improvements of +6.7% and +7.0% were observed with 32.1% compression. In language modeling, it achieved 2.8% and 2.9% performance improvement with 16.9% model compression. Further efficiency analysis showed GHT-PS-LITE achieving 90.36% fewer parameters, 62.05% faster inference speed, and 80.90% fewer FLOPs compared to Lite Conv while maintaining the same BLEU score.

The speaker concludes by highlighting future work on "Task-specific Automatic Pruning," based on the Lottery Ticket Hypothesis. They believe that all-in-one LLMs are redundant in real scenarios, as only a few tasks are needed for specific applications, suggesting that task-specific pruning can significantly reduce model size without sacrificing performance, akin to uninstalling unused apps on a smartphone.</sample>
    <sample id="115">The approach uses 1-second speech segments.</sample>
    <sample id="116">The entity-specific knowledge needed is that Servin is a judge and Kea is a baker.</sample>
    <sample id="117">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The speaker introduces their ACL 2023 paper on "Improving Pretraining Techniques for Code-Switched NLP." Code-switching refers to sentences that mix words from different languages, such as "Laptop mere bag me rakha hai" (English and Hindi). Building computational models for code-switching is important, as existing multilingual models like mBERT and XLM-R perform poorly on these tasks.

Their main contributions are:
1. Novel masked language modeling (MLM) pretraining objectives to incorporate code-switching information.
2. Architectural changes and auxiliary loss criteria for more effective code-switched pretraining.

They propose SwitchMLM, where only "switch-points" (words where language transitions occur) are maskable. This requires language identification (LID) tags, which are not always available. As a proxy, they propose FrequencyMLM, where LID tags are assigned based on negative log-likelihood of words in monolingual corpora.

Architectural modifications include residual connections, which leverage the finding that intermediate BERT layers encode more switch-point information. An auxiliary LID-based loss is also imposed on these intermediate layers to further encourage language information encoding.

Results show that combining SwitchMLM/FrequencyMLM with residual connections and auxiliary loss performs best on Question Answering (QA) and Sentiment Analysis (SA) tasks. Probing experiments using linear and conditional probing confirm that their methods increase the amount of switch-point information encoded in the intermediate and final layers, validating their claims.</sample>
    <sample id="119">The paper focuses on BERT, RoBERTa, and GPT-2 in its extended experiments.</sample>
    <sample id="120">The provided text does not specify whether the model uses attention scores from a specific layer or combines scores from several layers.</sample>
    <sample id="121">The examples of direct inference are: "easy on me" and "the first one".</sample>
    <sample id="122">The affiliations of the authors are Fudan University and Brain Technologies Inc.</sample>
    <sample id="123">This presentation introduces MultiInstruct, a novel benchmark dataset designed for improving multi-modal zero-shot learning through instruction tuning. While previous works have primarily focused on language-only tasks, MultiInstruct addresses the significant imbalance in available instructional datasets for multi-modal tasks. The dataset comprises 62 diverse multi-modal tasks, categorized into 10 broad groups, and each task is accompanied by five expert-written instructions.

The researchers employ OFA (One For All), a unified multi-modal pre-trained model, as their base model. OFA utilizes a unified vocabulary for language, image tokens, and bounding box coordinates, allowing for a consistent sequence-to-sequence format across all tasks.

Experimental results demonstrate that instruction tuning on MultiInstruct significantly enhances OFA's performance on unseen multi-modal tasks. Furthermore, transfer learning from existing language-only instructional datasets (Natural Instructions) proves beneficial, indicating a positive synergistic effect. The study also introduces a new evaluation metric called "sensitivity" to measure the model's ability to produce consistent outputs despite minor variations in instruction wording. Findings indicate that instruction tuning, especially with diverse instructions, significantly reduces model sensitivity. The researchers are actively expanding MultiInstruct with an additional 150 vision-language tasks and plan to release them soon.</sample>
    <sample id="124">The presenter introduces a study titled "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." He begins by segmenting temporal reasoning into three levels: Time-Time Relation, Time-Event Relation, and Event-Event Relation. Preliminary experiments revealed that Large Language Models (LLMs) are biased towards contemporary years (2000-2020) and that while ChatGPT performs well on year prediction, its performance drops when predicting months.

To address these limitations, the researchers propose the TempReason dataset, which covers all three temporal reasoning levels and a broad range of time periods. For Level 1 (L1) questions, the difficulty was increased from year to month prediction. Levels 2 (L2) and 3 (L3) were constructed using Wikidata knowledge base and Wikipedia articles. The evaluation was conducted across three settings: Close Book QA (CBQA) with no context, Open Book QA (OBQA) with Wikipedia articles as context, and Reasoning QA (ReasonQA) with structured temporal knowledge.

To enhance LLMs' temporal reasoning, a training strategy with two novel components was developed: temporal span extraction pretraining and time-sensitive reinforcement learning. The former is an intermediate pretraining step to reconstruct temporal and entity spans from raw text. The latter rewards correct predictions and penalizes temporally wrong ones. The final model is named TempT5. Experimental results show that ChatGPT's performance significantly drops on L1 month prediction and is not promising for L2 and L3 reasoning. TempT5, however, considerably improves performance in OBQA and ReasonQA settings. Further analysis of L2 reasoning by time period revealed performance variations across different periods, indicating a potential training data imbalance that future work could address.</sample>
    <sample id="125">There are seven authors involved in the paper.</sample>
    <sample id="126">Yes, it was considered a baseline.</sample>
    <sample id="127">The speaker introduces a paper titled "Large Language Models Are Reasoning Teachers." The problem addressed is that Chain-of-Thought (CoT) reasoning, while effective for complex tasks, is limited to huge models (over 100B parameters) requiring substantial memory and computation. The proposed solution involves using these large models as "reasoning teachers" to transfer their reasoning abilities to much smaller models (70M-6.7B parameters).

The method, called Fine-tune-CoT, begins by using a large teacher model (GPT-3 175B) to generate step-by-step reasoning solutions for questions from a dataset. If the generated solution leads to the correct final answer, it is formatted into a training sample for the smaller student model. A novel technique, "Diverse Reasoning," is introduced, where multiple distinct reasoning solutions are generated from the teacher model for each question using stochastic temperature sampling. This diversity in reasoning steps is shown to further boost the student model's performance.

The results indicate that Fine-tune-CoT significantly improves reasoning capabilities in small models, outperforming standard prompting and even vanilla fine-tuning on many tasks. Notably, Diverse Reasoning substantially enhances performance, particularly in arithmetic reasoning tasks, raising accuracy from 33% to 55% for MultiArith using a 6.7B parameter model. The paper also explores the scalability of this approach, showing that performance can be further improved by using more diverse reasoning samples, larger datasets, better teacher models, or larger student models. The work highlights trade-offs between development costs (diverse reasoning, dataset size, teacher model choice) and inference costs/quality (student model choice), emphasizing the method's accessibility and effectiveness as a highly scalable approach for distilling reasoning abilities into smaller language models.</sample>
    <sample id="128">Natural Language Understanding (NLU) models often draw on diverse knowledge sources, including pre-trained parameters and real-time inference inputs. While pre-trained models can handle general knowledge (e.g., what presidents do), they struggle with instance-specific or rapidly changing information (e.g., the current president's identity). 

To address this, we introduce KITMUS, a diagnostic test suite designed to evaluate an NLU model's ability to integrate knowledge from multiple sources. Our test suite uses a coreference resolution task, where pronoun resolution requires both entity-specific knowledge (often from inference-time context) and general background knowledge (potentially from pre-training). We define three settings: 
1. **Background-Pretrain:** Background knowledge is in parameters, entity-specific knowledge is in context.
2. **Background-Both:** Both knowledge types are available in both sources.
3. **Background-Inference:** Both knowledge types are only available in context (using fictional background knowledge to prevent pre-training access).

Our experiments with human participants and coreference resolution models (BERT4Coref and C2F) reveal several key insights. Models perform poorly without task-specific training on KITMUS, suggesting they exploit superficial cues rather than integrating deep knowledge. With task-specific training, some models successfully integrate knowledge across sources. However, they struggle significantly when background knowledge is only available at inference time, indicating a limitation in dynamically acquiring and integrating new general knowledge. This highlights the need for models that can effectively blend both static, pre-trained knowledge and dynamic, context-specific information for robust NLU. The dataset and code are publicly available on GitHub.</sample>
    <sample id="129">The authors cited "a woman warrior" as an example of a marked group, contrasting it with the unmarked "a warrior".</sample>
    <sample id="130">Transformer models generalize better, implying that non-transformer models may not generalize as well.</sample>
    <sample id="131">The names of the testing datasets are not mentioned in the provided text.</sample>
    <sample id="132">The paper involves six authors.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="134">Hi, I am Yanis Labrak and I will present you our works on DrBERT, a robust pre-trained model in French for biomedical and clinical domain. In this presentation, we first talk about language modeling in healthcare. Then, we will present the main contribution of our article. We introduce the first biomedical model in French, named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a dataset of medical crawled data from the web. We also introduce a comparison of models with multiple pre-training settings and data sources. Then, we present our result on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access to the models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing task and offer huge performing gain compared to historical static and contextualized methods such as Word2Vec, FastText or ELMo. Since then, this model has been adapted to many other languages, like in French with CamemBERT and other domain like biomedical with PubmedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized model for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open-source model for biomedical until now. We so we ask ourselves question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this questions, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterward, we ask ourselves, how much data do we need to train a specialized model on French data? Is it 4 gigabyte, 8 gigabyte or more? To answer this question, we first train and compare four from scratch model. A first version of DrBERT with 7 gigabyte of NACHOS. A second version of 4 gigabytes of NACHOS. A first version of ChuBERT which is a clinical model, with 4 gigabytes of sentences taken from clinical notes. And a final version of ChuBERT with a mix of 4 gigabyte of NACHOS and 4 gigabyte of clinical notes. In addition to this comparison, we introduce three model trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on 4 gigabytes of NACHOS. Another also based on CamemBERT, but trained this time on the 4 gigabytes of clinical notes. And finally, one based on an English biomedical model PubmedBERT and trained on 4 gigabytes of NACHOS. In total, we have seven models. To evaluate our seven models, we gather multiple public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging and question answering. These models are compared to six baseline model which are CamemBERT OSCAR 138 gigabyte, CamemBERT OSCAR 4 gigabyte, CamemBERT CCnet 4 gigabyte, PubmedBERT, BioBERT and ClinicalBERT. The evaluation of a highlight that model perform best on the task with data of the same nature as those on which the model has been trained. However, we have we can obtain that data from we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translate into better performance. In overall, from scratch pre-training seem to obtain higher performance on most of the task. However, our experiment on continual pre-training using the weight and tokenizer of PubmedBERT trained on the 4 gigabyte subset of NACHOS show comparable result to those from obtained with DrBERT 4 gigabyte from scratch, which is not the case for the model based on CamemBERT weight and tokenizer which suffer from stability issues. Finally, as a conclusion, our proposed system offer better performance on nine of the 11 downstream task and surpass globally the result of the generic model here CamemBERT. We also observing that specialized data is better. More specialized data is better, but it's doesn't scale well. All the pre-trained model obtained from NACHOS are freely available and on Hugging Face and all the training script are on our GitHub repository. So, thank you for for this presentation and we are looking forward to exchange at the poster session in Toronto.</sample>
    <sample id="135">The speakers introduce ABC-Eval, a novel method for evaluating chat-oriented dialogue systems. This approach aims to reduce the subjectivity of human evaluation by annotating specific model behaviors rather than relying on overall Likert scale ratings or pairwise comparisons. ABC-Eval identifies four main categories of behaviors: Coherence (ignoring partners, irrelevant information), Consistency (self-contradiction, partner contradiction), Knowledge (incorrect facts, common sense violations), and Emotional Understanding (empathetic or un-empathetic responses).

In their experiments, they evaluated four state-of-the-art chat models across 100 human-bot conversations using ABC-Eval and three existing methods (turn Likert, dialogue Likert, and comparative). Their analysis showed that ABC-Eval's behavior labels generally yield more reliable inter-annotator agreement compared to the labels collected by existing methods. Furthermore, ABC-Eval's labels proved more predictive of overall conversation quality, as demonstrated by a simple linear regression analysis. For instance, measuring self and partner contradictions using ABC-Eval explained 5% and 10% of conversation quality, respectively, while Likert consistency scores explained 4% or less.

The combination of all ABC-Eval metrics explained over 25% of conversation quality, with individual metrics contributing uniquely. In contrast, turn-level Likert metrics explained significantly less quality, with fewer metrics providing unique information. The results indicate that current chat models still face challenges in areas like common sense violations (around 20% of responses), irrelevance (around 15% of responses), and self- or partner contradictions (around 10% of responses). The speakers emphasize that ABC-Eval offers a higher resolution and more precise method for evaluating conversational AI, crucial for advancing the field.</sample>
    <sample id="136">The speaker, Jasivan Alex Sivakumar, presents the work he conducted with his supervisor, Nafise Sadat Moosavi, at the University of Sheffield. The presentation, titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning," explores the challenges of numerical reasoning in language models, particularly with smaller, more accessible models that tend to perform poorly. Current benchmarks, often relying on single accuracy scores, fail to provide informative insights into a model's mathematical strengths and weaknesses.

To address this, the researchers introduce FERMAT, a flexible evaluation set for arithmetic types. FERMAT evaluates models on number understanding, mathematical operations, and training dependency. For number understanding, the dataset modifies numerical representations and values to mimic real-world scenarios and test the models' breadth. For mathematical operations, it assesses performance on both simple and compound arithmetic.

Zero-shot evaluations with FERMAT reveal that most models perform poorly across various numerical aspects. However, fine-tuning these models with a diverse set of templates, including different numerical formats and mathematical operations, significantly improves their performance across the board. The study also investigates training dependency, showing that even when an exact expression is seen during training, accuracy remains below 50%, suggesting that models don't merely memorize but still struggle with the underlying linguistic notions. The findings highlight the unrepresentativeness of existing benchmarks and the importance of language and mathematical diversity in training data for improving numerical reasoning in language models.</sample>
    <sample id="137">The speaker introduces "Tell2Design", a new dataset for language-guided floor plan generation. Recent text-conditional generative AI models have shown impressive results in generating high-fidelity images, particularly artwork. However, floor plan generation requires meeting specific requirements given in natural language, which is more constrained than generating artwork. 

"Tell2Design" aims to allow non-experts to participate in the design process by using text instructions to generate floor plans. The dataset includes 5,051 human-annotated and 75,737 artificially generated language instructions. Each instruction describes the semantics, geometry, and topology of the rooms in a floor plan. The task is to generate a structured interior layout that aligns with these instructions.

The proposed approach uses a Transformer-based encoder-decoder large language model (LLM), initialized with T5, to convert natural language instructions into a structured sequence of room bounding boxes. This approach outperforms existing text-conditional image generation baselines, with a micro IOU of 54.34 and macro IOU of 53.30.

The speaker highlights the challenges of this task: design generation under strict constraints, handling fuzzy and entangled information from document-level unstructured text, and dealing with noisy human instructions. Case studies demonstrate that while text-conditional image generation models can produce realistic floor plan images, they struggle to align with the detailed requirements in human instructions. Combining artificial and human instructions improves performance, suggesting that both types of data are mutually beneficial for training. The speaker concludes by emphasizing that "Tell2Design" and the proposed method provide a foundation for future research in language-guided design generation.</sample>
    <sample id="138">The authors claim that the ability of NLU models to integrate and use knowledge from multiple sources (both pre-train time and inference time) is an understudied area.</sample>
    <sample id="139">Ying Shen and Zhiyang Xu.</sample>
    <sample id="140">Yes, to ensure the quality of validation and test sets, humans annotated and revised incorrect samples in Coscript.</sample>
    <sample id="141">Existing resources only support limited types of context-dependent translations and limited sets of languages, as they rely on domain knowledge and human curation.</sample>
    <sample id="143">The approach is compared to wait-k and local agreement (LA) strategies, as well as CAAT, a state-of-the-art architecture specifically tailored for SimulST.</sample>
    <sample id="144">The authors are affiliated with LIA, Avignon Université, LS2N, Nantes Université, Clinique des données, CHU de Nantes, and Zenidoc.</sample>
    <sample id="145">The speaker is Jenny.</sample>
    <sample id="146">In this presentation, the speaker, Yicheng Zou, introduces the topic of dialogue summarization, a subtask of text summarization that aims to condense dialogues from various scenarios into concise summaries. He highlights the prevalent issue of omission in dialogue summaries, emphasizing its impact on the quality of generated summaries. Studies indicate that even advanced models exhibit high omission rates, with approximately 70% of generated summaries suffering from this problem. Furthermore, omitted information is randomly distributed across dialogue positions, irrespective of length or domain, indicating the unstructured nature of dialogues and the difficulty for current models to identify key information.

To address this, Zou introduces a new task called "Omission Detection" and a dataset called OLDS (Omission Detection in Dialogue Summarization). This dataset is built upon five existing benchmarks across various domains and comprises 10 candidate summaries per dialogue, generated by different abstractive models and decoding strategies. Omission labels are created through automatic detection and human assessment. Baseline evaluations, including Pair-wise Classification, Sequence Labeling, and Pointer Network, show an F1 score of around 50%, indicating the challenging nature of the task and the need for more advanced detection models. Finally, the presentation demonstrates that omission detection is a valuable task, as providing omission information significantly boosts the performance of summary refinement using a post-editing method, suggesting a promising direction for improving dialogue summarization quality.</sample>
    <sample id="147">Three authors are involved in the paper: Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">This paper introduces MeetingQA, a novel extractive question-answering dataset derived from multi-party meeting transcripts. Recognizing that existing NLP research on meetings primarily focuses on summarization and action item extraction, MeetingQA addresses the significant, yet underutilized, QA component of meeting discussions. It comprises 7,735 questions extracted from 166 manually transcribed meetings from the AMI corpus. A unique aspect of MeetingQA is its emphasis on open-ended, discussion-seeking questions posed by meeting participants, often leading to multi-speaker and multi-span answers. Approximately 30% of questions are unanswerable, while 40% feature multi-span answers and 48% involve multiple speakers. A substantial 70% of multi-speaker answers include some form of disagreement, enriching the dataset's complexity.

Experimental results reveal a significant performance gap between current QA models and human performance. Fine-tuned models, including short-context models like RoBERTa and long-context models like Longformer, still lag by over 25 F1 points. In a zero-shot setting, this gap widens to nearly 50 F1 points. Error analysis indicates that models particularly struggle with identifying rhetorical questions and discerning which speakers contribute to an answer, especially in zero-shot scenarios. The dataset also highlights a challenge for single-span models in filtering out irrelevant sentences. Silver data augmentation proves effective in improving zero-shot performance, suggesting potential avenues for future research. MeetingQA aims to push the boundaries of extractive QA in complex, multi-party conversational settings.</sample>
    <sample id="152">The presenter, Frederick Riemenschneider, introduces a paper titled "Exploring Large Language Models for Classical Philology." He begins by discussing the current landscape of language models in classical studies, specifically mentioning Latin BERT (Bammann and Burns 2020), Ancient Greek BERT (Singh, Rutten, and Lefever 2021), and another Ancient Greek BERT (Yamshchikov et al. 2022). He notes that these models are all encoder-only and monolingual, and their performance is often not robustly evaluated.

To address these limitations, the research team aims to create new language models specifically for classical philology. Their goals are: to make existing models comparable, push the state of the art, explore different model architectures, and introduce multilingual models. They have pre-trained two monolingual models for Ancient Greek: GrεBERTa (RoBERTa-based) and GrεTa (T5 encoder-decoder based). Additionally, they developed multilingual equivalents, PhilBERTa and PhilTa, pre-trained on Ancient Greek, Latin, and English data.

For pre-training data, they used Open Greek &amp; Latin, Greek Medieval Texts, and Patrologia Graeca. They also leveraged the Internet Archive, developing a method to extract usable Greek text from poorly OCR'd scans, significantly expanding their dataset. For evaluation, they used the Universal Dependencies treebanks for Greek and the EvaLatin 2022 dataset for Latin, focusing on PoS tagging, dependency parsing, and lemmatization.

Their models significantly outperform previous models in dependency parsing and lemmatization, with an impressive 5 percentage point increase for Ancient Greek lemmatization. They also found that the T5 encoder behaves fundamentally differently from native encoder-only models. Interestingly, there was no significant difference in semantic or world knowledge performance between their monolingual and multilingual models, suggesting that while multilingual models are valuable for practical use, they do not inherently offer a performance advantage in these specific areas over well-trained monolingual models when data is abundant.</sample>
    <sample id="153">In this work, the speaker addresses the problem of prompt ambiguities in text-to-image (T2I) generative models. They propose two frameworks, Question Answering (QA-TIED) and Visual Setups (VS-TIED), to mitigate these ambiguities and generate images faithful to user intent.

The research begins by curating a comprehensive benchmark dataset called Text-to-image Ambiguity Benchmark (TAB), which is a modified version of the LAVA corpus and encompasses various types of ambiguities, including syntax-pp, syntax-vp, syntax-conjunction, discourse anaphora, discourse ellipsis, and fairness.

The QA-TIED framework utilizes in-context learning to enable a language model (LM) to generate clarifying questions based on an ambiguous prompt. Users then answer these questions, and their responses are concatenated with the original prompt to create a disambiguated prompt. This process allows for precise disambiguation tailored to the user's specific intent.

Alternatively, the VS-TIED framework also uses in-context learning for the LM to generate different possible visual setups for an ambiguous prompt. Users then choose the setup that best aligns with their intention, and this choice is used to disambiguate the prompt.

To evaluate the faithfulness of the generated images, an automatic evaluation framework is proposed. This framework inputs the generated images and the user's disambiguated intent (in question format) into a Visual Question Answering (VQA) model. The VQA model determines if the image accurately reflects the user's intention, providing a faithful generation metric.

Key findings of the research include the disparity in resolving ambiguities across different types, the overall positive effect of disambiguation on faithful generation, and a reasonable agreement between automatic and human evaluations. This suggests that the automatic evaluation framework can be reliably used for T2I model evaluation.</sample>
    <sample id="154">The authors are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Mohammad Javad Hosseini.</sample>
    <sample id="156">Hello everyone. My name is David Pilar and I will be giving a short overview of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PaLM is a 540 billion parameter large language model presented last year, in 2022. It's trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state of the art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compare to state-of-the-art systems, so the best performing systems of the WMT evaluation. We use state-of-the-art neural MT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the of LLMs for translation, as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences, 516 out of 1000, the difference observed is of more than one BLEURT points. And this can go in extreme cases up to 40 BLEURT points. So, it's important to select a good prompting strategy. In our experiments, we settled for a five-shot prompting strategy, where we just mark each each sentence that we provide to the system with the language it's in. So, in this example, here where we perform translation from German into English, the German sentences, the source sentences are marked with German: colon, and the English translations with English: colon. We saw that the actual form of the prompting doesn't have a big influence in in the case of several shot prompting. It's crucial for zero and one-shot prompting, but when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So, it's important to select the examples from high quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the training data that is more noisy, and the results show a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But PaLM comes pretty close to a commercial system. In our in our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we perform using the MQM framework is that the fluency of PaLM is comparable to state of the of the art systems. But the main difference comes from the accuracy. So, in particular, the most common error are omission errors. So, it seems that PaLM chooses the to produce a better sounding translation, sometimes by dropping parts of the source sentence that are omitted in the in the translation. However, the style awkward category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provide really fluent output, but still with some problems of of accuracy. And that's it for this really short overview. For more details, please come by to the full presentation of the paper. Thank you very much.</sample>
    <sample id="157">This presentation introduces a novel approach to dialogue summarization, termed "Dialogue Summarization with Static-Dynamic Structure Fusion Graph." The proposed model, SDDS, aims to distill salient information from complex, multi-participant dialogues into concise summaries. Unlike existing methods that rely on pre-computed static graph structures, often prone to error propagation due to external linguistic tools, SDDS integrates both static and dynamically learned dialogue structures.

The SDDS model comprises four main components: an Utterance Encoder, Static Graph Construction, a Static-Dynamic Graph Module, and a Summary Generator. The Utterance Encoder transforms dialogue utterances into vector representations. The Static Graph Construction builds dependency-based dialogue structures using discourse parsing, models keyword co-occurrence for topic correlation, and captures speaker interaction frequencies. This involves using a sliding window to count speaker co-occurrences and derive speaker interaction frequency matrices. The Static-Dynamic Graph Module then fuses these static graphs through a 1x1 convolutional layer and dynamically captures semantic relationships between utterances using a multi-head attention model. Finally, the Summary Generator, a pre-trained language model, incorporates the fused graph representation into the summarization process using a dual cross-attention mechanism with a graph attention layer. This framework aims to overcome the limitations of relying solely on fixed linguistic tools, providing a more robust and adaptive approach to dialogue summarization.</sample>
    <sample id="158">This paper introduces "Dual Cache," a novel approach to coreference resolution in long documents, addressing the limitations of conventional and single-cache methods. Traditional coreference resolution faces quadratic complexity in computation and memory for long documents, due to enumerating all possible mention pairs. Cache-based models offer a linear complexity by storing entity representations in a fixed-size cache, typically using an LRU (Least Recently Used) eviction policy.

However, in long documents, frequent topic switching scatters mentions of a single entity across the text, leading to high cache miss ratios with LRU. High-frequency entities, despite being mentioned globally, are often evicted due to long periods between their mentions.

Dual Cache proposes a two-level caching system: an L-cache for local entities with an LRU policy and a G-cache for global entities with an LFU (Least Frequently Used) policy. When a new mention is encountered, the model first determines if it represents a new entity or belongs to an existing one in the cache. If it's a new or updated entity, its frequency is evaluated. If its frequency surpasses a threshold, it's transferred to the G-cache; otherwise, it's added to the L-cache. Both caches initiate an eviction process when full, based on their respective policies.

Experiments on public benchmarks (LitBank, OntoNotes, WikiCoref) demonstrate that Dual Cache outperforms single-cache methods, even those with unbounded memory, especially on book-level documents where the performance gap is significant. Dual Cache largely reduces cache misses and exhibits a higher performance/cost ratio compared to single-cache methods, making it a more cost-effective solution for long document coreference resolution.</sample>
    <sample id="160">The first step of the method maps input tokens to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">Coscript contains 55,000 scripts.</sample>
    <sample id="162">Hello everyone. I'm Akshatha and today my co-author Martin and I are presenting our work, the Kitmus test, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila and Microsoft Research. NLU models draw on multiple knowledge sources. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired via pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-train time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, John saw the newly elected president on TV. Pre-trained parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance specific entity John is or who the new president is, because the president might have changed since pre-training. Therefore, successful models for knowledge intensive NLU tasks require the ability to integrate and use both pre-train time and inference time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and established coreference resolution models. Here is an example from our dataset. Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]. The resolution of a given pronoun requires two types of information. First, entity specific knowledge, such as Servin is a judge. And second, background knowledge, such as judges decide cases in courts of law. Generally, background knowledge is learned during the pre-training of large language models, while entity specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of Kitmus. First, we have the typical setting: background-pretrain. Where background knowledge is assumed to be available at pre-train time. Second, there's a background-both setting. Where background knowledge is available both at pre-train time and inference time. Lastly, the background-inference setting. Where both knowledge types are available only at inference time. This last setting is especially interesting. Since it simulates a case where the background knowledge necessary to solve the task is not part of the pre-train data of models. For example, because new occupations have developed since the time of pre-training. Here's an example of how we control the availability of facts in the two sources. In the background-pretrain setting, we assume that the background knowledge, "politicians seek elected seats in government" is contained in the pre-trained parameters. In the inference-time context, we provide the entity specific knowledge, "Chichester is a politician". In the background-both setting, we additionally provide not only entity specific, but also background knowledge about politicians in the inference-time context. And in the background-inference setting, we provide the fictional occupation "mirituer" instead of politician, because mirituers are unlikely to be contained in the pre-trained parameters. We evaluated the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best performing models on the most difficult variant of the background-pretrain setting. Without task-specific training on Kitmus, both models do not perform well. When trained on Kitmus, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues which are not useful when testing on Kitmus where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper. Many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you are interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="163">The best alignment method for DEplain is MASAlign.</sample>
    <sample id="164">Weak supervision alleviates the annotation bottleneck, as manually labeling data is time-consuming and expensive.</sample>
    <sample id="165">The speaker, Wenting Zhao, presents a paper on Abductive Commonsense Reasoning, which focuses on exploiting mutually exclusive explanations. They begin by defining abductive reasoning with a concrete example: given a context (Emily was stuck in traffic) and an outcome (Emily made it to her flight), the goal is to find a plausible explanation (her flight was delayed or her flight left on time). The speaker emphasizes that these explanations are mutually exclusive.

The paper focuses on unsupervised abductive reasoning, addressing the challenge that annotating plausible explanations can be noisy and subjective, with crowd workers disagreeing significantly. The proposed solution is LiPoR (Likelihood learning with Posterior Regularization), an unsupervised learning method. LiPoR treats explanations as a latent variable and maximizes the log-likelihood of the outcome given the context, by marginalizing out all possible explanations. To ensure the model favors a subset of explanations, an additional regularizer, Omega, is introduced. Omega encourages the probability mass of explanations to collapse to a subset, considering the mutual exclusivity of explanations. If one explanation is true, others are automatically ruled out.

Experimental results on the aNLI dataset show that LiPoR outperforms previous unsupervised approaches and even a strong zero-shot GPT-3 baseline by over four absolute points in accuracy. The paper concludes by providing a link to the full paper.</sample>
    <sample id="166">The presented research introduces a "Neural Divide-and-Conquer Reasoning Framework" for image retrieval from linguistically complex text. This task is challenging due to the high similarity of images and the length of their descriptions. Existing visual language models (VLMs) perform well on simple image-sentence retrieval but show significant performance drops when confronted with complex texts.

Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, the proposed framework aims to address this by integrating two thinking systems:
1. **System 1: Visual-Linguistic Interactor:** Based on a VLM, this system performs analogical reasoning by conducting visual-proposition information interaction. It generates matching scores for propositions-images and the reasoning states of propositions-on-images.
2. **System 2: Neural-Symbolic Reasoner:** This module is responsible for integrating the reasoning states and results of simple propositions to obtain the final solution for complex propositions. It employs negation and conjunction operations to gain positive and negative reasoning states.

The framework also includes a "Proposition Generator" that decomposes complex proposition text into simple proposition sentences. The combination of System 1 and System 2 integrates both analogical and logical reasoning, leveraging their respective advantages. Experimental results show that the proposed NDCR (Neural Divide-and-Conquer Reasoning) method outperforms existing baselines and demonstrates the effectiveness of each module, highlighting the benefits of this divide-and-conquer approach for complex reasoning tasks. The case analysis further illustrates the interpretability of the intermediate inference results.</sample>
    <sample id="167">The documents in DEplain-web were aligned with both manual and automatic alignment methods. 756 documents were aligned, with some aligned manually and others with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it using the CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">The speaker, David Vilar Torres, introduces a paper titled "Prompting PaLM for Translation: Assessing Strategies and Performance," a collaborative effort with his Google Translate colleagues. He highlights that PaLM, a 540 billion parameter language model, was trained on 780 billion tokens and achieved state-of-the-art results in numerous NLP tasks at the time of its publication in 2022.

The study systematically evaluates the translation capabilities of large language models (LLMs) using best practices from the machine translation (MT) community, including the latest test sets, comparisons to state-of-the-art WMT submissions, advanced MT metrics, and expert-based human evaluation. The research reveals that prompt quality significantly impacts translation quality, with differences of over 1 BLEURT point in the majority of sentences and up to 40 BLEURT points in extreme cases.

Key experimental findings indicate that example quality in prompts is more crucial than similarity to the source sentence, suggesting the importance of selecting high-quality translations for prompts. While specialized state-of-the-art systems generally outperform PaLM, PaLM's performance is comparable to commercial systems like Google Translate. Insights from human evaluation (MQM) show that PaLM's fluency is comparable to state-of-the-art systems, but its accuracy scores are generally lower, primarily due to "accuracy/omission" errors. Conversely, PaLM scores lower in "style/awkwardness," indicating that it produces more fluent, albeit sometimes less accurate, output. The speaker concludes by inviting viewers to the full presentation for more details.</sample>
    <sample id="171">Existing works are classified into parameter-based, lexical, backdoor-based, and adversarial-based watermarks.</sample>
    <sample id="172">No, multilingual LLMs like Codex and BLOOM are currently inadequate for cross-lingual semantic parsing (CLSP) tasks.</sample>
    <sample id="173">Hello everyone. My name is Shuheng. Today, I'm going to present our paper: Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023? Let's get started. Our paper investigated the problem of generalization, using the named entity recognition task, or the NER task. We observed that models have been using CoNLL-2003 to develop NER for almost 20 years. And this naturally raises several problems. Firstly, can these models generalize to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ dataset. This is a dataset that we collected from Reuters news from 2020 and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-2003 test set and the CoNLL++ test set. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So, what is needed for a good generalization? Throughout our experiments, we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments, we found that the Transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here, we also found that more fine-tuning examples actually also leads to better generalization. To our next question. What causes the performance drop of some models? We had two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. And this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For adaptive overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made on CoNLL-2003 translates to more than one unit improvement on CoNLL++. Which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data. And we found that the performance degrades with larger temporal gap. And this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these goes hand in hand. We can't just have one ingredient, but throughout the others. At the same time, we also found that the performance drop here is caused by temporal drift, and kind of surprisingly, it is not caused by adaptive overfitting, even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper, do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding YES! We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our dataset, and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="174">The speaker introduces ArgAnalysis35K, a large-scale dataset for Argument Quality Analysis, co-authored by her. She explains that argument quality analysis involves rating arguments on a scale of 0 to 1, with higher scores for more coherent and persuasive arguments.

The speaker highlights the problems with current datasets: lack of argument quality (often from crowdsourcing), limited diversity of motions (typically 30-40 specific topics), lack of depth in explanations, and motion-specific scores.

ArgAnalysis35K addresses these issues by being the largest dataset in argument quality detection with 35K argument-analysis pairs. It sources high-quality arguments directly from winning debates and debaters (85% from experts), enhancing argument quality. The dataset features diverse arguments by considering 24 themes (politics, environment, authoritarian regimes, etc.) instead of specific motions, capturing more motions within each theme.

A unique feature is the "analysis" element, which provides logical links to prove why an argument is true, unlike simple claims or premises, offering more depth. The dataset also introduces an "Instance-Based Annotator Reliability" model to mitigate human biases in annotations by considering reliability at an individual argument level rather than dismissing entire annotators. Finally, a "Relevance Model" assigns a score (0-1) to each argument-analysis pair for each theme, quantifying the relevance of an argument to various topics, moving beyond motion-specific scores. The speaker encourages viewers to read their paper for more details.</sample>
    <sample id="175">It uses a GPU-friendly continuous relaxation to approximate finding the highest-scoring permutation, which allows for backpropagation to learn the linguistically more plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by its accuracy on hate speech detection targeting different identity groups (e.g., Black, Muslim, LGBTQ+, Jews, Asian, LatinX, Women, Christian, Men, White) and misinformation from different news sources (e.g., HP (L), NYT (L), CNN (L), NPR (L), Guard (L), Fox (R), WaEx (R), BBart (R), WAT (R), NR (R)).</sample>
    <sample id="177">The speaker's name is Yanis Labrak.</sample>
    <sample id="178">The speaker is Koustuv Sinha.</sample>
    <sample id="179">This paper introduces SymbolicToM, a plug-and-play method designed to enhance the Theory of Mind (ToM) reasoning capabilities of large language models (LLMs). ToM, which traditionally involves assessing understanding in reading comprehension tasks with multiple characters, is evaluated through false-belief questions. These are scenarios where reality and a character’s belief diverge. LLMs typically perform poorly on such tasks.

SymbolicToM employs an inference-time algorithm that avoids overfitting risk and utilizes explicit graphical symbolic representations for greater interpretability. This approach involves several graphical representations for mental states, as a single graph is insufficient. These graphs, referred to as belief graphs ($B_{p_1, p_2, \ldots, p_m}$), capture what one character thinks another character thinks about the world state, up to a predefined depth (m).

When answering a question, SymbolicToM first detects entities, retrieves the relevant belief graph, and then performs recursion to transform the question into a factual one about the graph. The sentences from the graph and the factual question are then fed to an LLM to generate the final answer.

The method was tested on various LLMs, including Macaw-3B, GPT-3 variants, Flan-T5, LLaMA, GPT-3.5, and GPT-4, and compared against supervised baselines like Textual Time Travel and fine-tuned GPT-3 Curie. Experiments showed dramatic improvements in out-of-the-box LLM performance for in-domain ToM questions, with significant accuracy gains across models (e.g., +65 points for GPT-3 Davinci, +67 for Macaw-3B, and +51 for Flan-T5-XXL). SymbolicToM also outperformed supervised methods on out-of-domain story understanding and remained beneficial on a new linguistic diversity dataset (ParaphrasedToMi), demonstrating strong generalization capabilities.</sample>
    <sample id="180">The speaker's name is Myra.</sample>
    <sample id="181">A presentation discusses the problem of "constrained language planning", which involves generating step-by-step instructions for tasks with specific constraints, like "make a chocolate cake". The presenter notes that while large language models (LLMs) can effectively decompose abstract goals into steps, they struggle with constrained planning. The paper introduces a new dataset, "Coscript", created by extending abstract goals with multi-faceted constraints using InstructGPT. 

The evaluation of LLMs on this task shows unsatisfactory results. A detailed error analysis reveals that while the semantic completeness of generated scripts is acceptable, their faithfulness to the constraints is not guaranteed. The planning performance of InstructGPT varies considerably across different categories of goals.

To address this, the presented method first generates specific goals from an abstract goal using InstructGPT via in-context learning. Then, it over-generates candidate scripts for these specific goals and employs a filter model to select faithful scripts based on similarity scores and keyword presence. This method significantly improves planning quality.

The paper's motivation is to enable constrained language planning for smaller models, as LLMs are costly. By following the idea of symbolic knowledge distillation, 55,000 scripts with constraints are generated for Coscript. Crowd-sourced workers annotate and revise validation and test sets to ensure quality. Constraint analysis reveals high heterogeneity and pluralism in the generated goals. Ultimately, smaller LLMs fine-tuned on Coscript can generate higher quality scripts than most LLMs, indicating their potential when trained on suitable datasets. The proposed method for improving LLMs is a post-hoc re-ranking approach, and Coscript dataset is a valuable resource for advancing research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="182">Tropicalism is connected to the words "vibrant" and "curvaceous" that describe Latina women, reflecting a long-standing trope in the context of this paper.</sample>
    <sample id="183">The human-written portrayals of target groups were created in a psychology study where human subjects were given the same prompts.</sample>
    <sample id="184">Conditional Cross-Mutual Information (CXMI) was used to measure context usage.</sample>
    <sample id="185">DrBERT is a biomedical model trained on Nachos (open-source medical data from the web), while ChuBERT is a clinical model trained on NBDW (private medical records from a university hospital).</sample>
    <sample id="186">Hi, I'm Myra, and today I'll be talking about our paper Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models or LLMs. However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate, and they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general, broad associations like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions in prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual, using a prompt like, "Imagine you are an Asian woman. Describe yourself." And we can immediately see that this is very generalizable to any demographic, because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming, the Middle-Eastern woman is referred to using words like exotic, um, and like referring to a mesmerizing region, and both of the woman of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study, um, where they gave these prompts to human subjects, finding that by giving it to human subjects they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human-written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. Um, the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon. So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default, and any group that differs from that default, um, is linguistically marked. So for instance, the word man, or sorry, the word warrior is usually associated with men. Um, so when people are describing a warrior who is a woman, they'll usually actually specify woman warrior, and mark the term with woman. And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are. And then we compare the personas using the fighting words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do fighting words and compare the log-odds ratios against both white personas and man personas, because those are the two corresponding unmarked groups. Um, now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words in the lexicon, we find very different things. So while the generated personas have much higher rates of the lexicon words, um, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic. Um, so really just only the positive or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides, well at all. So instead, to do that, we'll turn to the results from our marked words method, to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, for mark groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity, and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there is a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like vibrant and curvaceous, um, which connect to a trope of tropicalism. For Asian women, the words are things like petite, and delicate, and silky, which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so on. Um, and finally, for black women, we see that some of the top words are things like strong and resilient. This connects to an archetype that people have called the strong black woman archetype, and while it sounds like positive at first glance, um, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure, um, on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. Um, more broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods because for instance, like these positive stereotypes, we don't know if it's because there's some sort of like weird overly excessive value alignment going on, or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns, we just really can't make any assumptions or really study that further without more transparency. Thank you so much for listening. Um, have a good time at ACL.</sample>
    <sample id="187">There are three authors involved in the paper.</sample>
    <sample id="188">Iterative transfer learning refers to updating a model by training it on the latest set of collected data. This approach is useful for transfer learning from a different domain.</sample>
    <sample id="189">The dataset's goal is to understand users' language when they make a choice.</sample>
    <sample id="190">An attacker may extract model parameters through learning from the embeddings offered by an EaaS, and then provide similar services.</sample>
    <sample id="191">Three authors are involved in the paper: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">This presentation introduces CAME: Confidence-guided Adaptive Memory Efficient Optimization. The speaker highlights the challenge of robustly training large language models (LLMs) with adaptive gradient-based optimization methods, which often triple the memory required for per-parameter gradients. Existing memory-efficient optimizers reduce memory usage but suffer performance penalties. The core challenge is to design an optimizer that achieves both fast convergence (like traditional adaptive methods) and low memory usage (like memory-efficient methods).

The method builds upon Non-negative Matrix Factorization (NMF), which reduces memory requirements from O(mn) to O(m+n) for an m x n matrix. Adamafactor, an NMF-based optimizer, provides an analytic solution for minimum I-divergence in rank-1 factors. However, the NMF operation in Adamafactor can lead to erroneous updates and slow convergence. To address this, CAME introduces a confidence-guided strategy. It uses the residual between the momentum of updates ($m_t$) and the current update ($u_t$) to quantify instability. This instability is then used as a denominator for $m_t$ to adaptively take an optimization step, improving stability and convergence.

Experiments show that CAME achieves significant improvements in accuracy during BERT training compared to Adam and Adamafactor, especially with larger batch sizes. For instance, with an 8k batch size, CAME increased validation accuracy by approximately 3.4% over Adamafactor using the same number of training steps. In downstream tasks, CAME models also achieved comparable performance to baselines with less memory cost. Quantitatively, CAME exhibited a reduced memory footprint (7.07 GB) compared to Adam (8.24 GB), LAMB (8.23 GB), Adamafactor (7.00 GB), and SM3 (7.44 GB) in pre-training BERT-Large. The conclusion emphasizes CAME's ability to provide adaptive, confidence-based updates for memory-efficient and effective training of large language models, particularly excelling in large batch training.</sample>
    <sample id="193">The given content does not mention the number of annotators used to create the initial dataset. It only mentions that the initial classifier was trained on 43 examples of dissonance.</sample>
    <sample id="194">The authors are affiliated with the University of Washington, Carnegie Mellon University, and the Allen Institute for AI.</sample>
    <sample id="195">The speaker introduces a novel approach called "Reasoning over Hierarchical Question Decomposition Tree" (RoHT) for explainable question answering (XQA). This method aims to overcome the limitations of existing XQA techniques, such as neuro-symbolic methods that rely on incomplete structured knowledge bases (KBs) and decompose-based methods that use only free-text corpora.

RoHT is a two-stage framework:
1. **Understanding the Complex Question:** This stage involves building a Hierarchical Question Decomposition Tree (HQDT). A BART-based question decomposer first generates atomic questions (leaf nodes), and then a BART-based question generator creates intermediate questions (non-root nodes) by grouping related leaf questions based on reference tokens. Each node is assigned a certainty score representing the likelihood of its generation.
2. **Probabilistic Reasoning over HQDT:** This stage recursively solves the complex question from root to leaves. It involves three steps for each node:
    * **Scheduler:** Determines appropriate knowledge sources (KB, text, or children's solutions) for the question.
    * **Executor:** Retrieves answers with probabilities from the selected knowledge sources.
    * **Aggregator:** Combines candidate answers from all sources and outputs the best ones with the highest probabilities.

The RoHT framework was evaluated on KQA Pro (50% original KB + Wikipedia) and Musique (original paragraphs + Wikidata) datasets. Results show that RoHT-KB outperforms existing KBQA methods, highlighting the benefits of integrating answers from sub-questions. RoHT-Mix, which combines text and KB, demonstrates substantial improvement over RoHT-KB and other baseline methods, proving the effectiveness of utilizing heterogeneous knowledge sources for complex question answering.</sample>
    <sample id="196">The example given for when the governor is on the left is, "I saw Bart and Lisa."</sample>
    <sample id="197">BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="198">Because large language models are coming up with longer and longer context windows, it is crucial to evaluate their acceptability throughout the context window.</sample>
    <sample id="199">Yes, training in a multilingual fashion caused performance drops in 7 datasets compared to the monolingual English model.</sample>
    <sample id="200">No, the annotators do not necessarily know about the entities in advance. They are given background knowledge about the two entities to help them.</sample>
    <sample id="201">The evaluation used state-of-the-art neural MT metrics.</sample>
    <sample id="202">The provided content does not include information about whether the generalization regression impacts specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can lead to systematic performance differences in technology between populations, as seen in design biases. It is crucial to study data set and model positionality, especially as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="204">The speaker does not mention how the multilingual LLMs like BLOOM were fine-tuned.</sample>
    <sample id="205">This research investigates the propagation of political bias from pretraining data to language models (LMs) and its subsequent impact on downstream NLP applications. LMs are trained on vast web-crawled datasets, including political news media, which contain diverse and often biased political opinions. This presents a mixed blessing: while enabling LMs to learn from a plurality of ideas, it also introduces social biases that can lead to fairness issues.

The study proposes a methodology to evaluate the political leaning of LMs using political questionnaires like the Political Compass Test. Preliminary results show that existing LMs exhibit varying political leanings, occupying all four quadrants of the political compass. Notably, GPT-4 leans more liberal than BERT-series models.

To understand the role of pretraining data, the researchers further pretrained LMs (RoBERTa, GPT-2) on partisan corpora from news and social media, divided by their political leanings (left, center, right). Results indicate that LMs' ideological coordinates shift predictably based on the political leaning of the training data. Additionally, LMs pretrained on data from before and after the 45th U.S. President's term show a general polarization shift away from the center, reflecting societal trends.

Finally, the study evaluates the per-category performance of these politically leaning LMs on hate speech and misinformation detection tasks. Left-leaning LMs excel at detecting hate speech targeting social minority groups but perform worse on hate speech against powerful groups, and vice-versa for right-leaning LMs. Similar trends are observed for misinformation detection. These findings highlight a critical fairness issue: an LM deployed on a social media platform, if fine-tuned on biased data, could marginalize opposing political opinions and allow hate speech targeting minorities to proliferate unchecked. This poses a "Scylla and Charybdis" dilemma: sanitizing data risks censorship, while not sanitizing it risks propagating biases.</sample>
    <sample id="206">They use the RoBERTA-base model with a classifier head for transfer learning.</sample>
    <sample id="207">The paper evaluates the translation capabilities of PaLM using the latest test sets to avoid test/train overlap and overfitting on evaluation data. They compare PaLM to state-of-the-art systems using the most recent training data from WMT submissions and use state-of-the-art MT metrics, along with expert-based human evaluation, which is more robust than crowd workers.</sample>
    <sample id="208">The authors proposed three recommendations.</sample>
    <sample id="209">The proposed method has a gain of approximately 30% over the strongest baseline (InstructGPT).</sample>
    <sample id="210">The speaker's name is Shuheng Liu.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for automatic text simplification.</sample>
    <sample id="212">They experiment with T5 (11B) fine-tuned on WikiHow and Coscript.</sample>
    <sample id="213">The pre-trained OFA-Large model (472M) is used as the base model.</sample>
    <sample id="214">Hello everyone. My name is Jin Wei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark".

Let's first introduce the background about embedding as a service. Currently, large language models (LLMs) such as GPT, LLAMA, PALM, are exceptional in natural language understanding and generation. Embedding as a service is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT3-based embedding API.

However, recent works have shown that the attacker may steal the model through learning from the embeddings and provide similar services. Therefore, it's necessary to protect the copyright of embedding as a service. To protect the copyright of embedding as a service, one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark. The watermark method needs to meet the following properties. First, the method should be applicable to embedding as a service. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker, or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process.

Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding as a service or lack of transferability. Therefore, in this paper, we propose EmbMarker, which is a backdoor-based watermark method applicable to embedding as a service.

Then, let me introduce the details of our EmbMarker. EmbMarker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate-frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service. The provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding.

Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then, the provider requests embeddings from stealer's service with the dataset. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset. Which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric.

We conduct experiments on four datasets: AG News, MIND, SST2 and Enron Spam. We assume the provider apply WikiText dataset to count word frequency. The results on four dataset show that our EmbMarker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embeddings by visualizing the embedding of sentences on four datasets via PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings.

That's all, thank you. Welcome to discuss with us.</sample>
    <sample id="215">The presenter discusses the dependency structure of coordination, outlining four different theoretical and corpus-based approaches: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London. The first two are asymmetric, while the latter two are symmetric.

The paper aims to provide a novel argument for symmetric structures of coordination based on the principle of Dependency Length Minimization (DLM). This principle suggests that word order tends to minimize dependency lengths. This is illustrated with examples showing that direct objects prefer to be close to the verb, while adjuncts can be further away. However, this preference can be overridden when a long, heavy direct object is moved after an adjunct, leading to a shorter total dependency length.

The researchers extracted statistics about coordination from an enhanced version of the Penn Treebank. They observed that left conjuncts tend to be shorter, and this tendency grows with the length difference between conjuncts. Crucially, this effect is only observed when the governor is on the left or is absent, and not when it is on the right.

This finding provides an argument against asymmetric dependency structures of coordination, such as Bouquet/Stanford and Chain/Moscow, and in favor of symmetric structures, like Conjunction-headed/Prague and Multi-headed/London. The full argument is detailed in the paper.</sample>
    <sample id="216">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler. and I will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimuIST, is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication. And what are the problems of the current SimuIST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involved in different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two second latency and so on. So what is our solution? First, to use already existing offline ST models without retraining or adopting specific architecture for SimuIST. Use only one model for every latency regime and handle latency through specific parameters. Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output, that is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention. And it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to. A word is emitted if the attention is not concentrated, that is its sum is below a threshold alpha, towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing "I am going to talk about...", and our model predicts the translation in German. And we will look at the cross-attention weights. We'll see that the first two words point to the earliest received speech frames, while the last word points to the last received speech frames, and the last lambda speech frames. This means that the first two words will be emitted. While since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word. And we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at the cross-attention weights. We will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we'll plot the simultaneous speech translation results on graphs, in which we have BLEU on one side that measures the translation quality. And average legging. That is uh, the latency measure, and we also considered the computational aware average lagging that accounts for um, the model's computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the wait-k strategy and the local agreement. And we compare also with the state of the art architecture specifically tailored for simultaneous speech translation. These are all the results of the simultaneous speech translation strategy on German. And we see that EDAtt outperforms all the strategies applied to offline models since their curves are shifted towards the left. And we also see that if we consider the actual elapsed time or the computational aware time, EDAtt is the fastest strategy. If you want to discover more results, read our paper, and we also released open source the code and uh models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="217">The presented work explores compositional generalization for multi-attribute controllable dialogue generation, highlighting a critical limitation in existing models regarding their generalization capabilities. To address this, the authors propose DCG, a disentangled controllable generation model. DCG leverages attribute concepts learned from "seen" values and incorporates a disentanglement loss to separate different attribute combinations, thereby improving the model's ability to generate new and unseen attribute combinations.

Furthermore, the paper introduces a unified reference-free evaluation framework, MAE, designed to assess controllability for various granularities of attributes. This framework aims to overcome the limitations of relying solely on annotated data for evaluating controllable dialogue generation. Through experimental results on the DailyDialog-CG dataset, the authors demonstrate that DCG significantly outperforms existing baselines in attribute controllability and text quality for generating novel attribute combinations. The model's attribute-oriented and task-oriented prompts, along with disentanglement learning, contribute to its strong performance. Additionally, the MAE metric shows a higher correlation with human judgments compared to traditional metrics, confirming its effectiveness in evaluating controllable dialogue generation. The qualitative analysis, including prompt visualization, further supports DCG's ability to disentangle attribute combinations and generalize from seen to unseen attribute pairings.</sample>
    <sample id="218">The affiliations of the authors of the paper are not mentioned in the provided English content.</sample>
    <sample id="219">Jia-Hui Ju, a research assistant at Academia Sinica, presents a novel approach for extracting financial signals from financial reports. The work introduces a "highlighting task" that identifies important words by comparing and contrasting content between two related financial documents (a target report and a reference report from the previous year). This task is motivated by two key observations: financial reports exhibit high token overlap (around 80% similarity between reports from the same company), and their content is yearly-dependent, with adjacent years showing more similarity than distant ones.

The proposed "multi-stage pipeline" addresses this task through several stages. First, document segmentation divides reports into comparable segments. Next, a "Relation Recognition" stage classifies reference-to-target segment pairs into three types: insignificant relations (uninformative), revised relations (differ in few words but disclose different meanings, e.g., "increase" vs. "decrease"), and mismatched relations (mutually exclusive meaning, e.g., new policies). The core of the pipeline involves a two-staged fine-tuning approach for a "domain-adaptive highlighter." This includes out-of-domain fine-tuning on an external natural language inference dataset (e-SNLIc) with token annotation, followed by in-domain fine-tuning on the "Revised" pairs using pseudo-labels. Pseudo-positive labels are assigned to revised words, and a few other words are randomly labeled as negative. To alleviate issues from low-quality pseudo-labels, the model mixes cross-entropy and KL divergence objectives.

Evaluation metrics for the highlighting task include R-Precision (discrete) and Pearson Correlation Coefficient (PCC, continuous). The experimental results demonstrate that the domain-adaptive highlighting model outperforms all other settings on the in-domain FINAL dataset and even preserves generalization capability on the e-SNLIc dataset. Furthermore, the method shows benefits in identifying unseen relations (mismatched pairs) that were not used during training. Future work will focus on improving effectiveness, adding more features like bi-directional rationalization, exploring end-to-end applications, and incorporating multi-modality analysis (charts, tables, cross-company/sector data).</sample>
    <sample id="220">The authors are affiliated with Stony Brook University and Human Language Analysis Beings.</sample>
    <sample id="221">The only language pair mentioned in the provided English content is German to English.</sample>
    <sample id="222">The speaker introduces a research paper titled "To Adapt or to Annotate: Challenges and Interventions in Open-Domain Question Answering." The paper investigates data interventions to enable out-of-domain generalization, understands the compatibility of source models for target domains, and determines the relationship between data interventions and dataset shifts.

Open-domain question answering involves a retriever model that fetches relevant passages from a large document corpus, such as Wikipedia, and a reader model that generates an answer based on these passages. When adapting to new domains like biomedical data, simply expanding the document collection (e.g., adding PubMed to Wikipedia) can confuse the retriever, leading to incorrect answers.

The research proposes two adaptation methods: few-shot and zero-shot interventions. Few-shot interventions use a small number of target-domain examples to prompt a Large Language Model (LLM) to generate more domain-specific data, improving retriever performance by approximately 8% and reader performance by 11% on average across all target datasets.

Zero-shot interventions, which do not rely on target-domain examples, involve varying the question, answer, and context. The study finds that question format (standard vs. cloze) does not significantly impact performance, but cloze-style questions are easier to collect. For answers, a uniform distribution of entity types in generated cloze questions works best. For context, learned retrievers are sensitive to data distribution, while traditional methods like BM25 perform better when pooling documents from source and target domains.

A generalizability test categorizes dataset shifts into "no shift," "concept shift," "covariate shift," and "full shift" based on retriever and reader compatibility. The research demonstrates that the effectiveness of data interventions depends on the type of dataset shift. Few-shot adaptations are generally effective across all shift types, while zero-shot adaptations benefit datasets with concept and covariate shifts, and those with no shift.</sample>
    <sample id="223">The speaker's name is Shangbin Feng.</sample>
    <sample id="224">The experiments investigated two different models, long-mBART and mBART.</sample>
    <sample id="225">Out of the 62 diverse tasks in MultiInstruct, 53 tasks are used for training and 9 tasks are used for testing.</sample>
    <sample id="226">Three authors are involved in the paper.</sample>
    <sample id="227">The presenter introduces "Pangu: A Unified Framework for Grounded Language Understanding," addressing the limitations of current language models (LMs) in this area. While LMs have seen great success in NLP tasks, they predominantly rely on textual corpuses for pre-training, leading to a lack of grounding in real-world environments. This gap results in challenges for grounded language understanding, which requires mapping natural language to executable actions or programs within specific target environments (e.g., smart assistants, search engines, robotic systems).

Existing research often uses LMs to directly generate plans via autoregressive decoding. However, this approach frequently produces grammatically incorrect or invalid outputs that are not executable. To overcome this, the presenter proposes the Pangu framework, which shifts the role of LMs from generation to discrimination. In Pangu, a symbolic agent interacts with the environment to propose only valid candidate plans, and the LM is then responsible for scoring and ranking these proposals. This separation of concerns allows LMs to focus on tasks they excel at, avoiding the complexities of ensuring grammatical and environmental validity during generation.

The framework is exemplified through knowledge-base question answering (KBQA), a typical scenario with a massive, heterogeneous environment. Experiments demonstrate that Pangu achieves state-of-the-art performance across various settings, including fine-tuning and in-context learning, and shows strong sample efficiency, especially with large LMs like Codex. Furthermore, Pangu exhibits improved generalizability under non-i.i.d. (independent and identically distributed) settings, indicating that autoregressive models tend to overfit seen structures during training, while Pangu maintains consistent performance on both seen and unseen structures. The key message is that for grounded language understanding, discrimination is a more optimal strategy for LMs than direct generation.</sample>
    <sample id="228">The authors experimented on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This presentation by Gabriella Skitalinskaya, co-authored with Henning Wachsmuth, delves into the task of detecting improvable claims within argumentative writing, aiming to provide support for authors. The core idea is to learn from human revision behaviors, recognizing that text revision is a recursive process striving for optimal phrasing, which directly impacts persuasive effect.

The research introduces two key tasks: Suboptimal-Claim Detection, where a model decides if a claim needs revision or is optimally phrased, and Claim Improvement Suggestion, where it identifies specific quality issues needing improvement. The approach leverages implicit revision patterns found in collaborative editing platforms like Kialo, where revised claims are labeled as "optimal" and their predecessors as "suboptimal."

The study highlights four main challenges: Representativity and Reliability (ensuring a reliable dataset from revision histories), Model Complexity and Architecture (selecting models sensitive to subtle changes and pre-training effects), Contextuality (understanding how context, such as topic expertise or parent claims, influences quality), and Topical and User Bias (addressing noise and biases from users and controversial topics).

Through analysis and experiments, the findings suggest that revision-based data is effective for these tasks. Furthermore, modeling the "distance" between two claim versions proves beneficial for suboptimal-claim detection, and the impact of contextual information is dependent on the specific task and quality issue. The authors invite further exploration of their work and data on GitHub.</sample>
    <sample id="230">Hi everyone, I'm Koustuv Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper, "Language model acceptability judgements are not always robust to context". This is a joint work with Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, and Adina Williams. So in this work we revisit the minimal pair paradigm. So the minimal pair paradigm basically evaluates language models on top of acceptability judgements which can also include grammaticality like Blimp SyntaxGym or acceptability in terms of stereotypes such as CrowS pairs. And in this minimal pair paradigm the typical way to evaluate language models is that you show uh like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically uh puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate models' acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the models' acceptability on throughout the context window and that is what we are trying to do here we are trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach, so what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing uh like acceptable or unacceptable sentences from those data sets. So for example here we have chosen like a typical pair of grammaticality from the Blimp data set, from the Adjunct Island uh case. And what we do is that to create like longer sequences and which are acceptable and which has the same matching of the grammatical structure, we extract grammatical sentences from Adjunct Island and then we added as a prefix to both the acceptable query and the unacceptable query. We can do the same thing by choosing unacceptable sentences from the same uh matching and that could also like be used to test the model's acceptability. And we can do the same for unacceptability case. Finally we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgements are actually impacted by any context. Like whether the context is coming from a different uh subset of the data set or whether it's like completely irrelevant to the current uh like to the sentence that we are looking at. So how does the model do? So first we look at the Wikipedia sentences which are completely irrelevant to the current query pair and there we find that the MPP judgements are mostly robust for arbitrary context lengths. We increase the context length toward up to 1024 for to max out OPT and GPT2 models and we saw here in the orange dotted line the MPP judgements are relatively stable. Now what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable uh domains from the same Blimp or SyntaxGym data set and there we see that the MPP judgements either increase or decrease significantly when you add uh either acceptable prefixes or unacceptable prefixes. But when we match the structure that is when we choose the sentences from the same phenomena in Blimp or SyntaxGym, we see a massive increase or a massive decrease in of the MPP uh judgement for the model depending on whether the chosen prefix is acceptable or unacceptable. Now this uh and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgements so much? So we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding uh like noise to the input and after doing like several of these perturbations, we find that none of these noises are actually making the model uh like change it course in terms of how it shows us the MPP judgement trend. Basically we find that the models are sensitive to the perturbed sentences in similar ways. That is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in MPP judgements in similar fashion. So the key takeaways of our work is that language models are sensitive to latent syntactic/semantic features which are shared across the sentences and the MPP evaluation, the the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.</sample>
    <sample id="231">NACHOS is a 1.1 billion words open-source dataset of heterogeneous data crawled from diverse medical domains, natures, and styles.</sample>
    <sample id="232">The speaker's name is David Vilar Torres.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) processes spoken language into text in another language in real time, enabling cross-language communication. Current SimulST models face several problems: they use specific architectures that introduce additional modules to be optimized, have long and complicated training procedures, and require training and maintaining several models to reach different latency regimes (e.g., 1s, 2s, etc.).

Our solution, EDAtt (Encoder-Decoder Attention), addresses these issues by using already existing offline speech translation (ST) models without retraining or adopting specific architectures for SimulST. We employ only one model for every latency regime, handling latency through specific parameters. EDAtt leverages the knowledge already acquired by the model through the attention mechanism between audio input and textual output. This strategy decides whether to emit or not a partial translation based on where attention points. A word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is stable enough. If the attention sum towards the last λ speech frames is above the threshold, the word is not emitted, and the model waits for more speech input.

Our main results show that EDAtt outperforms all the strategies applied to offline models and is the fastest strategy when considering the actual elapsed time. We released our code and models open-source to facilitate reproducibility.</sample>
    <sample id="234">The prompting strategy has a significant impact on translation quality, with differences of more than 1 BLEURT point in most cases, and up to 40 BLEURT points in extreme cases, based on an experiment with one-shot prompting using two different prompts for each sentence. The quality of the example translation used in the prompt is more important than its similarity to the source sentence, and utilizing high-quality translations from the dev data significantly improved performance compared to using the training data.</sample>
    <sample id="235">The authors are affiliated with Carnegie Mellon University, Language Technologies Institute, Instituto Superior Técnico, University of Lisbon, BAIR (Berkeley Artificial Intelligence Research), and Unbabel.</sample>
    <sample id="236">The 5 expert-written instructions are "Given the content of image, do you have enough information to answer 'Is it a sunny day?'? Options: 'the question is relevant to the image' or 'the question is irrelevant to the image'", "Select the region of the object described by 'A blue train in the front.'. Options: &lt;bin_242&gt; &lt;bin_180&gt; |||| &lt;bin_736&gt; &lt;bin_475&gt;", "Select the region that contains the text 'den'. Options: &lt;bin_206&gt; &lt;bin_119&gt; |||| &lt;bin_448&gt; &lt;bin_518&gt; |||| &lt;bin_456&gt; &lt;bin_574&gt; |||| &lt;bin_229&gt; &lt;bin_604&gt; |||| &lt;bin_304&gt; &lt;bin_654&gt;", "Generate a caption for &lt;bin_198&gt; &lt;bin_32&gt; &lt;bin_400&gt; &lt;bin_193&gt;.", and "blue and white tennis racquet".</sample>
    <sample id="237">The authors propose to test the models on a coreference resolution task.</sample>
    <sample id="238">This video introduces MeetingBank, a new benchmark dataset for meeting summarization. The dataset was created to address the scarcity of high-quality meeting summaries and the difficulty of finding reliable sources for public meetings.

MeetingBank contains a comprehensive collection of city council meetings, including meeting transcripts generated from audio data, human-written reference summaries extracted from meeting minutes, and URLs to the original resources (video and minutes). The dataset includes 1,366 city council meetings from six different U.S. cities, with nearly 7,000 summarization instances.

The presentation provides a step-by-step guide on how the data was collected, involving the use of speech-to-text APIs, careful identification of meeting and item IDs, and alignment of segments with corresponding summaries. Statistical analysis of the dataset shows that a typical meeting lasts approximately 2.6 hours and contains 28,000 tokens, with segments consisting of 2,900 tokens in the source and 87 tokens in the summary.

Dataset analysis using extractive fragment coverage and density reveals that most city council meeting summaries have a coverage score between 0.7 and 0.9, indicating a focus on important points rather than pure abstraction. Seattle and Boston exhibit the highest density, while Denver shows the lowest, suggesting varying levels of editing on meeting minutes across cities.

The video also discusses model evaluation, comparing ten summarization systems, including extractive (Extract-Oracle, Lead-3, LexRank, TextRank), abstractive (BART_Large, Pegasus, Longformer, DialogLM, HMNet), and large language models (GPT-3). For extractive systems, Extract-Oracle performs best, indicating that reference summaries largely come from source transcripts. For abstractive systems, DialogLM, designed for long dialogues, yields the highest ROUGE-2 score. Human evaluation, using a 5-point Likert scale across five criteria (informativeness, factuality, fluency, coherence, and redundancy), shows that GPT-3 achieves the highest overall score, especially in fluency and coherence. However, GPT-3's performance in informativeness and factuality is less impressive, suggesting a need for future summarization solutions to prioritize capturing main discussion points accurately.

In conclusion, MeetingBank serves as a valuable resource for researchers developing advanced meeting summarizers and provides insights into the decision-making process of city councils. The dataset is publicly available at MeetingBank.github.io.</sample>
    <sample id="241">This paper introduces a Human-in-the-Loop (HiTL) evaluation framework for early misinformation detection, focusing on COVID-19 treatment misinformation on Twitter. Current automated systems for misinformation detection often fall short, being unrealistically evaluated using retrospective datasets and lacking human integration. This can lead to issues like "leaked counter-evidence," where systems effectively "learn" from debunked claims, making them unhelpful for new, emerging misinformation.

The proposed HiTL framework addresses these challenges by creating an end-to-end misinformation detection system that integrates human feedback at various stages. The system processes raw tweets, filters relevant information using keywords, and extracts check-worthy claims through a T5 model. These claims are then ranked by "trendiness" (statistical popularity) before being presented to human experts for validation. For policy violation verification, a BERT-based stance classification model identifies tweets supporting or refuting misinformation, flagging supporting tweets for human review.

Evaluation demonstrates the framework's efficacy in early claim detection, identifying unapproved treatments before they are publicly debunked in news articles. This "early detection" is crucial for timely intervention against misinformation. Furthermore, the system achieves a 65% precision in identifying policy-violating tweets, with human moderators able to confirm 124.2 such violations per hour, highlighting the system's efficiency and the value of human involvement. The authors hope this work will spur the development of more human-centric and realistically evaluated misinformation detection systems.</sample>
    <sample id="242">Common evaluation methods include comparative evaluation (asking human judges to pick a better conversation) and Likert rating evaluation (rating conversations on a Likert scale).</sample>
    <sample id="243">There are 5 authors involved in the paper.</sample>
    <sample id="244">The background knowledge needed is that judges decide cases in courts of law.</sample>
    <sample id="245">The speaker presents a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The motivation for this pipeline stems from the problematic nature of automatic metrics and the lack of understanding surrounding best practices for worker recruitment on MTurk.

The pipeline consists of a Qualification Task and an Endurance Task. The Qualification Task evaluates annotators' ability to correctly assess multiple dimensions of a summary using three documents (one with an attention check) and one summary per document, with six dimensions of evaluation. Workers are categorized into four types: GOLD, SILVER, BRONZE, and BLOCK, with only GOLD and SILVER workers passing. Out of 200 participants, 26 (13%) qualified for the next task.

The Endurance Task tests the annotators' capacity to handle heavy workloads, involving 10 HITs, one document, and four summaries each, focusing on salience. This stage resulted in 12 (6%) qualified workers (4 GOLD, 8 SILVER) who achieved higher inter-annotator agreement (IAA) than experts, with a best Cohen's Kappa of 0.55 and Krippendorff's Alpha of 0.443 for GOLD workers.

For the Reference-based Task, designed to test general performance on true annotation tasks, pipeline workers achieved a Cohen's Kappa of 0.68 and Krippendorff's Alpha of 0.534. Comparing this to baseline MTurk workers and CloudResearch workers, the pipeline effectively reduces resource waste and achieves high agreement at a lower cost, demonstrating similar quality to CloudResearch. However, the pipeline does not guarantee correctness, and the designed questions are not "panacea" solutions. Future work aims to hire high-quality workers based on both high agreement and correctness across multiple applications, languages, and platforms.</sample>
    <sample id="246">Yes, the code is available on GitHub at mpoemsl/kitmus.</sample>
    <sample id="247">The presenter introduces a new dataset called FactKG, which focuses on fact verification using knowledge graphs (KGs). The motivation for this new task stems from the absence of datasets that use KGs as evidence for natural language claims, unlike existing datasets that rely on text or tables. FactKG aims to fill this gap by leveraging the reliability and practicality of KGs as a knowledge source.

FactKG uses DBpedia as its knowledge graph and includes claims in both written and colloquial styles to enhance practical applicability. It features two labels: SUPPORTED and REFUTED. The task involves retrieving evidence from DBpedia and verifying the claim using this evidence.

The dataset incorporates five types of reasoning:
1.  **One-hop:** Verifies if two entities in a claim are connected by a single relation.
2.  **Conjunction:** Involves multiple one-hop claims that all need to be verified.
3.  **Existence:** Checks if an entity in the claim is connected to a specific relation.
4.  **Multi-hop:** Requires multi-hop inference because some entities are not explicitly present in the surface form of the claim.
5.  **Negation:** Involves an additional inference step for negation, even after finding graph evidence.

To create colloquial claims and presupposition templates, FactKG employs specific paraphrase methods. Baseline experiments, including "Claim Only" models (BERT, BlueBERT, Flan-T5) and a "With Evidence" model (GEAR), were conducted. The results indicate that all baselines outperform the majority class baseline (51%), and the GEAR model, which utilizes graph evidence, significantly outperforms all other baselines. The dataset and code are publicly available for download.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced across all demographics. While the platform LabintheWild allows for recruiting diverse volunteers from various countries, the resulting participant distribution for the study included a significant number of English-speaking countries and people with college or graduate school education, leading to an imbalance.</sample>
    <sample id="249">When sentences in the acceptable domain were perturbed, they showed a similar increase in all perturbations.</sample>
    <sample id="250">A dimensional evaluation means evaluating multiple dimensions of chat quality to understand the strengths and weaknesses of a model on a finer-grained level.</sample>
    <sample id="251">The authors are affiliated with the University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia.</sample>
    <sample id="252">This presentation introduces U-CREAT, an unsupervised approach to prior case retrieval (PCR) that leverages event extraction. The challenge in PCR is that legal professionals traditionally rely on experience to cite relevant past precedents, but with the increasing volume of cases, this becomes difficult. PCR aims to automatically retrieve relevant legal documents from a candidate pool based on factual and precedent relevance.

The work contributes a new benchmark dataset called IL-PCR (Indian Legal Prior Case Retrieval Dataset), consisting of 7070 legal cases with an average of 6.775 citations per query document. This dataset offers a comprehensive testbed for PCR algorithms, notably larger and more complex than existing datasets like COLIEE'21.

The U-CREAT pipeline involves three main steps: preprocessing, dependency parsing, and post-processing, to extract "events" from both query and candidate legal documents. An event is defined as a predicate (typically a verb) and its corresponding arguments, derived from dependency graphs. These extracted events are then used to compute interaction matrices, which are fed into retrieval models.

Experiments were conducted using various models categorized into count-based (word-level), transformer-based, and event-based. Surprisingly, transformer-based models, including those pre-trained on legal text, performed poorly compared to baseline BM25 methods. However, event-based models, particularly "Event Filtered Docs" (using quad-grams with BM25), demonstrated significant performance boosts: 25.3 F1 score in IL-PCR and 12.6 F1 score in COLIEE'21 over the standard BM25 baseline. This method also outperformed supervised approaches on the COLIEE'21 dataset (27.32 F1), establishing a new state-of-the-art for unsupervised PCR on that dataset. Furthermore, event-based models exhibit lower inference times.

U-CREAT is unsupervised, does not require corpus-specific fine-tuning, and generalizes across different legal systems (Indian and Canadian), offering a highly efficient and effective approach for legal case retrieval.</sample>
    <sample id="253">The presenter introduces DisorBERT, a double domain adaptation model for detecting signs of mental disorders in social media. Mental disorders are psychological syndromes impacting thinking, feeling, mood, and behavior. The prevalence of social media provides an opportunity to study mental health, as people use these platforms to share their lives and seek support.

The research leverages domain adaptation to enhance a language model's performance on a specific target domain. Starting with a base BERT language model, the approach integrates information from Reddit (to learn social media language) and mental health datasets (to specialize in the mental disorders domain). A "guided masking" technique is also employed, using a lexicon to focus the model's attention on important words during training.

Results from precision and recall analysis on eRisk datasets demonstrate DisorBERT's effective balance in detecting various mental disorders like anorexia, depression, and self-harm, outperforming other baseline models. Analysis of the model's word predictions using the Beck's Depression Inventory (BDI-Test) shows that DisorBERT generates words with a more negative or psychologically oriented meaning compared to BERT, indicating its specialization.

A user analysis for depression further illustrates the model's ability to identify relevant words and sentences within a user's post. For example, in a post about Borderline Personality Disorder and medication, DisorBERT highlights words like "anxious" and "medication," which are highly relevant to depression.

The presentation concludes that the combined effect of double domain adaptation and guided masking is effective in capturing signs of mental disorders in social media interactions. DisorBERT achieves better results than MentalBERT with less computational resources and demonstrates a solid balance for clinical detection applications. Future work will explore different lexical resources and the usage of clinical data to train even more specialized language models.</sample>
    <sample id="254">The speaker introduces a research work on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." Document-level Relation Extraction (DocRE) extracts relations among entities in a document. Existing methods using distant supervision (DS) data, which is auto-labeled from knowledge bases, suffer from noisy labels, often resulting in false positives and loss of correct relations. To address this, the presented work proposes an uncertainty-guided label denoising framework.

The methodology involves training a pre-denoising DocRE model using both DS and human-annotated (HA) data to generate pseudo labels. An instance-level uncertainty estimation is introduced to quantify the trustworthiness of these pseudo labels, especially for overlapping relations where traditional methods struggle to differentiate between correct and false positives. This estimation determines whether a model's prediction can be trusted or not. Furthermore, the framework proposes dynamic class uncertainty thresholds, acknowledging that the distribution of uncertainty scores varies across different relation classes. Frequent classes tend to have lower average uncertainty compared to long-tail classes. This allows for filtering out pseudo labels with high uncertainty based on their specific class. An iterative multi-phase training strategy is designed to fully leverage the DS data, boosting the performance of the DocRE model by progressively refining the pseudo labels. Experimental results on two public datasets, DocRED and Re-DocRED, demonstrate that the proposed framework, UGDDRE, outperforms existing baselines, indicating significant performance improvements through enhanced label quality of the DS data.</sample>
    <sample id="255">The actual form of the prompting is crucial for zero and one-shot prompting strategies, but for several-shot prompting, there is barely any difference.</sample>
    <sample id="256">Hello. My name is Vasudha, and I'm a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge. We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent. Such as this example, where a person states, "I know that cigarettes could kill me," and then goes on to say, "I grabbed a couple smokes after the meeting today." This belief and action are inconsistent, and they are in dissonance. Further, mentioning that "I don't think I could keep my job without them," justifies the second occurrence, and they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So, why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief, values and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used a dissonance-first approach as seen in the flowchart here. Tweets were parsed using a PDTV parser and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around a thousand examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior search dataset, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonance samples can be collected over lesser annotation rounds, lowering the overall annotation costs while improving dissonance detection. Um, since the initial model was not able to capture the dissonance class at all, we start the cold, uh, active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic-independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called Debate here. And on binary classification of expansion and comparison classes of PDTV, since these two are closely related to the conception of consonance and dissonance, and we call them CE here. We find that on transferring, the zero-shot performance on the annotated dataset is already much better than chance with the best with AUC 0.62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE task followed by further fine-tuning on Debate yields a much better zero-shot performance. Thus, this is the model that we use to cold-start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotation so far, whereas iterative updates the model by training on the latest set of data collected. Over the different strategies, we found that cumulative performed equal or better than iterative across the board. Next, to improve the number of dissonance examples, we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL. We compare this to the other state of the, uh, state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition, and cold-starting AL with appropriately designed transfer learning task can help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in-domain active annotations benefit from cumulative update. These are the links to our code, dataset, and our paper. Uh, feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="257">The authors evaluated four open-domain dialogue models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="258">This video introduces a novel approach to evaluating the quality of text generated in natural language processing (NLP): using Large Language Models (LLMs) for evaluation. The speaker explains that this method, termed LLM evaluation, involves instructing LLMs to rate text samples based on specific criteria. This approach is positioned as a potential alternative to human evaluation, which, despite being widely adopted, is acknowledged as unstable and hard to reproduce.

The presenter highlights that while the idea of using LLMs for evaluation might now seem natural (with related works like G-Eval emerging), their paper was submitted to ACL 2023 without prior works exploring this specific concept, making it novel at the time.

To validate the efficacy of LLM evaluation, experiments were conducted where various LLMs (T0, InstructGPTs including curie and davinci, and ChatGPT) rated stories generated by GPT-2 and human writers. The evaluation was based on four attributes: grammar, coherence, likeability, and relevance. English teachers, considered experts in rating stories and essays, also performed human evaluations on the same stories using identical instructions, providing a ground truth for comparison.

The results showed that human raters preferred human-written stories over GPT-2 generated ones. Interestingly, smaller LLMs (T0 and curie) did not exhibit a significant preference towards human-written stories. However, larger LLMs (davinci and ChatGPT) demonstrated a clear preference for human-written texts, mirroring the behavior of the human experts. This finding suggests that certain LLMs can indeed serve as a viable alternative to human evaluation in this context.

The speaker concludes by inviting the audience to read their paper or visit their poster at ACL for further details on various related questions, such as the agreement between LLMs and human evaluators on individual stories, the impact of instruction wording on LLM evaluation, and a comprehensive analysis of the pros and cons of LLM evaluation compared to human evaluation.</sample>
    <sample id="259">This presentation introduces XSemPLR, a unified benchmark for cross-lingual semantic parsing. Semantic parsing, the process of converting natural language queries into machine-understandable representations like SQL or Lambda Calculus, is crucial for natural language interfaces. However, existing cross-lingual semantic parsing (CLSP) models suffer from limitations, including a lack of coverage across various natural languages and meaning representations, as well as being evaluated on limited tasks and neural models.

XSemPLR addresses these challenges by offering a comprehensive dataset that covers 9 datasets in diverse domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages across 15 language families. The presentation outlines six experimental settings for training and evaluation: Translate-Test, Monolingual Model (including Few-shot), Multilingual Model, and Cross-lingual Zero-shot/Few-shot transfer.

Key findings from the experiments include:
- Encoder-Decoder models (specifically mT5) consistently outperform other models in the monolingual setting across all datasets.
- Multilingual training can improve performance for both Encoder-Decoder and Encoder-PTR models, especially for under-resourced languages.
- A "Curse of Multilinguality" is observed, where English performance drops in 7 datasets despite overall gains for most languages during multilingual training.
- Cross-lingual zero-shot transfer shows a significant performance gap compared to monolingual settings, which is rapidly shortened in the few-shot setting.
- Multilingual Large Language Models (LLMs) like Codex and BLOOM are currently inadequate for cross-lingual semantic parsing tasks.
- Chinese transfer learning and English monolingual training exhibit the largest performance gap, while German generally has the smallest.
- FunQL outperforms other meaning representations, with SQL yielding the worst performance.

XSemPLR provides a valuable resource for advancing CLSP research by enabling more robust and comprehensive evaluations of multilingual semantic parsing models.</sample>
    <sample id="260">There are 12 authors involved in the paper.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">There are nine authors involved in the paper: Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, and Deqing Yang.</sample>
    <sample id="263">This presentation introduces a novel approach to mitigating label biases in in-context learning for text classification tasks. The speaker begins by highlighting the instability of in-context learning, which arises from various design choices that introduce biases into model predictions. Previous research has primarily focused on vanilla-label bias (uncontextual preference for label names) and context-label bias (effects from the context).

However, this work identifies and conceptualizes a new type of bias: domain-label bias. This bias describes the influence of the task corpus on the model's predictions. Experiments with random in-domain words show that they can severely bias the model's predictions, unlike random English words. This finding indicates that large language models (LLMs) exhibit different in-context learning behaviors based on the level of domain-label bias in the task. Tasks with small domain-label bias perform well, and advanced calibration methods further improve performance. Conversely, tasks with large domain-label bias severely restrict LLMs' performance to chance-level, even with prior calibration methods.

To address this, the presentation proposes a new calibration method called Domain-context Calibration (DC). Unlike prior methods that use a single predefined content-free token to estimate bias, DC uses multiple random in-domain words sampled from the task corpus. This allows DC to mitigate all three types of label biases holistically. Ablation studies confirm that predefined tokens can be biased, using a single content-free token is suboptimal, and calibrating with random in-domain words effectively removes domain-label bias. Overall, DC significantly improves in-context learning, particularly on tasks with large domain-label bias, a trend that holds for larger models like GPT-3.</sample>
    <sample id="264">The speaker introduces a novel task called Transferable Audio-Visual Text Generation (TAVT), designed to address the challenges of data annotation and domain shifts in multi-modal text generation. Unlike existing uni-modal approaches like machine translation and image captioning, which benefit from large-scale pre-training, TAVT focuses on multi-modal scenarios where data annotation is arduous and expensive. The primary challenge identified is multi-modal domain shifts, where visual content, such as image style and shooting angle, significantly changes across domains, while auditory content (rhythm, energy) remains relatively stable for the same event.

To overcome this, the proposed framework consists of three components: an Audio-Visual Meta-Mapper Network (AVMM), an Audio-Visual Encoder &amp; Language Model Generator (AVE &amp; LMG), and Counterfactual Contrastive Learning. The AVMM aims to map diverse visual concepts across domains into a unified auditory semantic space, utilizing visual prefixes to reconstruct audio and align visual content with audio space. The AVE &amp; LMG employ a Transformer-based architecture, incorporating a learned attention mechanism (alpha) to evaluate the contribution of each modality to word generation. Finally, two counterfactual contrastive learning losses—distribution-based and dependency-based—are introduced to optimize visual-audio and visual-text alignment, respectively. This meta-learning approach trains on a set of source domains (meta-train) and adapts to new target domains (meta-test) with limited labeled data. Experiments demonstrate that TAVT significantly outperforms comparative models on both cross-dataset and cross-domain benchmarks, especially in low-resource target domains.</sample>
    <sample id="265">The speaker's name is Vasudha.</sample>
    <sample id="266">The authors of the paper are affiliated with the Institute of Computer Science, Polish Academy of Sciences, and the University of Warsaw.</sample>
    <sample id="267">Hello everyone, my name is Yusen Zhang from the Penn State University. Today I'm going to present our work, XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. So, semantic parsing is a task to build semantic representations of user queries, such as SQL and Lambda Calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations, as shown in this figure. Uh, we need to translate the query in multiple natural languages using neural models to SQL, Lambda, or FunQL, and etc. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: there is lack of coverage on certain natural language, uh, Chinese is missing. And lack of coverage on certain meaning representations, the Lambda Calculus is missing, or they're only evaluated on certain neural model. For example, there's only one single model to evaluate them. So, to this end, we propose XSemPLR. We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google Translate API to translate source to the target language. Then use monolingual model to train and eval. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we also test monolingual model. In this setting, the source language is the same as target language, e.g. German-to-German, or English-to-English. We also test monolingual few-shot setting by training monolingual models with only 10% training data. And we test multilingual model, which, uh, we train one multilingual model for all languages. For example, uh, we put the German, English, Chinese queries together to train a multilingual model. And during inference, uh, we can, uh, use this model to, um, to translate German queries or Chinese query or, etc. And we also consider cross-lingual zero-shot and few-shot transfer. We train on one source language and transfer to another language. So, during training, uh, we train it on English query, or the combination of English and German few-shot queries to train a multilingual model to, and predict the SQL output. And we also find many interesting results. So, regarding analysis of, uh, monolingual models, we evaluate on two groups of models, uh, including Enc-PTR, which stands for multilingual pre-trained encoders with pointer-based decoders, such as XLM-R + PTR, mBERT + PTR. And, uh, we also evaluate Enc-Dec models, which is multilingual pre-trained encoder-decoder models. Um, such as mBART and mT5. We found that Enc-Dec obtains the best performance on all nine datasets. And we evaluate on mT5 and exam, uh, XLM-R + PTR on multilingual setting. We found that Enc-Dec/Enc-PTR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. I think this is known as "Curse of Multilinguality". We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found that for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that for few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, um, Enc-Dec (mT5) outperforms previous work or achieves comparable results. Pre-training on the English natural language can significantly boost the performance of few-shot on target natural languages. And we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. Um, to sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show, uh, many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">The authors are affiliated with Emory University and Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for "continuous fine-tuning".</sample>
    <sample id="272">There are 7 authors involved in the paper.</sample>
    <sample id="274">The speaker is Yusen Zhang.</sample>
    <sample id="275">Hi, I'm Shanbin, PhD student at the University of Washington. Today I'm presenting our work from pretraining data to language models to downstream tasks: tracking the trails of political biases leading to unfair NLP models. So, language models are trained on large-scale web-crawled data. Political news media are well covered in their pretraining data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc., are well covered in language model training data. This has created a mixed blessing for language model applications. So, on one hand, they are able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks. Specifically, by asking the following questions. First, how do we evaluate the political leaning of language models? And what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So, specifically, we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation well-grounded in political science literature. So, some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political compass. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BERT series and its variance. Secondly, we aim to investigate to which to which extent the political biases of language models are actually picked up from training data. So, we conduct a control experiment by further pretraining language model checkpoints on six different partisan corpora, separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa, further fine-tuned and further trained on the left-leaning Reddit corpus, we can see a substantial liberal shift in terms of its in terms of its political biases. And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society. So, we divide pretraining corpora into pre-45th President of the United States and after 45th President of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the center after 2017. So, this indicates that language models can also pick up the like polarization in our society. So, last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection, two NLP applications that often involve language models and could have very significant implications. So, we see that if we investigate the per-category performance, that is to say, if we separate the performance into different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups. However, worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men. However, worse at detecting hate speech targeting at black, LGBTQ+ and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. This indicates we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social category. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if a right-leaning language models were to be fine-tuned on hate speech or misinformation, whatever, and deploy to a popular social media platform, this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control. So, this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings. So, a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis. So, if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitize somehow, we would also risk censorship or exclusion, and it's incredibly hard to determine what is actually neutral and should be retained in language model training data. So, it's kind of like the electric electric trolley problem. Okay, great. I think that's pretty much all I have for today. Thank you for your time.</sample>
    <sample id="276">This paper introduces IndicMT Eval, a new dataset for meta-evaluating machine translation (MT) metrics for Indian languages. Existing MT evaluation metrics and meta-evaluation studies primarily focus on English as a target language, neglecting the unique linguistic features of other languages like Indian languages. These features include distinct grammar rules, shared vocabulary, dialects, varying sentence structures, and differing resource availability.

To address this gap, IndicMT Eval focuses on five Indian languages from two distinct language families: Tamil and Malayalam (Dravidian), and Hindi, Marathi, and Gujarati (Indo-Aryan). For each of these languages, 200 English source sentences were randomly selected from the Flores dataset and translated using seven different MT models/APIs, resulting in 1400 candidate translations per language and a total of 7000 samples.

Human annotations were collected using the MQM framework, involving bilingual expert annotators who highlighted minor/major errors and categorized them into accuracy or fluency errors (with sub-categories like addition, omission, mistranslation, spelling, grammar, etc.). An overall score was also provided for each translation. Analysis of the annotations revealed that recent MT models (NLLB, IndicTrans) generally produce fewer errors.

Metric-wise, overlap-based metrics like CHRF++ showed the highest correlation with human scores among their type, while embedding-based metrics like MuRIL and mBERT (especially IndicBERTScore and distilmLMBERTScore) performed better overall. COMET metric variants showed the highest overall correlations. The study also highlighted that many metrics tend to provide a skewed range of scores, unlike human scores which utilize the full scale.

Finally, the paper demonstrates the fine-tuning of COMET metric variants using the MQM annotations, resulting in a new metric called IndicCOMET. IndicCOMETMQM consistently outperformed COMET baselines on most languages and showed better robustness scores on the ACES Translation Accuracy Challenge Set, demonstrating the value of the IndicMT Eval dataset. The dataset and code are publicly available for further research.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The "marked words" method finds words that distinguish personas of "marked" groups from "unmarked" groups, enabling the identification of specific stereotypes and patterns without requiring a lexicon.</sample>
    <sample id="279">The authors of the paper are affiliated with the Paul G. Allen School at the University of Washington and the Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="280">This presentation introduces "MultiEMO," an Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations (ERC). The ERC task involves predicting the emotion label of each utterance in a dialogue, utilizing textual, audio, and visual modalities.

Existing ERC approaches often fall short in three key areas:
1.  **Exploiting Multimodal Complementarity:** Many methods primarily focus on textual modality or simply concatenate features, which is inadequate for capturing rich multimodal interactions.
2.  **Performance in Minority Emotion Classes:** Current state-of-the-art methods struggle with less frequent emotion categories due to class imbalance.
3.  **Distinguishing Semantically Similar Emotions:** Differentiating between closely related emotions remains a challenge.

To address these issues, MultiEMO proposes:
1.  **VisExtNet:** A novel visual feature extractor that effectively captures facial expressions of interlocutors across multiple frames, without encoding redundant scene information. This avoids the problems of existing methods that process unnecessary visual context.
2.  **MultiAttn:** A multimodal fusion model based on bidirectional multi-head cross-attention layers. This model integrates complementary information from textual, audio, and visual modalities, enabling successful modeling of complex correlations between them.
3.  **Sample-Weighted Focal Contrastive (SWFC) Loss:** This new loss function assigns higher importance to hard-to-classify minority classes and encourages mutual exclusivity between sample pairs with different emotion labels, thereby maximizing inter-class distances and improving the distinction of semantically similar emotions.

Experimental results on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performances. Significant improvements are observed, especially in minority and semantically similar emotion classes, highlighting the framework's ability to tackle the asynchronization of emotional tendencies from different modalities. However, limitations include VisExtNet's inability to distinguish between speakers and irrelevant people in the scene, the SWFC loss requiring large batch sizes, and minority emotion performance still lagging behind majority classes.</sample>
    <sample id="281">This presentation introduces a data-driven, multilingual exploration into when translation requires context. Traditional machine translation (MT) evaluation methods, like corpus-level metrics (BLEU), often fail to capture the nuances of context-dependent translations because only a small portion of words require context. Existing targeted evaluations are limited in the discourse phenomena and languages they support, relying heavily on domain knowledge and human curation.

To address these limitations, the authors introduce Conditional Cross-Mutual Information (CXMI) and its extension, Pointwise CXMI (P-CXMI), to quantify the context usage of MT models at both sentence and word levels. High P-CXMI values indicate words that strongly depend on context for accurate translation.

The research then performs a thematic analysis of high P-CXMI words across 14 language pairs using TED Talk transcripts, identifying five key discourse phenomena: pronouns, verb form, lexical cohesion, formality, and ellipsis. This analysis reveals that certain languages require context for specific grammatical features (e.g., dual pronouns in Arabic) or to maintain consistency in proper noun translation and formality.

Based on these findings, the authors develop the Multilingual Discourse-Aware (MuDA) tagger to automatically identify words pertaining to these phenomena. This tagger is integrated into a novel dataset-agnostic benchmark for document-level MT, allowing for targeted evaluation of models.

The evaluation results show that while corpus-level metrics offer an unclear picture of the best document-level MT systems, the MuDA benchmark highlights that context-aware models significantly outperform context-agnostic models for phenomena like formality and lexical cohesion. However, there's less improvement for ellipsis, pronouns, and verb forms, indicating areas for future progress. Commercial system comparisons, as of April 2021, demonstrate DeepL's superior accuracy over Google Translate for most phenomena and language pairs within this benchmark.</sample>
    <sample id="282">This work presents StoryTrans, a novel approach for non-parallel story author-style transfer that emphasizes discourse-level representations and content enhancement. The task of long-text style transfer, particularly at the discourse level, faces two main challenges: imitating an author's linguistic choices and addressing the high association between author styles and specific writing topics, which often leads to missing content during transfer.

StoryTrans addresses these issues by learning discourse representations from source text and combining them with learned style embeddings to generate text in the target style. The generation process is divided into two stages. First, the model transfers the source text, with style-specific keywords masked, to generate a masked transferred story. Second, it incorporates the original style-specific content to generate the complete transferred text, aiming to fill in the correct content and remove masked tokens.

The training framework for StoryTrans is also two-staged. The first stage uses an adversarial training framework with a self-reconstruction loss to recover the input, a disentanglement loss to separate style and content at the sentence-level, and a sentence order loss to capture sentence-level dependencies. The second stage utilizes a denoising autoencoder to reconstruct the original masked content, preserving content accuracy.

Evaluation on Chinese and English datasets, involving the transfer of fairy tales or encyclopedic stories to typical author styles, shows that StoryTrans outperforms strong baselines in terms of both style control and content preservation. The style visualization also indicates that the transferred texts align well with the golden texts in the style feature space, demonstrating the model's effectiveness in maintaining the main content while adopting the target style.</sample>
    <sample id="283">Conjunction-headed/Prague.</sample>
    <sample id="284">The presented paper introduces FSSUIE, a novel fuzzy span mechanism designed to enhance universal information extraction (UIE). Existing UIE models primarily rely on precise span boundaries, which often leads to ambiguity in annotation. FSSUIE addresses this by proposing a fuzzy span boundary, modeling it as a continuous distribution of correctness probabilities within a specific range. This fuzzy approach is implemented through a Fuzzy Span Loss, which combines binary cross-entropy with KL divergence between the predicted and fuzzy boundaries, providing supplementary information for better learning.

Furthermore, FSSUIE tackles the mismatch between global features extracted by Transformers and the local features crucial for information extraction. It introduces a Fuzzy Span Attention mechanism that employs a mask function to adaptively adjust the attention span. This mechanism dynamically modifies the length of the full attention range and linearly attenuates the attention distribution at span boundaries, rather than abruptly truncating it. This encourages the model to focus on semantic information within a limited range of preceding tokens, aligning better with the local nature of information extraction.

Experimental results across various information extraction tasks, including Named Entity Recognition (NER), Relation Extraction (RE), and Aspect Sentiment Triplet Extraction (ASTE), demonstrate FSSUIE's effectiveness. Compared to UIE-base, FSSUIE achieves significant performance improvements, particularly showing stronger generalization capabilities on smaller datasets and better information extraction with a simpler, unified structure. Ablation studies further confirm that Fuzzy Span Attention (FSA) accelerates convergence by guiding the model to appropriate attention distributions, while Fuzzy Span Loss (FSL) enhances information extraction capability by leveraging fuzzy boundary information, with both components synergistically boosting overall performance.</sample>
    <sample id="285">The presenter introduces "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." They acknowledge that AI-generated summaries often contain factual errors, leading to two solutions: improving summarization models for factuality or designing a factual error correction (FEC) model. The presenter highlights a flaw in current FEC evaluation, where factuality metrics provide a vague overall score and don't differentiate between error correction and new summary generation.

To address this, the presenter proposes manually annotating "reference corrections" for model-generated summaries with factual errors. These corrections aim to fix errors with minimal changes, resulting in a fluent and non-redundant summary. This approach provides more valuable data for FEC model training and enables a more accurate evaluation.

A new taxonomy of factual errors is proposed, categorizing them as "form-based" (missing, replacement, unnecessary) and "content-based" (entity, attribute, predicate errors, etc.). This taxonomy, inspired by ERRANT, forms the basis of a reference-based evaluation framework. This framework involves three steps: alignment, classification of errors based on form and content, and comparison to compute scores.

Experiments with FEC models using different training modes yielded key findings:
1. Training FEC models with reference summaries from dialogue summarization datasets yields the best results for unreliable factuality metrics, indicating an urgent need for new evaluation methods.
2. Incorporating human-corrected summaries in FEC model training for dialogue summarization improves performance, suggesting a promising direction in combining human-annotated and synthetic data.
3. Current FEC models struggle with factual error correction by addition and cannot effectively address attribute, modality, or link errors.</sample>
    <sample id="286">The speakers are James Finch and Sarah Finch.</sample>
    <sample id="287">There are four authors involved in the paper.</sample>
    <sample id="288">To test syntactic phenomena, the following datasets can be used: BLiMP, SyntaxGym, and CrowS.</sample>
    <sample id="289">Hello, my name is Kayo Yin and I will be presenting our work titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration." This work was done in collaboration with Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate "mole" in this sentence? Well, if the previous sentence was "Things could start to get dangerous if the ministers find out," then "mole" refers to a spy. But if the previous sentence was "Could it be anything serious, Doctor?" then "mole" refers to a birthmark. So depending on context, the meaning of the word changes and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages, since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions: First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to pointwise CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as words that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels: First, we look at part-of-speech tags that have high means P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns, uh to make sure that you're using the same translation within the document. And similarly, we find that context is supported to translate in the right formality. And finally, we look at different um, at individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured uh by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we call our tagger the Multilingual Discourse-Aware or MuDA tagger. We can then um, also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger by applying the tagger on the parallel corpus that we want to use for evaluation. And we apply our translation metrics of choice uh on the context-dependent examples that the MuDA tagger has identified. And finally, uh we use um our benchmark as well as other metrics to evaluate different models um on document-level machine translation. First of all, when we use corpus-level metrics, uh so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word F-measure, then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now we use the MuDA benchmark to evaluate models, and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests uh where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context. And then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.</sample>
    <sample id="290">FTw, BOND, COSINE, MLC, and L2R.</sample>
    <sample id="291">The model is evaluated on 11 downstream tasks, including named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="292">Hi, welcome to our presentation of DEplain. A new corpus for German textification on the document level and on the sentence level. My name is Regina Stodden and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model, we require parallel pairs of text, for example, of documents or sentences. In the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible, as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus DEplain. Because in the recent years, there were some problems with existing corpora. So, for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be over error prone in their alignments. Therefore, we propose our new corpus DEplain, which is split into two sub corpora, DEplain-APA and DEplain-web. DEplain-APA is based on news texts. In DEplain-APA, we aligned 483 documents all manually. It results in roughly 30,000, 13,000 parallel sentence pairs. For DEplain-web, this corpus include different domains, and we also aligned all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods. In total, we result in 3,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible text are much stronger simplified than for example, the news text, also language learner texts. On all levels, regarding for example, lexical simplification, structural simplification, also overall level of simplification. Furthermore, you can see that our DEplain corpus has a high variety of different simplification transformations. So, for example, in the DEplain-APA corpus, we have much more reordering and word additions, then we have in the DEplain-web corpus. On the other hand, in the web corpus, we have much more rephrasing. So, let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEplain. So for the first use case, uh we can evaluate uh automatic alignment methods. Uh in the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents, but they are on a different complexity levels. And now as we have our data set DEplain which have uh manually uh aligned sentences, uh we can use these sentences as gold standard alignments to evaluate some of the proposed uh alignment methods. And we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that uh the best alignment automatic alignment method to use for text for German text simplification, uh is the method of MASSAlign. And you can also find the code to uh run this method on your own documents in the paper. The second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce uh simplified text from the complex input text. We have fine-tuned two different models. Uh we have fine-tuned the model of long mBART to produce uh document-level simplifications and we also fine-tuned the normal based long uh the normal based mBART to produce sentence-level simplifications. Uh you can also find all the checkpoints and uh you can look into more details at the scores and the evaluation metrics of our experiments in the paper. Uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you uh during the conference. Thank you.</sample>
    <sample id="293">Hi. I'm going to talk about our work on resolving indirect referring expressions for entity selection, in which we introduce the AltEntities corpus. My name is Javad Hosseini, and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users' language when they make a choice. Consider this alternative question. Did you mean "Easy On Me" or "I Gotta Feeling?" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song, "Easy On Me", or its position, the first one. But sometimes, an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other, and hard to disambiguate. Or when the user wants to specify a preference. Here are some example indirect references, for example, the newer one, or the song that's not energetic. This is an important problem in conversational systems, and also for benchmarking LLMs' entity understanding. We're not aware of a public dataset, a large-scale public dataset for the task. So we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes. Our dataset collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialog context. In the In the second speech bubble, Alice says, "Do you mean "Easy On Me" or "I Gotta Feeling?"" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, the newer one. We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template: "Do you mean A or B?" Where A and B are sampled from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles. For example, two books with the name "The Return". The third one is when they have similar descriptions on Wikipedia. And finally, when they have similar infoboxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show these alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google Search link to each song. And then ask the annotators to listen to at least some of each song, and read about each song. Here's for example, the Google Search result for the song, "Easy On Me." For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, the one without words. Not the one with the 12-year-old 12-year-old boy, or the fictional one, or comes from Azerbaijan, and so on. The AltEntities corpus has 6,000 alternative questions across the three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%. So there's a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.</sample>
    <sample id="294">CamemBERT is initially trained on a 4GB French generic corpus.</sample>
    <sample id="295">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="296">This presentation introduces EPIC, a new multi-perspective irony corpus, and discusses its creation and initial findings. Modern Natural Language Understanding (NLU) relies heavily on supervised machine learning, which necessitates large, manually annotated datasets encoding human knowledge. However, the "ground truth" paradigm in subjective tasks like irony detection reveals its limitations.

EPIC (English Perspectivist Irony Corpus) aims to address this by focusing on irony from multiple perspectives. The corpus comprises approximately 3,000 short text/reply conversations collected from Reddit and Twitter between January 2020 and June 2021, spanning five English varieties (United Kingdom, United States, Ireland, Australia, India).

The annotation process involved 74 annotators (around 15 per variety), each labeling 200 text-reply pairs as "ironic" or "not ironic," with an average of five annotations per text. Annotator selection was balanced across gender, country of residence, and other demographic factors.

Analysis of inter-annotator agreement (IAA) revealed differences in irony perception across various demographic groups. While raw performance of perspective-aware models versus standard aggregated models didn't show consistent trends, perspective-aware models generally exhibited less uncertainty in their decisions and were more confident when tested on data representative of their specific perspective. Further investigation into the causes of variation in irony perception revealed that contiguous generations (e.g., boomers vs. Gen Y) and certain geographical regions (e.g., United Kingdom and Ireland) displayed the highest variations in irony perception.</sample>
    <sample id="297">The speaker introduces a project on "dogwhistles," which are terms that send one message to an out-group and a second, often inflammatory, message to an in-group, allowing for plausible deniability. Understanding dogwhistles is crucial for NLP and linguistics because their meaning is highly context-dependent, they act as mechanisms of political influence, and they enable hateful rhetoric while evading content moderation.

The project developed a typology and glossary of over 340 dogwhistles, categorized by register (informal/online, formal/offline), type (persona signal, or persona signal + added meaning), and persona (e.g., anti-Semitic, transphobic). The glossary provides rich contextual information, including explanations and real-world examples.

A case study of historical U.S. political speeches revealed that the usage of racial dogwhistles increased significantly after the Civil Rights Era, aligning with the Republican Southern Strategy, and became more associated with conservatism over time.

The project also evaluated dogwhistle recognition in language models, specifically GPT-3. GPT-3 successfully surfaced 45% of dogwhistles in the glossary, with higher performance for formal registers (69%) but lower for informal/online and transphobic dogwhistles. While GPT-3 could identify covert meanings, its performance varied, and dogwhistle definitions and "secret cues" significantly boosted its accuracy, particularly for formal language.

Finally, the study demonstrated how dogwhistles can evade content moderation. When standard group labels or slurs in hateful sentences were replaced with dogwhistles, automated toxicity detection scores from Google/Jigsaw Perspective API rated these sentences as less toxic, highlighting the challenge of detecting coded rhetoric.</sample>
    <sample id="298">The findings that led to the conclusion that temporal drift is the main cause of performance loss were that performance degrades with a larger temporal gap, and adaptive overfitting was not observed.</sample>
    <sample id="299">The video discusses the issue of "shortcut learning" in Natural Language Inference (NLI) models, where models rely on spurious correlations in training data rather than understanding true logical relationships. This leads to good performance on in-distribution data but poor generalization on out-of-distribution (OOD) and adversarial test sets.

The presentation highlights the limitations of existing shortcut mitigation techniques, such as the need for prior knowledge of shortcuts, potential instability in training due to divergent learner and auxiliary behaviors, and increased computational overhead from using large pre-trained language models as auxiliaries.

To address these issues, the speakers propose a minimax training approach. The core idea is to learn an example weight distribution that emphasizes "under-represented hard examples"—those instances that contradict the shortcuts learned from easy, dominant examples. This is achieved through a game-theoretic approach where a "learner" model tries to minimize the NLI task loss, while an "auxiliary" model tries to maximize the learner's loss by generating example weights. This incentivizes the learner to focus on challenging examples that require deeper semantic understanding.

A key advantage of this minimax training is that it does not require any prior assumptions about the types of shortcuts present in the dataset. It leverages the learner's own training dynamics to identify and up-weight hard examples. Furthermore, the auxiliary model used in this approach is a simple feed-forward network, significantly reducing computational overhead compared to methods relying on large pre-trained models.

Experimental results on three common NLI datasets (FEVER, MNLI, and QQP) demonstrate that minimax training consistently improves OOD performance while maintaining high in-distribution accuracy. The paper also explores the transferability of these improvements to larger models, synthetic shortcuts, out-of-domain test sets, the effect of pre-training, the optimal size of the auxiliary, and a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">Belinda Z. Li introduces Interactive Dictation, a novel task for users to both dictate and edit documents using voice commands in a natural and intuitive manner. Unlike traditional speech-to-text systems, which often require reserved trigger words and fixed command templates, Interactive Dictation aims for a flexible interleaving of dictation and editing, with open-ended natural language utterances. 

The task follows a four-step procedure:
1. **ASR (Automatic Speech Recognition):** Raw audio is transcribed into a speech transcript.
2. **Segmentation:** The transcript is segmented into distinct dictation and command utterances.
3. **Normalization:** Commands are extracted and normalized, fixing ASR misdetections and speech errors.
4. **Interpretation:** Each dictation and command utterance is executed sequentially to arrive at the final document state.

To support this new task, the team designed a data collection interface and built a dataset called TERTIUS. Annotators were instructed to either replicate an email verbatim, elaborate on a terse description to create a full email, or replicate the effect of a single command segment.

For the baseline system, separate models were trained for each step:
- **Segmentation Model (MSEG):** A T5 encoder trained for BIOES tagging identifies command boundaries.
- **ASR Repair Step (MNOR):** A T5 model fixes ASR and speech errors.
- **Interpretation Model (MINT):** T5 and GPT3 architectures were explored with two output types: predicting executable programs or directly predicting the next state.

Results indicate that the segmentation model is accurate and efficient. For ASR repair and interpretation models, a trade-off exists between runtime and accuracy. GPT3 models are more accurate but slower, with direct state prediction outperforming intermediate program prediction for GPT3. However, there is still significant room for improvement, and the code and data are openly available for future research.</sample>
    <sample id="301">Hi everyone, I'm Jenny, a first-year PhD student at Carnegie Mellon University, and today I'll be presenting our work, NLO Positionality: Characterizing Design Biases of Datasets and Models. This work was done in collaboration with some folks at the University of Washington and um the Allen Institute for AI, namely, Sebastian Santi, Ronan Le Bras, Katharina Reinecke, and Martin Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article, trying to remove toxic content. You might turn towards a popular API, like Perspective API for toxicity detection. And this works really well if you're Carl Jones, um where Perspective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma, where Perspective API is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results, because it can change the decisions that researchers make. And so, one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models and cells and datasets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality. However, these works really don't look at comparing end-users with the datasets and models themselves. And studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented. And it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So, to study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework, NL Positionality. Our framework works in two main steps. The first step is to re-annotate datasets with diverse annotators. And we ought to do this over looking at the demographics of original datasets, um annotators, because usually only a few instances annotate each instance, and because demographics are rarely collected and shared. And so we opt to re-annotate data to get many annotators for instance, and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score. And thus, our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels, as opposed to looking at just annotator agreement or modeling, um annotator distributions. Our framework is largely enabled through Lab in the Wild, an online crowdsourcing platform from our HCI collaborator. And Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers, um compared to like platforms like Mturk which largely have participants from the US or India. And further, Lab in the Wild still is able to get high-quality data. We host two tasks all on the Wild, one of them being social acceptability. And the way this works is that participants will read a situation from the Social Chemistry dataset, and then they'll rate how socially acceptable the situation is. Afterwards, to stay engaged in the study, they can compare their responses to an AI and others. We then compared these, um annotations with Social Chemistry, Delphi, and GPT-4. We then replicated a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and rate whether they think it's an instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate RoBERTa, and GPT-4. Our study in the end amassed over 16,000 annotations from over 1,000 annotators from 87 countries. So now we're better equipped to answer, who do NLP datasets and models align with the most? We find that there is positionality in NLP. For example, we find that datasets and models are most aligned to English-speaking countries. So for the GPT-4 social acceptability analysis, we find that it's most aligned to Confucius and English-speaking countries. We find that Dynahate is also most aligned to English-speaking countries. We also find most additional alignment with people who have a college education. So, for GPT-4 in the social acceptability task, we find that it's most aligned to people with a college education or graduate school education. And we find the same for Dynahate where it's most aligned to people with a college education. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is, keep a record of all relevant design choices made throughout the research process. And the other is to do NLP research at the lens of perspectivism. Our third recommendation is to build specialized datasets and models with and for specific communities. And a good example of this is the Masakhane Initiative. I mean, we want to emphasize that inclusive NLP isn't just making, you know, all technologies work for everyone. And so, that concludes our presentation, but if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="302">The tokens need to be permuted in order to arrange them into the correct output order.</sample>
    <sample id="303">The authors recommended increased transparency about bias mitigation methods because it is difficult to determine the underlying reasons for pernicious patterns, such as seemingly positive stereotypes, without knowing if they are a result of excessive value alignment or other anti-stereotyping methods. Transparency is needed to study these patterns further.</sample>
    <sample id="304">Minimal-pair unacceptable inputs include an ungrammatical sentence or a sentence with a stereotypical bias.</sample>
    <sample id="305">Weakly supervised learning (WSL) approaches aim to train models on noisy data labeled using weak labeling sources like heuristics or crowd-sourcing. While weak supervision alleviates the annotation bottleneck, weak labels are inherently noisy, potentially leading to noise memorization and poor generalization in neural networks. WSL algorithms are designed to counteract this noise and enable models to generalize well.

Recent WSL works commonly claim to train models solely on weakly supervised data, achieving high accuracy on clean test sets. However, this claim often overlooks the crucial role of clean validation data used for model selection. Our research investigates the necessity of clean validation data, the number of clean samples required, and how to use available clean samples more efficiently.

Our findings reveal that modern WSL methods indeed necessitate clean validation samples to function effectively, as their absence results in a significant performance drop, rendering the training process futile. This implies that the annotation cost for obtaining clean validation samples should not be disregarded. We also observe that increasing the number of clean validation samples generally improves WSL approaches. However, direct fine-tuning on clean samples can outperform WSL methods, especially with around 10 clean samples per class. Furthermore, continuous fine-tuning (CFT) on clean validation samples eliminates performance gaps between WSL approaches and a simple fine-tuned model (FTw), suggesting that complex WSL methods, which demand more computational resources, may not always be necessary.

In conclusion, recent WSL approaches heavily rely on clean samples and tend to overestimate their practicality. We recommend that future WSL research: 1) clearly reports model selection criteria, 2) uses few-shot learning baselines for comparison, and 3) always considers applying continuous fine-tuning (CFT) on clean samples. Our code is open-source for further exploration.</sample>
    <sample id="306">This presentation investigates the ability of pre-trained language models to track entities in discourses, a crucial skill for understanding complex language. The researchers identified three challenges in evaluating this ability: avoiding accidental correct predictions due to common associations in pre-training data, preventing models from relying on simple heuristics by considering the broader discourse, and avoiding memorization of entity state sequences during fine-tuning.

To address these challenges, they designed a task involving boxes and objects where models predict box contents after a series of operations. The task is structured to require models to combine initial descriptions with operations, preventing reliance on simple copying or heuristics. Experiments with Flan-T5 and GPT-3/3.5 models using two-shot in-context learning revealed that most models struggle with non-trivial entity tracking, often repeating the initial state. Only GPT-3.5 text-davinci-003 exhibited significant entity tracking capabilities, while other models performed below a random baseline.

Further analysis suggests that pre-training on code, rather than just text, is responsible for this emergent ability in GPT-3.5 models. Interestingly, smaller models like T5-base can learn entity tracking when directly fine-tuned, but randomly initialized models of the same architecture cannot. While these findings highlight the importance of pre-training data, the extent to which these entity tracking abilities generalize beyond the specific box setup remains an open question.</sample>
    <sample id="307">The authors used NER, CLS, POS, F1, Hamming, and EMR as evaluation metrics.</sample>
    <sample id="308">The speaker, Jenny, introduces "NLPositionality," a framework for characterizing design biases in NLP datasets and models, developed in collaboration with researchers from the University of Washington and the Allen Institute for AI. She highlights a "design bias" where technology performs differently for various populations, illustrating it with an example of a toxicity detection API being less sensitive to offensive terms in Indian contexts.

Jenny explains that these biases stem from the "positionality" of NLP researchers and model developers—their perspectives shaped by demographics, identity, and life experiences. She clarifies that while models and datasets don't have identities themselves, they aggregate human judgments, leading to skewed positionalities.

To address this, the NLPositionality framework involves two steps: (1) re-annotating datasets with diverse annotators and (2) comparing these annotations by demographic to existing models and datasets using Pearson's R scores. This approach differs from typical annotator disagreement studies by focusing on end-user perspectives.

The research leverages "LabintheWild," an online crowdsourcing platform, for two tasks: social acceptability and toxicity detection. For social acceptability, participants rate situations from the Social Chemistry dataset. For toxicity, they identify hate speech from the Dynahate dataset. Over 16,000 annotations were collected from more than 1,000 annotators across 87 countries.

Key findings include:
1. **Positionality exists in NLP.** Datasets and models are most aligned with English-speaking countries and individuals with a college education.
2. **Some populations are left behind.** Datasets and models show less alignment with non-binary individuals.

Jenny concludes by recommending:
1. Documenting all relevant design choices in dataset and model development.
2. Conducting NLP research through the lens of perspectivism, including sharing disaggregated dataset labels and using modeling techniques that handle annotator disagreement.
3. Building specialized datasets and models for specific communities (e.g., the Masakhane initiative) to foster inclusive NLP.</sample>
    <sample id="309">Krippendorff's Alpha was used for measuring inter-annotator agreement.</sample>
    <sample id="310">Wikipedia was chosen for completely unrelated sentences.</sample>
    <sample id="311">The authors of the paper are affiliated with Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="312">MultiInstruct is the first multimodal instruction tuning benchmark dataset.</sample>
    <sample id="313">Three authors are involved in the paper: Sarah E. Finch, James D. Finch, and Jinho D. Choi.</sample>
    <sample id="314">The video does not provide a definition of binary coordination.</sample>
    <sample id="315">The speaker does not mention the average length of the prompts.</sample>
    <sample id="316">The T5 model, when fine-tuned on Coscript, can surpass larger models in generating higher-quality scripts for constrained language planning.</sample>
    <sample id="317">The speaker introduces CodeIE, a novel approach to few-shot information extraction (IE) using large code generation models. Traditional IE methods, which transform unstructured text into structured information, often struggle due to mismatches between the input and output formats in pre-training and inference stages. This mismatch forces models to generate plain text during pre-training but structured output during inference, requiring extensive training data and specialized decoding strategies.

CodeIE addresses this by reframing IE as a struct-to-struct code generation task. This involves encoding both the input text and the desired structured output into a code-based format, making the pre-training and inference processes more aligned and controllable. For instance, in named entity recognition (NER), the input text and extracted entities are represented as Python code, allowing code large language models (Code-LLMs) like Codex to generate structured output.

Experimental results demonstrate CodeIE's superiority over traditional methods, especially in low-resource few-shot settings. The proposed approach significantly outperforms existing baselines, including T5 and GPT-3, across various NER and relation extraction benchmarks. The analysis reveals that the consistent input/output format and the inherent structural understanding of Code-LLMs contribute to higher structural fidelity and fewer errors. Furthermore, Code-LLMs exhibit better recall, suggesting their ability to identify more relevant information despite the format constraints. These findings highlight the potential of Code-LLMs for complex IE tasks, providing new insights and directions for future research in natural language processing.</sample>
    <sample id="319">The work investigates "from scratch" learning with full model construction and "continual pre-training" using an existing pre-trained model (specifically CamemBERT, a French generic model, and PudMedBERT, an English-based model).</sample>
    <sample id="320">The presentation explicitly states that no diminishing returns were observed, meaning adaptive overfitting (due to test set reuse) was not observed.</sample>
    <sample id="321">The simplification quality was evaluated using two different models: long-mBART for document-level simplifications and normal base mBART for sentence-level simplifications. The results of these fine-tunings were compared to baseline scores to assess their effectiveness.</sample>
    <sample id="322">Enrico Liscio will present a paper at ACL 2023 on the topic "What does a Text Classifier Learn about Morality?". Human morality is essential to our societies, and it's crucial for language models to comprehend and recognize morality within text.

Traditionally, morality in NLP has been treated as a singular scale between "immoral" and "moral." However, morality is subjective, and different individuals might label the same concept differently on this scale. The speaker cites the examples of abortion and LGBTQ rights to illustrate how a simple average or majority aggregation can obscure the true pluralistic nature of moral interpretations.

To address this, the research utilizes the Moral Foundation Theory, which suggests five distinct ways humans perceive morality: Care, Fairness, Loyalty, Authority, and Purity. Each concept or action "tickles" a different moral foundation, and individuals prioritize these foundations differently.

The paper aims to understand how morality classifiers learn, specifically focusing on how morality is expressed differently across various domains. The researchers employed explainable AI techniques on language models trained on the Moral Foundation Twitter Corpus, which comprises 35,000 tweets from seven distinct domains.

One of the key findings, highlighted by the speaker, reveals the difference in how the language models interpret "subversion" (rebellion to authority) in the "All Lives Matter" (ALM) and "Black Lives Matter" (BLM) domains. While both domains share similar rhetoric, the ALM domain associates subversion with negative terms like "overthrow" and "mayhem," implying it's frowned upon. In contrast, the BLM domain associates subversion with terms like "encourage" and "defiance," suggesting it's encouraged.

This finding underscores the importance of recognizing that morality is expressed differently across domains and warns against using a single model for diverse contexts to avoid misinterpretations of morality.</sample>
    <sample id="323">The video presents a paper on Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering (QA). The speaker, Yujie Wang from Shanxi University, China, explains that Commonsense QA is a challenging task requiring machines to answer questions using common knowledge and retrieving relevant knowledge from external sources. The knowledge is stored in language models and knowledge bases.

The presented method, DHLK, addresses issues in previous approaches, such as noisy entities in retrieved subgraphs and limited interaction between modalities due to isolated encoding. DHLK proposes building a heterogeneous knowledge graph (HKG) from multiple knowledge bases. This HKG is then optimized for structure and knowledge representation using a two-stage pruning strategy and knowledge representation learning (KRL). Finally, a language model is used for fusion and encoding of the two modalities.

The HKG construction involves first-stage pruning to remove subwords and retrieve paraphrases of key entities. The LM Encoder and Dynamic Pruning Module uses RoBERTa and Mask Self-Attention to encode and fuse QA context and subgraph entities. Dynamic pruning removes entities with weak relevance based on attention weights. The KRL Module uses TransE to optimize entity and relationship embeddings in the HKG. The KG2QA Layer incorporates path information from HKG into the QA context. Answer prediction is done by an MLP.

The experiments conducted on CommonsenseQA and OpenBookQA datasets, using ConceptNet, WordNet, and Wiktionary as knowledge sources, show that the DHLK method achieves good results compared to other language model and HKG methods.</sample>
    <sample id="324">Yes, language models do have different political leanings.</sample>
    <sample id="326">Cognitive dissonance is when two elements of cognition, such as thoughts, actions, or beliefs, are inconsistent.</sample>
    <sample id="327">This presentation introduces ManagerTower, a novel vision-language model architecture designed to enhance the integration of multimodal data for tasks like visual question answering and image retrieval. While existing approaches, such as the Two-Tower and BridgeTower architectures, have made significant progress by connecting uni-modal and cross-modal encoders, they suffer from limitations in effectively utilizing deep uni-modal representations.

BridgeTower, for instance, links multiple top uni-modal layers to cross-modal layers in a layer-by-layer fashion, but its utilization of semantic knowledge from different uni-modal layers is ineffective and its scalability is limited by the fixed number of cross-modal layers.

ManagerTower addresses these limitations by introducing "managers" within each cross-modal layer. These managers adaptively aggregate multi-layer uni-modal representations, treating them as insights from pre-trained uni-modal experts at different levels. This allows for a more comprehensive cross-modal alignment and fusion, leading to significant performance improvements.

Evaluated on various downstream tasks with only 4 million image-text pairs for pre-training, ManagerTower outperforms BridgeTower and other base-size models, even surpassing some models trained with substantially more data or parameters. Notably, ManagerTower achieves 79.15% accuracy on the VQA v2 test standard. Visualization of aggregation weights further demonstrates that adaptive managers exhibit diverse weight distributions, indicating their ability to dynamically exploit different levels of uni-modal semantic knowledge for better representation learning. The code and models are available on arXiv and GitHub.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">This paper presents a novel approach for zero-shot video sentence localization, addressing the challenges of manual annotation, simple pseudo-queries, misaligned pseudo-events, and noisy pseudo-labels in existing methods. The proposed method, Structured Pseudo-Label (SPL) generation, aims to generate robust pseudo-labels for noise-resistant training.

The approach involves two main steps:
1. **Pseudo Query Generation:** Instead of simple noun-verb combinations, the method uses a pre-trained image captioning model (BLIP) to generate more complex and free-form pseudo-queries from densely sampled video frames. This captures richer semantic information, reducing the gap between generated and real queries.
2. **Pseudo Event Generation:** To address misalignment and noise, pseudo-events are generated based on the video's event temporal structure. Similarity scores between video frames and pseudo-queries are calculated, and event quality is defined as the difference between mean similarity within and outside an event. High-quality pseudo-query-event pairs are selected, and overlapping low-quality pairs are eliminated using non-maximum suppression, ensuring better alignment and relevance.

Finally, during training, the influence of noisy pseudo-labels is reduced through two strategies:
1. **Sample Re-weighting:** Noise is estimated based on the model's prediction confidence and the Intersection over Union (IoU) of predictions with pseudo-labels. Samples with higher noise probability are down-weighted, ensuring the model focuses on more reliable pseudo-labels.
2. **Label Refinement:** If the model's prediction confidence and IoU with a pseudo-label are high, the prediction is treated as a new, refined pseudo-label for subsequent training rounds.

Experiments on ActivityNet Captions and Charades-STA datasets demonstrate that the proposed SPL method outperforms existing zero-shot methods, achieving state-of-the-art performance on most metrics. This highlights the effectiveness of generating structured and noise-robust pseudo-labels for improving zero-shot video sentence localization.</sample>
    <sample id="330">Yes, cumulative training performed equally or better than iterative training.</sample>
    <sample id="331">The speaker's name is Sara Papi.</sample>
    <sample id="332">The data for the MuDA benchmark was taken from transcripts of TED Talks.</sample>
    <sample id="333">This presentation introduces INK, a novel framework for injecting kNN knowledge into Nearest Neighbor Machine Translation (NMT) to improve its generalization ability and performance, especially when dealing with unseen domains. NMT models often exhibit non-smooth representation spaces where low-frequency tokens are sparsely distributed, leading to "semantic holes" and poor performance.

Traditional kNN-MT addresses this by smoothing predictions with nearest neighbors from a datastore. However, this approach has drawbacks: retrieving neighbors from a large datastore is time-consuming, and representations are not easily updated once the datastore is constructed.

INK overcomes these limitations with a two-step training loop:
1. **Representation Refinement:** kNN knowledge is extracted from the datastore to guide a small adapter in adjusting the NMT model's representations. This involves aligning contextualized representations with token embeddings to preserve and enrich semantic meaning, and also aligning representations of the same target token to address sparsity.
2. **Asynchronous Refresh:** Updated representations are used to asynchronously refresh the datastore. This loop runs until convergence.

During inference, INK only requires the off-the-shelf NMT model and the tuned adapter, allowing the datastore to be dropped, thus speeding up inference and reducing memory.

Experiments conducted on four benchmark datasets (Medical, Law, IT, Koran) using a WMT'19 news translation model show that INK significantly outperforms state-of-the-art kNN-MT systems. It achieves an average gain of 1.99 COMET and 1.0 BLEU score, with a 0.02x memory space footprint and a 1.9x inference speed-up compared to kNN-MT baselines. The results also indicate that jointly applying the adapter and datastore can further smooth predictions, suggesting additional potential for refinement.</sample>
    <sample id="334">Hi, my name is Adam. Przeplkowski and this talk is about the dependency structure of coordination. As you may know, there are different dependency structures assumed by different theories and and corpus approaches. So, for example, in Universal Dependencies, the structure of the coordinate coordination "Lisa, Bart, and Maggie" is such that the first conjunct is the head of the whole coordinate structure, so in this case, "Lisa". Uh, a similar approach is assumed in uh Igor Melchuk's uh meaning text theory, where again, uh the whole coordinate structure is headed by the first conjunct. So, these two approaches are asymmetric, right? They uh, they single out one of the conjuncts. Now, there are also symmetric approaches to coordinate coordinate structures, such as the Prague approach, the conjunction-headed approach, assumed in Prague Dependency Tree Banks, where coordinate structures are headed by the conjunction. So, uh, we get um um dependencies from "and" to all the conjuncts. And finally, there's also a multi-headed approach, uh, that's uh used, for example, in um, uh the Cartsons's uh Word Grammar, where, so to say, all conjuncts are heads of the coordinate structures. So, we get dependencies from the governor, here "loves", to all conjuncts separately, "Lisa, Bart, and Maggie". Now, uh, the aim of this paper is to um, produce a novel argument for uh the symmetric structures of coordination, like these two, and against the asymmetric structures of coordination, like these two. Okay. Uh, so what we did, we extracted various statistics from uh about coordination from the enhanced version of the Penn Treebank, and see the paper why we didn't use uh Universal Dependencies. And uh these statistics confirm the observation made many times before that left conjuncts tend to be shorter. Uh so, "salt and pepper" and not "pepper and salt", measured in syllables. Uh and uh also the observation that was made in passing that this tendency grows with length difference. So, when the difference between the lengths of the two conjuncts are grows, uh the shorter conjunct prefers to be the first one, stronger, right? So, the proportion is is is bigger of of the left uh short conjunct. However, when uh the governor is on the right, as here, "laughed" governed the coordination "Ted and Ned", uh this effect disappears. So, we show that um, uh by measuring length in characters, that's the first column, in syllables, the middle column, and in words, the right column. So, I'll concentrate on the right one. What we see here is that uh, when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily uh, with the absolute difference in words. And the same is observed when there is no governor, as in coordination of sentences, but when the governor is on the right, this tendency disappears. And uh, we show in the paper how this um, uh provides an argument against uh asymmetric structures of coordination, as these two, and for the symmetric structures as these two. So, see the paper for the full argument, and uh, arguments, sorry, and talk to us about uh, the poster session. Thank you.</sample>
    <sample id="335">The speaker's name is Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer is a setting where a model is trained on one source language and then transferred to another language for inference.</sample>
    <sample id="337">This presentation introduces a novel approach called Graph-based Relation Mining (GRM) for context-free Out-Of-Vocabulary (OOV) word embedding learning, drawing inspiration from human study habits. The core idea is to leverage the lexical rules of word formation and association, which are critical for understanding new words.

The GRM model constructs a two-level Word Relationship Graph (WRG). When an OOV word appears, it's tokenized into word pieces, which form the first layer of the graph. These word pieces are then associated with other relevant words, forming the second layer. Each word or word piece is a node in the graph, with its embedding serving as the node feature.

To address the challenge of assigning node attributes to OOV words, the model uses a self-attention network based on the characters of the OOV words for initialization. To process the WRG, a two-level Graph Attention Network (GAT) is employed, followed by concatenation and fusion of initial input with hidden embeddings from each layer to create a node-level representation.

To capture holistic graph information and summarize word formation, a readout block using a Graph Convolutional Network (GCN) generates a graph-level representation. The model then applies contrastive learning, pulling together positive samples (two-hop neighbor words, synonyms, or the OOV word itself) and pushing away other samples in the batch, to encourage proximity between related embeddings.

Extensive experiments demonstrate GRM's superior performance in both intrinsic (word similarity, word analogy) and extrinsic (named entity recognition, POS tagging) tasks, outperforming state-of-the-art baselines. This proves the effectiveness of learning OOV words through their word formation. The model also shows adaptability to both static and contextual models in downstream tasks, enhancing their performance.

GRM's graph structure is capable of handling complex word formations, suggesting its potential applicability to other languages, particularly agglutinative languages where word formation is direct. Even for fusional languages like English, the model performs well with reasonable word segmentation. The effectiveness of GRM in other languages would primarily depend on the rationality of word decomposition.</sample>
    <sample id="338">The presenter discusses their research on evaluating human-annotated natural language explanations. They begin by highlighting the motivations behind using human explanations, such as training Natural Language Generation (NLG) models, boosting prediction performance, and enhancing model reasoning. However, they acknowledge the challenge of evaluating human explanations, as they can be subjective and task-dependent, unlike objective labels.

The presentation then delves into related work, mentioning popular NLG metrics like BLEU and ROUGE, which treat human annotations as the gold standard, and the Simulatability Score, which measures baseline model performance but neglects task differences and the utility of explanations during fine-tuning.

To address these limitations, the researchers introduce a unified data structure that converts various tasks into a multiple-choice format, suitable for sequence-to-sequence models. They conduct preliminary experiments on ECQA and CoS-E datasets, revealing that fine-tuning with explanations teaches models to rely on the explanation part of the input, and that explanations from CoS-E are less helpful than those from ECQA on baseline models, aligning with previous work. Notably, fine-tuning with a small amount of explanatory data can lead to significant improvements.

Based on these observations, they propose a novel evaluation metric called TREU, which extends the Simulatability Score by also evaluating the helpfulness of explanations during fine-tuning. Their evaluation of TREU and Simulatability on five datasets and two models (T5 and BART) demonstrates that TREU can faithfully reflect the helpfulness of explanations, even when humans deem them low quality. Furthermore, TREU consistently ranks dataset qualities across different models, unlike Simulatability, which can be more affected by model variations. The researchers speculate that the helpfulness of explanations depends on the task and explanation style.

In conclusion, their work lays a foundation for high-quality Human-AI collaboration in annotation tasks, recommending similar quality checks for future human explanation collection.</sample>
    <sample id="339">The authors are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="340">This presentation introduces ParaAMR, a large-scale, syntactically diverse paraphrase dataset created using AMR (Abstract Meaning Representations) back-translation. The challenge in paraphrase generation is acquiring datasets that are both large-scale and high-quality, with existing human-annotated datasets being high-quality but limited in scale, and automatically generated datasets (like those from back-translation) being large-scale but lacking syntactic diversity.

ParaAMR addresses this by leveraging AMR graphs, which capture the abstract meaning of a sentence. The methodology involves using an AMR parser to obtain the AMR graph of a source sentence, then changing the "focus" (root node) of the graph to generate different semantic structures while maintaining overall meaning. These modified graphs are then converted back into text using an AMR-to-text generator, resulting in syntactically diverse paraphrases.

The dataset comprises approximately 15.5 million source sentences, with an average of 6.92 paraphrases per sentence. Quantitative analysis, including automatic and human evaluation scores, demonstrates that ParaAMR achieves similar semantic similarity to existing back-translation datasets but boasts significantly higher syntactic diversity. This balance of semantic fidelity and syntactic variation makes ParaAMR a valuable resource.

ParaAMR's benefits are showcased through three applications:
1.  **Learning Sentence Embeddings:** Sentence embeddings learned using ParaAMR perform better on Semantic Textual Similarity (STS) benchmarks compared to those trained on other datasets.
2.  **Syntactically Controlled Paraphrase Generation:** Models trained with ParaAMR exhibit improved syntactic control in paraphrase generation.
3.  **Data Augmentation for Few-Shot Learning:** ParaAMR's diverse paraphrases enhance performance in few-shot learning tasks (MRPC, QQP, RTE) when used for data augmentation.

ParaAMR is an open-source dataset, available for download at the provided GitHub link, contributing a novel approach to generating diverse and high-quality paraphrases for various NLP applications.</sample>
    <sample id="341">The authors use the average lagging latency measure and the computationally aware average lagging latency measure.</sample>
    <sample id="342">The presenter introduces LiveChat, a large-scale, personalized dialogue dataset constructed from live streaming. Traditional open-domain dialogue systems rely on text-sourced or manually extracted video-sourced data, both of which have limitations. Text-sourced data lack the natural flow of spoken conversation, while manual extraction limits the scale of video-sourced datasets. Furthermore, existing personalized dialogue research faces challenges with insufficient persona information and limited conversation length. Multi-party conversations, common in live streaming, also lack large-scale Chinese datasets.

LiveChat addresses these issues by providing a video-sourced, personalized, and multi-party Chinese dialogue dataset. The construction process involves three steps: first, extracting and transcribing utterances from Douyin live streaming videos; second, collecting audience comments and constructing dialogues by matching streamer responses and audience comments; and third, collecting persona information and adding manual annotations. Persona extraction includes both basic profiles, gathered through manual labeling and scraping, and text profiles, extracted using rules and a trained persona classifier.

Experimental results on two benchmark tasks, Response Modeling and Addressee Recognition, demonstrate the advantages of LiveChat. The extracted persona information and longer average sessions per persona significantly improve performance in response modeling. For addressee recognition, single-stream models outperform double-stream models. Additionally, transfer learning experiments with pre-trained generative models like BART, GLM, and GPT3 show that BART performs best, confirming the distinctiveness of LiveChat's video-sourced dialogue domain. In-context learning experiments also reveal that performance generally improves with more demonstrations, but too many randomly selected demonstrations can introduce noise and slightly decrease performance. Future work will focus on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="344">Tree-based methods are not always given, and thus are computationally expensive to obtain. This includes considerable formalism-specific pre-processing and grammar induction procedures.</sample>
    <sample id="345">Matthias Lindemann introduces their paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations." They address the challenge of compositional generalization in semantic parsing, where models need to handle deeper recursion and unseen compositions of phrases seen individually during training. Traditional sequence-to-sequence models often struggle with this out-of-distribution generalization, producing outputs detached from the input and failing to capture systematic correspondences.

A common approach to improve generalization is to incorporate trees into the model, which help capture the compositional process linking utterances to logical forms. However, obtaining these trees often requires complex pre-processing or grammar induction, adding computational overhead and requiring specialized formalisms.

Lindemann's paper proposes a novel neural sequence-to-sequence model that bypasses the need for explicit trees. Their approach consists of two steps:
1. **Multiset Tagging:** Each input token is tagged with an unordered multiset of tokens that will appear in the output logical form.
2. **Latent Permutations:** A separate model predicts a permutation to arrange these multiset tokens into the correct output order.

This two-step process allows for direct modeling of correspondences between input and output fragments, offering flexibility and expressiveness. Key technical challenges include the unknown alignment between input and output during training and the NP-hard nature of inferring the highest-scoring permutation. They address these by inducing alignments during training and approximating permutation inference with a GPU-friendly continuous relaxation that allows for backpropagation. Experimental results on the COGS benchmark show significant improvements over other treeless models in generalizing to deeper recursion.</sample>
    <sample id="346">Shuheng Liu and Alan Ritter are affiliated with the School of Interactive Computing, Georgia Institute of Technology.</sample>
    <sample id="348">Myra Cheng presents "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," a paper co-authored with Esin Durmus and Dan Jurafsky for ACL 2023. The paper addresses limitations in existing stereotype measures in Large Language Models (LLMs), such as a trade-off between specificity and generalizability, reliance on fixed datasets, and a lack of accounting for intersectionality.

To overcome these, the team leveraged instruction-tuned LLMs like GPT-3.5 and GPT-4. They prompted models to "Imagine you are an Asian woman. Describe yourself." and found the method to be generalizable to any intersectional identity.

The research's two steps are: 1) Generate personas, inspired by a psychology study where human subjects received similar prompts, allowing for direct comparison. 2) Employ "Marked Words" to find distinguishing terms for marked groups (e.g., Black women) compared to unmarked groups (e.g., White people, men), providing specific insights without requiring a lexicon.

Results show that generated personas contain more stereotypes than human-written ones. While generated personas exhibit a high frequency of lexicon words, human-written ones display a wider distribution. The lexicon, however, is incomplete, missing many harmful patterns. Marked Words analysis revealed othering through essentializing narratives (e.g., culture, tradition, exotic for marked groups) and pernicious positive portrayals (e.g., vibrant, curvaceous for Latina women; petite, delicate, silky for Asian women; strong, resilient for Black women). These positive portrayals contribute to harmful archetypes by defining groups solely by their identity and placing undue pressure on them.

Recommendations include addressing positive stereotypes and essentializing narratives, adopting an intersectional lens for bias studies, and promoting transparency in bias mitigation efforts.</sample>
    <sample id="350">The presentation, "What's The Meaning of Superhuman Performance in Today's NLU?", explores the current state of Natural Language Understanding (NLU) and the perceived "superhuman" performance of AI models on benchmarks. Leaderboard-based evaluations have become standard, with models frequently outperforming human baselines. This has led to claims that certain NLU tasks are "solved," despite known brittleness in models regarding out-of-domain generalization, adversarial attacks, spurious patterns, and sensitivity to linguistic perturbations.

The authors investigated two popular benchmarks, SuperGLUE and SQuAD, where models reportedly surpass human performance. However, they uncovered several issues that make human-to-system comparisons unreliable. Firstly, systems and humans are often evaluated on different test sets, with humans typically assessed on much smaller subsets. Secondly, numerous errors were found in the ground-truth annotations of the datasets. Thirdly, human performance is often vaguely estimated, with simple aggregation methods used instead of identifying the "best possible human" performance. Finally, heterogeneous and often unknown low pay rates for human annotators suggest low motivation and, consequently, low-quality annotations, further compromising the validity of human baselines.

These findings highlight that claims of superhuman performance are not yet scientifically grounded. The paper discusses the consequences of these identified issues and offers recommendations for constructing fairer and more transparent benchmarks in the future.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models, specifically focusing on those trained using the CoNLL-2003 dataset. With models having used CoNLL-2003 for almost two decades, the authors question their ability to generalize to modern data and identify factors contributing to good generalization or performance drops.

To address this, they created the CoNLL++ dataset by annotating Reuters news from 2020 using the same CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and then evaluated on both the original CoNLL-2003 test set and the new CoNLL++ set. Generalization was assessed by calculating the percentage change in F1 score.

The study found three main ingredients for good generalization:
1. **Model architecture:** Transformer models consistently generalized better.
2. **Model size:** Larger models exhibited improved generalization.
3. **Number of fine-tuning examples:** More fine-tuning data led to better generalization.

Regarding performance drops, two hypotheses were considered: adaptive overfitting (overfitting due to repeated use of the same test set) and temporal drift (performance degradation due to the increasing time gap between training and testing data). The results indicated no evidence of diminishing returns for adaptive overfitting, suggesting it was not a significant factor. However, performance degraded with a larger temporal gap between the training (CoNLL-2003) and test (CoNLL++) datasets, confirming that temporal drift is the main cause of the observed performance drop.

Despite the temporal drift, the conclusion is that CoNLL-2003 taggers can still work well in 2023, especially with better model architecture, larger model sizes, and more fine-tuning examples. The paper calls for more research into improving model generalization.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">In this paper, a novel approach to Python code generation is introduced by integrating interactivity into the process, specifically through clarification questions (CQs). This method aims to address the common challenge of input underspecification in natural language descriptions for code generation.

The proposed pipeline involves three key modules: a clarification need predictor (binary classifier), a question selector (retrieval model), and a code generator (CausalLM or seq2seq generator). To train these models, a synthetic dataset called CodeClarQA was created, which includes clarifications for key operations identified through a heuristic-based method using the GraphGen4Code API. This method extracts key operations from the code, represents them in a latent space, and calculates similarity scores between the natural language description (NLD) and the operation's documentation. Operations are deemed "missing" if their similarity score falls below a threshold, and "aligned" otherwise. Annotators were hired to validate the threshold and evaluate the overall approach. Templates are then used to generate Yes/No or Multiple-Choice CQs for these missing operations.

Experimental results show that the MPNet model outperforms others in identifying missing key operations. The module experiment results suggest that the task is more challenging than existing CQ ranking tasks, but clarifications significantly aid code generation. While the pipeline still underperforms models trained solely on NLDs and code, this is attributed to the fine-tuning on CQs and the challenging nature of CQ ranking. Error analysis reveals that false-positive predictions are rare, indicating the effectiveness of generating CQs for missing key operations. Common errors highlight areas for improvement, such as distinguishing aligned operations with similar names and better utilizing operation documentation when argument values are missing. The study concludes that clarified key operations are indeed a reason for better generated code.</sample>
    <sample id="354">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2018.</sample>
    <sample id="356">The authors are affiliated with the University of Edinburgh, Saarland University, and the University of Amsterdam.</sample>
    <sample id="357">The speaker's name is Siyu Yuan.</sample>
    <sample id="358">There are 5 authors involved in the paper.</sample>
    <sample id="359">The approach is compared to CAAT, a state-of-the-art architecture specifically tailored for SimulST.</sample>
    <sample id="360">Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language-only tasks, while computer vision and multimodal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multimodal pretrained models can actually improve generalization to unseen multimodal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between NLP and multimodal. There exists more than 1,600 language-only instruction tasks. However, there is no large scale publicly available multimodal instruction task. Therefore, this motivated us to build a multimodal instruction tuning dataset. Here we present MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset, and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed datasets, we take OFA, a unified multi-modal pretrained model as our base model. OFA use a unified vocabulary for language, image tokens and the coordinate of a bounding box. Here we show some example instances from our MultiInstruct dataset. To unify the processing of a various input and output data type, we follow the method from OFA and formulate all the tasks in a unified sequence to sequence format, in which the input text, images, instruction and bounding boxes are represented in the same token space. Okay, now I'm going to talk about multimodal instruction tuning. So for the training dataset, we use 53 task from nine group for training, and we sample 10,000 instance per task. Uh, for testing, we reserved the entire common sense reasoning group for testing and we select additional five task from VQA and miscellaneous group. We use all the instance in the test split for each task. Uh, in addition, we randomly sample 20 task from the test split of natural instruction dataset as unseen task for NLP. Uh, so we use uh pretrained OFA large model as the base model. Uh, during training, we mix all the instance for all the tasks. Uh, each instance is randomly combined with one of its five instruction template. Uh, so during test for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions uh in each experiment. We report the mean and max performance and the standard deviation of the performance across all five experiments. Uh, if the task is a multimodal classification task, we report accuracy. If it's a multimodal generation task, we report Rouge-L. Uh, for NLP task, we report Rouge-L as well. We also introduced uh additional uh evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task, regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, uh instruction tuning can significantly improve OFA's performance on all unseen multimodal tasks. Uh also transfer learning from natural instruction dataset can benefit uh instruction tuning. Uh, here we can see as the amount of task increase, the model achieve better performance and in the mean time, uh lower sensitivity. Uh, so we also did one experiments, we use one instruction versus five instruction. As we can see, uh using more instruction can improve the model's overall performance and reduce its sensitivity a lot. Uh, so this shows the effect of different fine tuning strategy on the model sensitivity. Uh, as we can see, by transfer learning from natural instruction dataset, the model can uh achieve much better sensitivity compared to the original OFA model. Uh, we also can see transfer learning from natural instruction dataset, uh, can help OFA to achieve much better performance on the natural instruct uh dataset. So overall, we are proposed the first large scale multimodal instruction tuning dataset. We significantly improved the zero-shot capability of OFA, and we explored different transfer learning technique and showed their benefits. Uh, we designed a new metric called sensitivity. Uh so one more thing, we are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. Uh, this is a QR code for our uh data and the model. Thank you.</sample>
    <sample id="361">The speaker, Armineh Nourbakhsh, a PhD student at Carnegie Mellon University, is presenting on CounterComp, a project focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning in question-answering tasks. She highlights the challenge with state-of-the-art neural models that tend to memorize spurious patterns, particularly when outputs involve more than two steps. This is problematic because models may mistakenly associate repeated input tokens with common arithmetic operations in the output, such as a subtraction operation.

The core idea of CounterComp is to exploit the interchangeable nature of certain question components (e.g., changing "net change" to "percent change" affects the output operations from subtraction to division and multiplication). By treating training samples as anchors, the team mines positive and negative counterfactual examples. A positive example is where an intervention in the question doesn't change the output, while a negative example results in an output change. These triplets are used to add an auxiliary metric learning loss with a dynamic margin to the training procedure.

The results show that integrating this auxiliary loss into three state-of-the-art baselines consistently improves performance, especially for questions requiring more than two reasoning steps. This improvement is observed on in-distribution samples (trained and tested on the same dataset) and, more importantly, on out-of-distribution samples, including unseen programs. Qualitatively, CounterComp helps the model attend to more meaningful tokens during the generation of operations, indicating better generalization beyond superficial patterns.</sample>
  </task>
</testset>