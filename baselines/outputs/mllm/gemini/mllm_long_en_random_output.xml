<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="211">The results and dataset in the paper can be used as a benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="212">The speaker mentions experiments with T5, but the slide only references T5 and T5 with Coscript, implying a focus on one smaller model fine-tuned on different datasets.</sample>
    <sample id="213">The OFA (One For All) large model, a unified multi-modal pre-trained model, is used as the base model for investigating multi-modal instruction tuning.</sample>
    <sample id="214">Hello, everyone. My name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper, "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding as a Service via Backdoor Watermark." Let's first introduce the background about embedding as a service. Currently, Large Language Models (LLMs) such as GPT, Llama, and PaLM are exceptional in Natural Language Understanding (NLU) and Natural Language Generation (NLG). Embedding as a Service (EaaS) is one of the services built upon Large Language Models to assist various NLP tasks. For example, OpenAI offers a GPT3-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embeddings and provide similar services. Therefore, it's necessary to protect the copyright of Embedding as a Service. To protect the copyright of Embedding as a Service, one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to Embedding as a Service. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker, or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, these methods are either not applicable to Embedding as a Service or lack of transferability. Therefore, in this paper, we propose EmbMarker, which is a backdoor-based watermark method applicable to Embedding as a Service. Then, let me introduce the details of our EmbMarker. EmbMarker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate-frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and a benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set, while all words in the sentences of benign datasets do not belong to the trigger set. Then, the provider requests embeddings from stealer's service with the datasets. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AG News, MIND, SST2, and Enron Spam. We assume the provider apply WikiText dataset to count word frequency. The results on four datasets show that our EmbMarker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on four datasets via PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.</sample>
    <sample id="215">Adam Przepiórkowski presents a novel argument for symmetric dependency structures in coordination, contrasting them with asymmetric approaches like Universal Dependencies. The argument is based on Dependency Length Minimization (DLM), which postulates that word order tends to minimize dependency lengths, especially for direct objects. While short direct objects prefer to be close to the verb, long direct objects can be moved after an adjunct to reduce the sum of dependency lengths for both the direct object and the adjunct.

Przepiórkowski and Michał Woźniak’s research analyzed coordination statistics from an enhanced Penn Treebank. Their findings confirm previous observations that left conjuncts tend to be shorter, and this tendency increases with the length difference between conjuncts. However, their novel finding is that this tendency only holds true when the governor of the coordination is on the left or absent. If the governor is on the right, this effect disappears.

They show that this empirical finding, stemming from DLM, provides an argument against asymmetric dependency structures (such as those where the first conjunct is the head) and in favor of symmetric structures (where either the conjunction is the head or all conjuncts are heads). The asymmetric approaches predict the preference for a shorter left conjunct regardless of the governor's position, which contradicts the observed data. In contrast, symmetric approaches align with the observed dependency on the governor's position. This suggests that symmetric dependency structures are a more accurate representation of coordination in English.</sample>
    <sample id="216">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler. And I will briefly introduce the attention as a guide for simultaneous speech translation paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation or SimulST is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized, long and complicated training procedures, for example training involving different optimization objectives, and training and maintaining several models to reach different latency regimes, for example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output, that is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention. And it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to. A word is emitted if the attention is not concentrated, that is its sum is below a threshold Alpha, towards the last Lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing "I'm going to talk about", and our model predicts the translation in German. And we will look at the cross-attention weights, we'll see that the first two words point to the earliest received speech frames, while the last word points to the last received speech frames, and the last Lambda speech frames. This means that the first two words will be emitted, while since the sum of the cross-attention is above a certain threshold Alpha, we will not emit the last word. And we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at the cross-attention weights, we will see that no words point to the last Lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we'll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measure the translation quality, and average lagging that is, the latency measure, and we also considered the computationally aware average lagging that accounts for the models computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models, that are the Wait-k strategy and the Local Agreement. And we compare also with the state of the art architecture specifically tailored for simultaneous speech translation. These are all the results of the simultaneous speech translation strategy on German. And we see that EDAtt outperforms all the strategies applied to offline models since their curves are shifted towards the left. And we also see that if we consider the actual elapsed time or the computationally aware time, EDAtt is the fastest strategy. If you want to discover more results, read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="217">Weihao Zeng presents "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation." He highlights that previous controllable dialogue generation (CDG) methods focused on single attributes, overlooking the practical need for multi-attribute generation, and that their evaluation is limited by annotated data.

Zeng's team explores compositional generalization for multi-attribute CDG, finding that existing models lack generalization capability. They propose DCG, a disentangled controllable generation model that learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations. They also introduce MAE, a unified reference-free evaluation framework for multi-attribute generation.

Their methodology involves attribute-oriented and task-oriented prompts within a DilaGPT framework, along with disentanglement learning to enhance the model's generation ability and differentiate between attribute value combinations.

Experimental results on DailyDialog-CG demonstrate that DCG outperforms other baselines in both attribute controllability and text quality for new attribute combinations. The study also shows that attribute-oriented prompts guide the model to focus on controllable information, while task-oriented prompts improve text quality. Disentanglement learning enhances compositional generalization.

Qualitative analysis reveals that DCG successfully disentangles changes in compositional generalization with only a small drop in performance compared to human judgments. The MAE metric also shows higher correlation with human judgments compared to classical metrics. Prompt visualizations further confirm that DCG can disentangle attribute combinations and learn relationships between different attributes, facilitating generalization from seen to unseen combinations.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The speaker, Jia-Hui Ju, presents a paper titled "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." He introduces the Form 10-K, an annual report required by the SEC, as the target corpus for their research. This document is highly informative but requires significant human effort to extract useful signals.

The work is motivated by two key observations: first, financial reports exhibit high overlapping characteristics, with about 80% of tokens being the same across a company's reports; second, the content is yearly-dependent, with adjacent years showing more similarity than distant ones.

Based on these characteristics, they introduce a "highlighting task" and propose a multistage pipeline. The highlighting task aims to identify rationale/important words by comparing and contrasting contexts of given sentence pairs. The words with high importance are regarded as financial signals. For instance, the word "decreased" would have high importance in a sentence comparing sales figures from one year to the next.

The proposed pipeline includes document segmentation, relation recognition, and two stages of fine-tuning: out-of-domain and in-domain. In the relation recognition stage, they classify reference-to-target segment pairs into three types: insignificant (uninformative), revised (differ in few words but disclose different meanings, e.g., increase vs. decrease), and mismatched (mutually exclusive meaning, e.g., new policies).

For the fine-tuning stages, they use an external dataset, e-SNLIc, for out-of-domain fine-tuning and revised pairs with pseudo-labels for in-domain fine-tuning. They employ soft labeling techniques by mixing cross-entropy loss and KL divergence to mitigate the problem of low-quality pseudo-labels.

Their domain-adaptive highlighting model achieves the best performance on the FINAL dataset and preserves generalization capability on e-SNLIc, also demonstrating benefits for unseen relations. Future work includes improving effectiveness, adding more features, and exploring end-to-end applications.</sample>
    <sample id="220">The authors of the paper are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed translations from German into English.</sample>
    <sample id="222">This presentation discusses open-domain question answering (QA), where a retriever identifies relevant passages from a document corpus, and a reader generates an answer. The problem arises when models, trained on a general-purpose domain like Wikipedia, are applied to a new domain like biomedical data. Expanding the document collection with biomedical data can confuse the retriever, leading to incorrect answers.

The research proposes two approaches: few-shot and zero-shot data interventions to enable out-of-domain generalization. Few-shot interventions use a few examples from the target domain to prompt a Large Language Model (LLM) to generate more target domain data. Zero-shot interventions control interactions between the question, answer, and context. These interventions improve retriever performance by an average of 8% and reader performance by 11%.

The study also explores a generalizability test to understand the compatibility of a source model with a target domain. This test categorizes dataset shifts into "no shift," "concept shift," "covariate shift," and "full shift." It was found that all target datasets respond well to few-shot adaptations, while datasets with concept and covariate shifts, as well as "no shift" cases, respond well to zero-shot adaptations. The research concludes that the effectiveness of data interventions depends on the type of dataset shift.</sample>
    <sample id="223">The speaker's name is Shangbin Feng.</sample>
    <sample id="224">The models investigated during the experiments were long-mBART for document-level simplification and normal-base mBART for sentence-level simplification.</sample>
    <sample id="225">The MultiInstruct dataset uses 62 diverse tasks. Out of these, 53 tasks from 9 groups are used for training, with 10,000 instances per task. For testing, the entire Commonsense Reasoning group is reserved, along with 5 additional tasks from the VQA and Miscellaneous groups. All instances in the test split are used for each task. Additionally, 20 tasks from the test split of the Natural Instructions dataset are randomly sampled as unseen tasks for NLP.</sample>
    <sample id="226">There are three authors involved in the paper: Regina Stodden, Omar Momen, and Laura Kallmeyer.</sample>
    <sample id="227">The speaker begins by discussing the success of language models, but questions what is missing in current research. They suggest the answer is "grounded language understanding," which involves grounding a natural language expression into something executable in a target environment. They point out the many applications of this, from smart assistants and search engines to medical databases and robotic systems. The speaker then addresses the challenge of grounded language understanding, explaining that current language models are mostly trained with textual corpus and without grounding. This, they argue, is the reason for the "octopus test" and the gap between pre-training and downstream applications.

The speaker introduces Pangu, a novel framework for grounded language understanding. Their goal is to allow language models to focus on discrimination rather than generation, because discrimination is easier for language models to excel at. In their framework, a symbolic agent interacts with the environment and proposes valid candidate plans, while a language model scores and ranks them. This approach means the language model does not need to handle the validity and grammar of the target plan itself. They then present experimental results on knowledge-based question answering, showing that Pangu achieves state-of-the-art performance across all settings, including fine-tuning and in-context learning. Pangu also demonstrates strong sample efficiency, achieving over 50% accuracy on GraphQ with only one demo example. Finally, the speaker highlights Pangu's strong generalizability under non-I.I.D. settings, where the distributions of probability for both seen and unseen structures are almost the same. Their key message is that directly generating plans may not be the optimal way of using language models for grounded language understanding; instead, discrimination might be a much better strategy.</sample>
    <sample id="228">The authors experimented on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">The speaker, Gabriella Skitalinskaya, presents a joint work with Henning Wachsmuth on detecting improvable claims for argumentative writing support. The presentation begins with an introduction to text revision, highlighting its essential and recursive nature in argumentative writing and its influence on persuasive impact. It also includes an example of a claim evolving through revisions, illustrating how phrasing changes can lead to optimal communication.

The speaker introduces two new tasks: suboptimal-claim detection, where the goal is to determine if a claim needs revisions or is optimally phrased, and claim improvement suggestion, which involves selecting quality issues to improve during revision. The core of their work is to explore how to model the quality of argumentative texts based on implicit revision patterns from collaborative editing behaviors in online debate platforms like Kialo. They showcase examples of revision histories with optimal (green) and suboptimal (red) versions.

The presentation acknowledges four key challenges in this research: representativity and reliability of data from revision histories, model complexity and architecture in aligning models with revision ideas, contextuality in understanding how various contextual information impacts argument quality, and topical and user bias from controversial topics and user predispositions. The speaker concludes by inviting the audience to read their paper for further details on how these challenges are tackled and their findings, emphasizing the effectiveness of revision-based data for these tasks and the benefits of modeling the distance between claim versions for suboptimal-claim detection.</sample>
    <sample id="230">Hi everyone. I'm Koustuv Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper, "Language model acceptability judgements are not always robust to context." This is a joint work with Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, and Adina Williams. So, in this work, we revisit the minimal pair paradigm. So, the minimal pair paradigm basically evaluates language models on top of acceptability judgments, which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS-Pairs. And in this minimal pair paradigm, the typical way to evaluate language models is that you show uh, like an acceptable sentence or a grammatical sentence, and then you show an unacceptable sentence or an ungrammatical sentence. And then the hope is that the model basically puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate models' acceptance towards longer sentences. These days, large language models are coming up with longer and longer context windows, so it's uh, crucial that we evaluate the models' uh, acceptability throughout the context window. And that is what we are trying to do here. We are trying to uh, revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So, that is the approach. So, what we do is that to simulate these longer sequences, we revisit the data sets themselves, and then we recreate sentences by choosing uh, like acceptable or unacceptable sentences from those data sets. So, for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set, from the Adjunct Island case. And what we do is that to create like long sequences, and which are acceptable and which has the same matching of the grammatical structure, we extract grammatical sentences from Adjunct Island, and then we add it as a prefix to both the acceptable query and the unacceptable query. So, we can do the same thing by choosing unacceptable sentences from the same uh, matching, and that could also like be used to test the models' acceptability. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So, this will tell us like whether the models' uh, acceptability judgments are actually impacted by any context. Like whether the context is coming from a different uh, subset of the data set, or whether it's like completely irrelevant to the current uh, like to the sentence that we are looking at. So, how does the model do? So, first, we look at the Wikipedia sentences which are completely irrelevant to the current query pair. And there, we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT-2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So, here, we are choosing or creating sentences from acceptable and unacceptable domains, from the same BLiMP or SyntaxGym data set. And there, we see that the MPP judgments either increase or decrease significantly when you add uh, either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease in of the MPP uh, judgment for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now, this uh, and this is very large. Like this effect increases throughout the context length, and this would probably affect like newer language models which has large context window. So, why does the matched prefix affect the language model judgments so much? So, we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure, but adding uh, like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model uh, like change its course in terms of how it shows us the MPP judgment trend. Basically, we find that the models are sensitive to the perturbed sentences in similar ways, that is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations, and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic/semantic features which are shared across the sentences. And the MPP evaluation, the the way that we do it currently with short and single-sentence input, may not fully capture the language models' abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.</sample>
    <sample id="231">NACHOS is a 1.1 billion word open-source dataset of heterogeneous data crawled from diverse medical domains, natures, and styles.</sample>
    <sample id="232">The speaker's name is David Vilar.</sample>
    <sample id="233">The speaker introduces herself as Sara Papi from the University of Trento and Fondazione Bruno Kessler. She introduces their paper, "Attention as a Guide for Simultaneous Speech Translation," a joint work with Matteo Negri and Marco Turchi.

She defines Simultaneous Speech Translation (SimulST) as the real-time translation of spoken language into a text in another language, enabling cross-language communication. She then highlights the current problems with SimulST models: they require specific architectures and additional modules to be optimized, have long and complicated training procedures, and need several models to be trained and maintained for different latency regimes.

Their proposed solution, EDAtt (Encoder-Decoder Attention), addresses these issues. It uses already existing offline Speech Translation models without retraining or adopting specific architectures for SimulST. It also uses only one model for every latency regime and handles latency through specific parameters. The core of EDAtt is leveraging the knowledge acquired by the model through the attention mechanism between audio input and textual output. She illustrates how EDAtt works by showing when a word is emitted (when attention is not concentrated on the last lambda speech frames) and when it's not (when the sum of attention towards the last lambda speech frames is above a certain threshold).

She then presents the main results of EDAtt, plotting BLEU (translation quality) against Average Lagging (latency, including computational awareness). The goal is to have curves that are as high and as far to the left as possible. Comparing EDAtt with other popular strategies (wait-k, LA, CAAT) applied to offline models, she concludes that EDAtt outperforms all of them, showing superior quality while maintaining lower latency, making it the fastest strategy if considering actual elapsed time.

Finally, she encourages the audience to read their paper for more results and provides contact information, a GitHub link, Twitter handles, and a QR code to access their work.</sample>
    <sample id="234">The prompting strategy significantly impacts the results of translation, with differences of over 1 BLEURT point in most cases and up to 40 BLEURT points in extreme cases. This highlights the importance of choosing an effective prompting strategy. However, the exact form of the prompt has minimal influence when using a several-shot prompting strategy, as the examples carry most of the weight.</sample>
    <sample id="235">The authors of the paper are affiliated with Carnegie Mellon University Language Technologies Institute, Instituto Superior Técnico (Técnico Lisboa), Berkeley Artificial Intelligence Research (BAIR), and Unbabel.</sample>
    <sample id="236">The instructions are not explicitly mentioned in the transcript. However, the speaker says "each task is equipped with five expert-written instructions".</sample>
    <sample id="237">The authors propose a diagnostic test suite called KITMUS (Knowledge Integration from Multiple Sources) to evaluate the models' ability to integrate knowledge from different sources. This test suite introduces a coreference resolution task designed to probe the ability to draw on both pre-train-time knowledge and inference-time knowledge.</sample>
    <sample id="238">The presenter introduces "MeetingBank," a new benchmark dataset for meeting summarization, specifically created from city council meetings. The motivation behind this dataset stems from the increasing need for summarization technologies in a fast-paced world where numerous meetings occur daily for various purposes.

The creation of "MeetingBank" addressed two main challenges: the scarcity of high-quality meeting summaries and the difficulty in finding reliable sources for public meetings. The dataset includes meeting transcripts, reference summaries, and URLs that link to useful resources such as video and audio recordings, and minutes. The collection process involves converting audio data to transcripts, identifying meeting details like type and date to form a unique meeting ID, and then using this ID to find corresponding reference summaries and segment timestamps. Finally, these timestamps are aligned with the transcripts to create paired segment transcripts and summaries.

"MeetingBank" comprises 1,366 city council meetings from various U.S. cities, totaling nearly 7,000 summarization instances. On average, a typical meeting lasts about 2.6 hours with 28,000 tokens, and a segment contains around 2,900 tokens in the source and 87 in the summary.

Dataset analysis using extractive fragment coverage and density reveals that most summaries have a coverage score between 0.7 and 0.9, indicating they include important points rather than being purely abstract. Seattle and Boston show the highest density, while Denver exhibits the lowest, suggesting a higher level of editing on Denver's meeting minutes.

For model evaluation, ten top-tier summarization systems were tested, including extractive, abstractive (pre-trained and fine-tuned), and a large language model (GPT-3). Extractive Oracle performed well for extractive systems, and DialongLM achieved the highest ROUGE-2 score among abstractive models. GPT-3, while not performing well by automatic metrics, showed promising results in human evaluation for fluency and coherence, but struggled with informativeness and factuality.

In conclusion, "MeetingBank" serves as a valuable resource for researchers in meeting summarization and offers insights into city council decision-making processes.</sample>
    <sample id="241">This presentation discusses a human-in-the-loop (HiTL) evaluation framework for early misinformation detection, focusing on COVID-19 treatments. Current approaches often fall short due to unrealistic evaluation methods and a lack of human-centric design. The proposed HiTL system aims to be an end-to-end detection system that integrates human feedback at various stages, making it assistive rather than authoritative.

The system has two main components: detection of misleading claims and policy violation verification. For early claim detection, the system gathers relevant tweets, extracts check-worthy claims using a T5 model, and ranks them by "trendiness" for human validation. The evaluation of this component defines "early" as detecting a claim before its first appearance in a debunking news article. For policy violation verification, a BERT-based stance classification model determines the author's stance towards an unapproved treatment, flagging supporting tweets for human review. The system achieved a 65% precision rate in detecting policy violations, with approximately 124 policy violation tweets identified per human hour worked. The researchers hope their work motivates the development of more useful HiTL frameworks for misinformation detection, providing a concrete standard for comparison and offering an external perspective on such systems.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include comparative evaluations, where human judges select the better of two conversations, and Likert scale evaluations, where human judges rate conversations on a numerical scale.</sample>
    <sample id="243">There are **5** authors involved in the paper.</sample>
    <sample id="244">In the example with Servin and Kea, the background knowledge needed is that judges decide cases in courts of law.</sample>
    <sample id="245">This presentation outlines a two-step pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers for summarization tasks, addressing the issues of problematic automatic metrics and poorly understood recruitment practices.

The pipeline's first step is a "Qualification Task" designed to evaluate an annotator's ability to correctly assess multiple dimensions. Workers are categorized into GOLD, SILVER, BRONZE, and BLOCK based on their performance, with only GOLD and SILVER workers passing. Out of 200 participants, 26 (13%) qualified for the next stage.

The second step is an "Endurance Task" to test a worker's capacity for handling heavy workloads. It involves 10 Human Intelligence Tasks (HITs), each with one document and four summaries to evaluate saliency. 12 MTurk workers (6% of the initial 200), comprising 4 GOLD and 8 SILVER, passed this task. These workers achieved higher inter-annotator agreement (IAA) than experts, with a Cohen's Kappa of 0.55 and Krippendorff's Alpha of 0.443.

Finally, a "Reference-based Task" assesses general performance on a true annotation task, including 30 HITs, one reference, and four candidate summaries for information coverage. This pipeline serves as a best practice for obtaining high-agreement annotations at a large scale and lower cost, while also avoiding resource waste from discarded annotations. Future work will focus on hiring high-quality workers (both in terms of agreement and correctness) and exploring multiple applications across tasks, languages, and platforms.</sample>
    <sample id="246">Yes, the dataset, generation, and evaluation code are available on GitHub at mpoemsl/kitmus.</sample>
    <sample id="247">The speaker introduces FactKG, a new dataset for fact verification via reasoning on knowledge graphs, addressing the lack of such datasets in the field. FactKG utilizes DBpedia as its knowledge graph and features claims in both written and colloquial styles for practical applicability. The claims are labeled as either "SUPPORTED" or "REFUTED".

The dataset incorporates five distinct reasoning types:
1.  **One-hop:** Verifies if two entities in a claim are connected by a single relation.
2.  **Conjunction:** Involves multiple one-hop claims that must all be true for the main claim to be supported.
3.  **Existence:** Checks if an entity in the claim is connected to a specific relation.
4.  **Multi-hop:** Requires multi-hop inference as some entities may not appear in their surface form in the claim.
5.  **Negation:** Involves an additional inference step for claims containing negative statements, even after finding graph evidence.

To increase practicality, colloquial style claims were generated using a style transfer model, and presupposition templates were created.

Baseline experiments show that models using graphical evidence, such as GEAR, significantly outperform claim-only models like BERT, BlueBERT, and Flan-T5 across all reasoning types. This suggests that incorporating knowledge graph evidence is crucial for robust fact verification.</sample>
    <sample id="248">The annotators for NLPositionality are diverse, representing 87 countries.</sample>
    <sample id="249">The speaker mentioned that sentences in the acceptable domain were perturbed by introducing noise while preserving the relevant structure. He then revealed a graph showing various perturbation methods, including prefix/suffix adverbs, long prefix adverbs, adding clauses, and using quotes. However, the speaker did not specify how these methods specifically altered the acceptable sentences. He only stated that the models were sensitive to these perturbed sentences in a similar fashion.</sample>
    <sample id="250">A dimensional evaluation measures multiple aspects of dialogue quality, such as relevance, consistency, and emotional understanding, to identify a model's strengths and weaknesses.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia.</sample>
    <sample id="252">This presentation introduces **U-CREAT: Unsupervised Case Retrieval using Events extrAcTion**, a joint work from IIT Kanpur.

**Motivation:** Legal professionals historically rely on experience to cite past precedents. However, with increasing cases, this becomes challenging. **Prior Case Retrieval (PCR)** automates this by retrieving relevant past legal documents based on factual and precedent relevance.

**Contributions:**
1.  **IL-PCR Dataset:** A new benchmark for PCR in the Indian legal system, containing 7070 legal cases with an average of 6.775 citations per query document. It's larger and more complex than existing datasets like COLIEE'21.
2.  **U-CREAT Pipeline:** An event-based unsupervised approach for PCR.

**Event Extraction:** Case documents are viewed as narratives of events. Events are extracted as predicate (verb) and argument triplets using dependency parsing with spaCy, which involves preprocessing, dependency parsing, and post-processing. For example, "These statements were forwarded to the Police" yields the event (statement, forward, police).

**U-CREAT Pipeline:** Query and candidate documents undergo event extraction. Interaction matrices are computed between query and candidate events, where common events are identified. These matrices are then fed into retrieval models to rank candidates.

**Experimental Models:** The research experimented with three categories of models:
*   **Count-based (Word-level):** BM25 and BM25(Bigram).
*   **Transformer-based:** Segmented-Doc Transformer (DistilBERT), Transformer (BERT finetuned), and Sentence Transformer (SBERT).
*   **Event-based:** Atomic Events, Non-Atomic Events, Event Filtered Docs, and RR Filtered Docs, all using BM25 with different n-grams.

**Results:** Transformer-based models performed poorly compared to BM25 baselines. However, event-based models significantly outperformed baselines. **Event Filtered Docs (BM25 Quad-gram)** was the best-performing model, demonstrating better performance than transformers and the baseline. It also achieved state-of-the-art results on the COLIEE'21 dataset (27.32 F1), outperforming existing supervised and unsupervised methods. Event-based models also exhibited lower inference times.

**Conclusion:** U-CREAT proposes a new IL-PCR dataset and an unsupervised event-based retrieval pipeline for legal documents, offering better performance and inference time without corpus-specific fine-tuning.</sample>
    <sample id="253">Mario Ezra Aragón introduces "DisorBERT," a double domain adaptation model for detecting mental disorders in social media. Mental disorders are psychological syndromes associated with distress and disability, affecting thinking, feeling, mood, and behavior. Given the prevalence of social media usage, with 4.76 billion active users, there's an opportunity to research how people express mental health issues online.

DisorBERT aims to contribute to this by automatically analyzing social media posts to warn about the onset of mental disorders. The model uses domain adaptation to improve performance on specific tasks, building on a base language model like BERT, which is trained on general data like Wikipedia and Google Books. DisorBERT fine-tunes BERT to learn the language of social media (Reddit data) and specialize in the mental disorders domain, incorporating knowledge from a lexicon to guide the masking process.

Experimental results on eRisk datasets show DisorBERT achieves a good balance between precision and recall, outperforming other methods that tend to favor one metric over the other. Analysis using the Beck Depression Inventory (BDI-Test) reveals that DisorBERT generates more negative and psychologically oriented words compared to BERT, indicating its specialization in mental health language. User analysis of a depression case shows DisorBERT highlights words related to anxiety and medication, relevant to depression.

In conclusion, the double domain adaptation and guided masking approach effectively capture signs of mental disorders in social media. Future work includes exploring different lexical resources and utilizing clinical data to train more specialized language models.</sample>
    <sample id="254">The speaker introduces a research work on uncertainty-guided label denoising for document-level distant relation extraction (DocRE). They explain that DocRE aims to extract relations among entities in a document. Existing methods rely on human-annotated corpora, which are time-consuming. To address this, recent works use distantly supervised (DS) data, which is auto-labeled from knowledge bases. However, DS data often contains noisy labels. The speaker points out that current efforts to mitigate noise by using pseudo labels still risk noise introduction due to false positive pseudo labels.

To overcome this, the paper proposes a DocRE framework with uncertainty-guided label denoising to improve the quality of DS data. The methodology involves a pre-denoising DocRE model trained with DS and human-annotated (HA) data to generate pseudo labels. An instance-level uncertainty estimation is introduced to determine the trustworthiness of model predictions, particularly for overlapping relations. They also design a re-labeling strategy using dynamic class uncertainty thresholds to filter out pseudo labels with high uncertainty, especially for long-tail classes. This multi-phase training iteratively improves the denoised DS data. Experimental results demonstrate that their UGDDRE framework outperforms previous baselines on both DocRED and Re-DocRED datasets. In conclusion, the framework effectively improves label quality, measures pseudo label reliability, filters high-uncertainty labels, and achieves significant performance improvements.</sample>
    <sample id="255">The form of prompting is crucial for zero- and one-shot prompting.</sample>
    <sample id="256">Hello, my name is Vasudha, and I'm a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, "I know that cigarettes could kill me," and then goes on to say, "I grabbed a couple smokes after the meeting today." This belief and action are inconsistent, and they are in dissonance. Further, mentioning that "I don't think I could keep my job without them" justifies the second occurrence, and they have a consonance relationship. While dissonance is a very common phenomenon we experience in daily decision-making, they are really rare to find expressed in language among other kinds of discourse relations. So, why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief, values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used a dissonance-first approach as seen in the flowchart here. Tweets were parsed using a PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior search dataset, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonance samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Um, since the initial model was not able to capture the dissonance class at all, we start the um active learning process by transferring weights from closely related tasks. We transfer from two different tasks. Topic-independent dissonance stance classification. A task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic. Called debate here. And on binary classification of expansion and comparison classes of PDTB, since these two are closely related to the conception of consonance and dissonance, and we call them CE here. We find that on transferring, the zero-shot performance on the annotated dataset is already much better than chance with the best with AUC 0.62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold-start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotation so far, whereas iterative updates the model by training on the latest set of data collected. Over the different strategies, we found that cumulative performed equal or better than iterative across the board. We find that the proposed PRC strategy works better than other state of the art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition, and cold-starting AL with appropriately designed transfer learning tasks can help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in-domain active annotations benefit from cumulative update. These are the links to our code, dataset, and our paper. Uh, feel free to get in touch with us if you have any questions. Thank you!</sample>
    <sample id="257">The authors evaluated 4 open-domain dialog models.</sample>
    <sample id="258">This video discusses a new method called Large Language Model (LLM) evaluation, which uses LLMs to assess the quality of texts. Unlike traditional human evaluations, which can be unstable and hard to reproduce, LLMs are trained to follow natural language instructions. The research proposes giving LLMs instructions and samples, and then using their output to generate ratings.

The experiment aims to verify the usefulness of LLM evaluation by comparing LLM ratings with human evaluations. Two types of stories are rated: those generated by GPT-2 and those written by humans. The LLMs are instructed to rate stories based on four attributes: grammar, coherence, likeability, and relevance. For comparison, English teachers are hired to conduct human evaluations on the same stories and with identical instructions, as they are considered experts in rating stories and essays.

The results show that human evaluators tend to prefer human-written stories over GPT-2 generated ones. Smaller LLMs (T0 and curie) do not show a significant preference for human-written stories. However, larger LLMs (davinci and ChatGPT) demonstrate a clear preference for human-written texts, similar to the English teachers. This suggests that some LLMs can indeed serve as a viable alternative to human evaluation in this context.

Further questions are raised, such as whether LLMs and human evaluators agree on individual story ratings, how changing instruction wordings affects results, and what are the pros and cons of LLM evaluation compared to human evaluation. These questions are addressed in the full paper.</sample>
    <sample id="259">Yusen Zhang presents XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations.

Semantic Parsing involves building semantic representations of user queries, such as SQL and Lambda Calculus. Cross-lingual Semantic Parsing (CLSP) translates queries in multiple natural languages into multiple meaning representations. Existing CLSP models often suffer from limited language and meaning representation coverage, or rely on a single neural model for evaluation.

XSemPLR addresses these limitations by providing a unified dataset for CLSP across diverse natural languages and meaning representations. It encompasses:
* 9 datasets in various domains
* 5 semantic parsing tasks
* 8 meaning representations
* 22 natural languages in 15 language families

The presentation outlines six experimental settings for training and evaluation:
1.  **Translate-Test**: Uses Google Translate API to translate source language to target language, then trains a monolingual model.
2.  **Monolingual Model**: Source language is the same as the target language (e.g., German-to-German), including a few-shot setting with 10% training data.
3.  **Multilingual Model**: Trains a single multilingual model for all languages.
4.  **Cross-lingual Zero-shot/Few-shot transfer**: Trains on one source language and transfers to another.

Key findings include:
*   In monolingual settings, Enc-Dec (mT5) models (multilingual pretrained encoder-decoder models) obtain the best performance across all datasets, outperforming Enc-PTR models (multilingual pretrained encoders with pointer-based decoders).
*   Multilingual training generally improves performance for most natural languages, except for English, which experiences a performance drop in 7 datasets (a phenomenon known as "Curse of Multilinguality").
*   A significant performance gap exists for zero-shot cross-lingual transfer, which is rapidly shortened in few-shot settings.
*   Pretraining on English natural language significantly boosts few-shot performance on target natural languages.
*   Multilingual LLMs (Codex &amp; BLOOM) are currently inadequate for CLSP tasks.
*   Chinese transfer learning and English monolingual training show the largest performance gap, while German typically has the smallest.
*   FunQL outperforms other meaning representations, with SQL yielding the worst performance.

The paper and code for XSemPLR are publicly available.</sample>
    <sample id="260">The paper, "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark," involves 10 authors: Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie.</sample>
    <sample id="261">A good planner writes scripts that are reasonable and faithful to the constraints.</sample>
    <sample id="262">There are nine authors involved in the paper: Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, and Deqing Yang.</sample>
    <sample id="263">In-context learning is a popular paradigm for utilizing large language models (LLMs). However, its ability is known to be unstable due to various design choices, which introduces biases into the model's predictions. The speaker begins by introducing a typology of label biases in in-context learning for classification tasks, defining three types of label biases:
1.  **Vanilla-label bias**: Captures the model's uncontextual preference for label names.
2.  **Context-label bias**: Captures the effects from the context.
3.  **Domain-label bias**: This is a new type of bias introduced in this work, capturing the effects of the task corpus on the model's prediction. The speaker demonstrates that random in-domain words can severely bias the model's predictions, while random English words do not show such a preference.

LLMs exhibit different in-context learning behaviors on tasks with varying levels of domain-label bias. On tasks with small domain-label bias, in-context learning performs well, and advanced calibration methods can improve performance. However, on tasks with large domain-label bias, LLMs struggle to outperform a chance-level baseline, even with prior calibration methods.

To mitigate all three types of label biases, the speaker proposes a novel method called **Domain-Context (DC) Calibration**. Unlike prior calibration methods that use a single predefined content-free token (which can be biased), DC calibration uses random in-domain words as content-free text to estimate and calibrate the model's bias. This approach addresses the limitations of previous methods by considering domain-specific biases.

Experimental results show that DC calibration generally improves in-context learning, especially on tasks with large domain-label bias. Ablation studies confirm that predefined content-free tokens can be biased, using only one content-free token is suboptimal, and calibrating with random in-domain words (DC) effectively removes domain-label bias, leading to significant improvements. This trend holds even for larger models like GPT-3.</sample>
    <sample id="264">The speaker introduces their paper, "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the limitations and challenges in existing text generation tasks. Data annotation is arduous and expensive, and current methods suffer from degradation due to varying conditions across domains. The main challenge is multi-modal domain shifts, where visual content changes significantly with image style, but audio content like rhythm and energy remain consistent.

The proposed framework aims to adapt to new multimodal domains with limited labeled data. It comprises three components: an Audio-Visual Meta-Mapper Network (AVMM), an Audio-Visual Encoder &amp; Language Model Generator (AVE &amp; LMG), and Counterfactual Contrastive Learning. The AVMM maps diverse visual concepts into a unified auditory semantic space, addressing shifts in semantic distribution. It uses learnable tokens, called visual prefixes, for audio clusters, improving their semantic quality by optimizing the coherence of reconstructed audio and aligning visual content with the audio space.

The encoder and generator utilize a transformer-based approach. The speaker introduces an alpha parameter to quantify each modality's contribution to a word, computed by matching the relevance between cross-attention of each modality and previous words. For the loss function and training, they propose two counterfactual contrastive learning methods to optimize visual-text alignment without relying on random negative samples.

Experiments validate the approach using cross-dataset and cross-domain benchmarks. TAVT outperforms compared models across various metrics, especially in low-resource domains like "Kids" and "Beauty," where other methods show significant performance degradation. Ablation studies confirm the effectiveness of audio features and the specific components of the model.</sample>
    <sample id="265">The speaker's name is Vasudha.</sample>
    <sample id="266">The authors are affiliated with the Institute of Computer Science, Polish Academy of Sciences, and the University of Warsaw.</sample>
    <sample id="267">Hello everyone, my name is Yusen Zhang from The Pennsylvania State University. Today, I'm going to present our work, XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. Semantic parsing is a task to build semantic representations of user queries, such as SQL and Lambda Calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda, or FunQL, and etc. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there's lack of coverage on certain natural language, the Chinese is missing, and lack of coverage on certain meaning representations. The Lambda Calculus is missing, or they're only evaluated on certain neural model. For example, there's only one single model to evaluate them. So to this end, we propose XSemPLR. We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains nine datasets in various domains, five semantic parsing tasks, eight meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate-test. We use Google Translate API to translate source to the target language, then use monolingual model to train and eval. And, for example, we train the English model on English query. And during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we also test monolingual model. In this setting, the source language is the same as target language, e.g. German-to-German or English-to-English. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. And we test monolingual multilingual model, which uh, we train one multilingual model for all languages. For example, uh, we put the German, English, Chinese queries together to train a multilingual model, and during inference, uh, we can, uh, use this model to, um, to translate German queries or Chinese query or, etc. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, uh, we'll train it on English query or the combination of English and German few-shot queries to train a multilingual model to, and predict the SQL output. And we also find many interesting results. So, regarding analysis of, um, monolingual models. We evaluate on two groups of models, uh, including Enc-PTR, which stands for multilingual pretrained encoders with pointer-based decoders, such as XLM-R+PTR, mBERT+PTR. And, uh, we also evaluate Enc-Dec models, which is multilingual pretrained encoder-decoder models. Um, such as mBART and mT5. We found that Enc-Dec obtains the best performance on all nine datasets. And we evaluate on mT5 and exam, uh, XLM-R+PTR on multilingual setting. We found that Enc-Dec/Enc-PTR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and gains in three datasets. I think this is known as "Curse of Multilinguality". We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found the for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, um, Enc-Dec (mT5) outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot on target NLs. And we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. Um, to sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show, uh, many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="268">The most common error of PaLM is omission error.</sample>
    <sample id="270">The authors are affiliated with Emory University and Amazon Alexa AI.</sample>
    <sample id="271">In this paper, CFT stands for continuous fine-tuning.</sample>
    <sample id="272">Eight authors are involved in the paper: Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, and Adina Williams.</sample>
    <sample id="274">The speaker's name is Yusen Zhang.</sample>
    <sample id="275">Hi, I'm Shangbin, a PhD student at the University of Washington. Today I'm presenting our work from Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. So language models are trained on large-scale web-crawled data. Political news media are well covered in their pretraining data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and may lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks. Specifically, we ask the following questions. First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation well-grounded in political science literature. So some preliminary results demonstrate that, first, language models do have varying political leanings. They occupy all four quadrants on the political compass. We can also see that GPT-4 is the most liberal language model of them all and GPT series are generally more socially liberal than BERT series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we conduct a control experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leaning. By further pretraining language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further fine-tuned and further trained on the left-leaning Reddit corpus, we can see a substantial liberal shift in terms of its in terms of its political biases. And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society. So we divide pretraining corpora into pre-45th President of the United States and after 45th President of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the like polarization in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection, two NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per-category performance, that is to say, if we separate the performance into different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups. However, at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men. However, worse at detecting hate speech targeting black, LGBTQ+, and other minority communities. Similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. This we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social category. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control. So this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis. So if we do not sanitize the political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitize somehow, we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retained in language model training data. So it's kind of like the electric electric trolley problem. Okay. Great. I think that's pretty much all I have for the day for today. Thank you for your time.</sample>
    <sample id="276">The presentation introduces "IndicMT Eval," a dataset for evaluating machine translation metrics across five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati. The project focuses on "English to Other Language" translations, which are typically under-researched.

To build the dataset, 200 English sentences were randomly selected from the Flores dataset. These sentences were translated into each of the five Indian languages using seven different machine translation models/APIs, resulting in 1,400 candidate translations per language and a total of 7,000 samples.

Human annotators, fluent in both English and the target Indian languages, evaluated these translations using the MQM (Multidimensional Quality Metrics) framework. Annotators highlighted minor/major errors and categorized them into "Accuracy" (meaning-based errors like addition, omission, mistranslation, untranslated text) and "Fluency" (grammar, spelling, register, character encoding). They also provided an overall score for each translated output.

Analysis of the human annotations revealed that newer MT models (like NLLB and IndicTrans) produced fewer errors than older ones (like CVIT). IndicTrans and NLLB also performed best overall according to human expert scores.

Correlations between various MT metrics and human scores were then examined. CHRF++ showed the highest correlation among overlap-based metrics, while LabSE embeddings performed best among embedding-based metrics. COMET metric variants (especially COMET-MQM) demonstrated the highest overall correlations.

The researchers also finetuned COMET metric variants using the MQM annotations to create "Indic COMET." IndicCOMET-MQM outperformed baseline COMET metrics in correlation on three out of five languages and showed higher overall correlation. It also demonstrated better zero-shot performance and robustness on the ACES Translation Accuracy Challenge Set compared to its COMET counterpart. The dataset and code are publicly available for further research.</sample>
    <sample id="277">The new method does not have a name.</sample>
    <sample id="278">The author describes the "marked words" method as a way to identify words that distinguish "marked groups" from "unmarked groups" by comparing personas using weighted log-odds ratios. This method allows for the identification of specific stereotypes and patterns without relying on a pre-existing lexicon.</sample>
    <sample id="279">The authors are affiliated with the Paul G. Allen School (University of Washington), UW NLP, Carnegie Mellon University's Language Technologies Institute, and Tsinghua University.</sample>
    <sample id="280">The paper introduces MultiEMO, an attention-based, correlation-aware multimodal fusion framework for emotion recognition in conversations. Existing approaches in Emotion Recognition in Conversations (ERC) face challenges such as inadequate exploitation of multimodal information, poor performance in minority emotion classes, and difficulty distinguishing semantically similar emotions.

To address these issues, the authors propose several contributions. First, they developed VisExtNet, a novel visual feature extractor that captures facial expressions of interlocutors without incorporating redundant scene-related information. This is achieved by using a MTCNN and a VGGface2 pre-trained ResNet-101, integrating facial expressions from multiple frames.

Second, they designed MultiAttn, a multimodal fusion model based on bidirectional multi-head cross-attention layers. MultiAttn has three components (MultiAttn_text, MultiAttn_audio, MultiAttn_visual), each designed to integrate complementary information from other modalities through stacked cross-attention layers. This approach successfully models complex correlations across textual, audio, and visual modalities.

Third, they introduced a Sample-Weighted Focal Contrastive (SWFC) loss. This loss assigns higher importance to hard-to-classify minority classes and ensures sample pairs with different emotion labels are mutually exclusive. This maximizes inter-class distances, leading to better distinction of semantically similar emotions.

Experimental results on MELD and IEMOCAP datasets show that MultiEMO achieves state-of-the-art performances, with notable improvements in minority and semantically similar emotion classes. A case study further demonstrates MultiEMO's ability to handle asynchronization of emotional tendencies from different modalities.

Limitations include VisExtNet's inability to distinguish between speakers and irrelevant people in a scene, the need for a large batch size for SWFC loss on MELD due to class imbalance, and persistent lower performance in minority emotions compared to majority classes.</sample>
    <sample id="281">The presented work, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," delves into the complexities of context-dependent translation. The speaker, Kayo Yin, highlights that a significant portion of translations relies on context, yet current evaluation methods struggle to capture these nuances. Existing corpus-level metrics like BLEU are often insufficient, and targeted evaluations, while helpful, are limited by specific discourse phenomena and languages, requiring extensive human annotation.

To address these challenges, the research introduces Pointwise Conditional Cross-Mutual Information (P-CXMI), a metric designed to quantify how much a word's translation depends on context at both sentence and word levels. High P-CXMI values indicate words that strongly require context for accurate translation.

A thematic analysis was conducted on TED Talk transcripts translated from English into 14 different languages. The analysis revealed that pronouns and verb forms often exhibit high P-CXMI, particularly in languages with grammatical features like dual pronouns (e.g., Arabic) or intricate verb conjugations. Lexical cohesion, ensuring consistent proper noun translation within a document, and formality, requiring context to choose appropriate registers, also emerged as significant context-dependent phenomena. Furthermore, ellipsis resolution, where sentence structure dictates meaning, was identified as a context-dependent factor not solely tied to individual words.

Building on these findings, the team developed the Multilingual Discourse-Aware (MuDA) tagger, an automatic tool to identify words related to these five discourse phenomena. This tagger is integral to the proposed MuDA benchmark, a dataset-agnostic evaluation framework for document-level machine translation.

Model evaluation using this benchmark showed that context-aware models significantly outperform context-agnostic models for phenomena like formality and lexical cohesion. However, there was no substantial improvement for ellipsis, pronouns, and verb forms, indicating areas where further progress is needed. Comparisons with commercial systems also revealed that DeepL generally outperforms Google Translate on most discourse phenomena and language pairs.

In summary, this research provides a data-driven, systematic approach to identify context-dependent translation phenomena without relying on prior linguistic knowledge. The MuDA benchmark offers a valuable tool for evaluating and advancing document-level machine translation systems, helping to pinpoint strengths and weaknesses in handling crucial contextual aspects.</sample>
    <sample id="282">In this presentation, I'm excited to introduce our new work, StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing. This research addresses a crucial task in natural language generation: non-parallel text style transfer, focusing on the story level at the discourse level, which is vital for accurately replicating author styles.

The primary challenges lie in imitating an author's linguistic choices at the discourse level, especially since author styles often correlate with specific writing topics. This makes it difficult to transfer style-specific content to another style, as seen in the missing content example in Table 1.

Our solution, StyleTrans, learns discourse representations from source texts and combines them with learned style embeddings to generate text in the target style. We've also designed a new training objective to disentangle style-specific features from discourse representations. The training process involves two stages: first, an adversarial training framework employing a self-reconstruction loss to recover input, a disentanglement loss to separate style and content at the sentence level, and a style classifier loss to produce style signals for the entire system. The second stage focuses on filling in correct style-specific content and removing masked tokens.

We collected new datasets in Chinese and English for this task and conducted extensive experiments to transfer stories to typical author styles. Both automatic and manual evaluations confirm the effectiveness of our model. StyleTrans outperforms strong baselines in terms of style control and content preservation. Style visualization shows that our transferred texts align well with the golden texts in the style feature space. Our data and code are available in the repository. Please feel free to email me with any questions.</sample>
    <sample id="283">Conjunction-headed/Prague</sample>
    <sample id="284">This presentation introduces FSU-IE, a novel Fuzzy Span Mechanism designed to enhance Universal Information Extraction. The speaker highlights two main motivations: first, existing UIE models over-rely on precise span boundary positions, leading to ambiguity. FSU-IE proposes a fuzzy span boundary, modeling the boundary as a continuous distribution to better capture this ambiguity. Second, there's a mismatch between Transformer's global feature extraction and Information Extraction's focus on local features. FSU-IE addresses this with a Fuzzy Span Attention mechanism that adaptively adjusts attention rather than statically, allowing the model to focus on semantic information within a limited range of preceding tokens.

The Fuzzy Span Loss converts continuous boundary distributions into discrete values for calculation and incorporates KL divergence between the predicted and fuzzy boundaries. The Fuzzy Span Attention, with its mask function, dynamically adjusts the attention span and linearly decays attention distribution at boundaries. The model's architecture integrates this attention layer at the top level to guide the decision process without compromising text encoding.

Experimental results on Named Entity Recognition, Relationship Extraction, and Aspect Sentiment Triplet Extraction tasks demonstrate FSU-IE's effectiveness. It shows significant performance improvement over UIE-base, better generalization capabilities, and improved information extraction with a simpler structure. Ablation studies further confirm that Fuzzy Span Attention speeds up convergence, and Fuzzy Span Loss enhances information extraction, with their combined effect boosting overall performance.</sample>
    <sample id="285">This presentation discusses the challenge of factual errors in dialogue summarization, which are present even in reference summaries. Two types of solutions are explored: improving summarization models for factuality and developing separate factual error correction (FEC) models. The latter takes the source document and a model-generated summary as input to produce a corrected summary. 

A key issue highlighted is the current evaluation method for FEC models, which relies on overall factuality metrics like FactCC. This approach is problematic because it provides a vague score and can blur the lines between summarization and FEC, as an FEC model might simply generate a new, factually correct summary rather than actively correcting errors.

To address these limitations, the presenters propose manually annotating "reference corrections" for model-generated summaries that contain factual errors. These annotations aim to correct errors using minimal substitutions, insertions, and deletions, ensuring a fluent and non-redundant summary. This approach provides more valuable training data for FEC models and creates a more comprehensive evaluation framework.

The presentation introduces a new taxonomy of factual errors, divided into form-based and content-based categories, to automatically classify errors. This system, inspired by ERRANT, involves three steps: alignment, classification, and comparison, to generate scores.

Experiments with different FEC model training modes, including using pseudo data, real data (human-annotated), or a combination of both, reveal several findings. Training FEC models with reference summaries from dialogue summarization datasets yields the best results on unreliable factuality metrics, underscoring the need for improved evaluation methods. Human-corrected summaries improve FEC model performance, suggesting that combining human-annotated data with synthetic data is a promising direction. Lastly, current FEC models struggle with correcting factual errors involving addition and cannot effectively address attribute, modality, or link errors.</sample>
    <sample id="286">The speaker's name is Sarah Finch.</sample>
    <sample id="287">There are **four** authors involved in the paper.</sample>
    <sample id="288">The following datasets can be used to test syntactic phenomena: BLiMP and SyntaxGym.</sample>
    <sample id="289">Hello, my name is Kayo Yin and I will be presenting our work titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration." This work was done in collaboration with Patrick Fernan, Emmy Liu, Andre F.T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate mole in this sentence? Well, if the previous sentence was, "Things could start to get dangerous if the ministers find out." Then mole refers to a spy. But if the previous sentence was, "Could it be anything serious, doctor?" Then mole refers to a birthmark. So depending on context, the meaning of the word changes and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluations on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages. Since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context for translation. In previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as words that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at Part of Speech tags that have high means P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because, uh, English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns, uh, to make sure that you're using the same translation within the document. And similarly, we find that context is supported to translate in the right formality. And finally, uh, we look at different, um, at individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured, uh, by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we call our tagger the Multilingual Discourse-Aware or MuDA tagger. We can then, um, also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation. And we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, uh, we use, um, our benchmark, as well as other metrics to evaluate different models, um, on document-level machine translation. First of all, when we use corpus-level metrics, uh, so for BLEU, we find that context agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word F-measure, then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now we use the MuDA benchmark to evaluate models, and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests, uh, where we would need to see more progress for document-level translation. We also compared different commercial systems, and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we performed a data-driven analysis across 14 language pairs to identify when translations require context. And then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.</sample>
    <sample id="290">The five methods are FTw, BOND, COSINE, MLC, and L2R.</sample>
    <sample id="291">The model is evaluated on 11 downstream tasks, including named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="292">Hi. Welcome to our presentation of DE-plain, a new corpus for German text simplification on the document level and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model, we require parallel pairs of text, for example, of documents or sentences. In the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible, as you can see in the example. such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus DE-plain. Because in the recent years, there were some problems with existing corpora. So, for example, these corpora here are too small to train a text simplification model on. The other three models, which are proposed in recent years, are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DE-plain, which is split into two subcorpora: DE-plain-APA and DE-plain-web. DE-plain-APA is based on news texts. In DE-plain-APA, we aligned 483 documents, all manually. It results in roughly 30,000, 13,000 parallel sentence pairs. For DE-plain-web, this corpus includes different domains. And we also align all of these 750 documents on the one hand manually, and on the other hand with automatic alignment methods. In total, we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible text are much stronger simplified than for example, the news text, also language learner texts. On all level, regarding, for example, lexical simplification, structural simplification, also overall level of simplification. Furthermore, you can see that our DE-plain corpus has a high variety of different simplification transformations. So, for example, in the DE-plain-APA corpus, we have much more reordering and word additions, then we have in the DE-plain-web corpus. On the other hand, in the web corpus, we have much more rephrasing. So, let's now see what we can do with this corpus. Hello, I'm Omar and now I will talk about the use cases for our dataset DE-plain. So for the first use case, uh we can evaluate uh automatic alignment methods. Uh in the recent years, there has been a lot of alignment methods, but in the context of machine translation, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, uh we are trying to extract alignments between sentences of two parallel documents, having the same language, having the same content, but they are on a different complexity levels. And now as we have our dataset DE-plain which have uh manually uh aligned sentences, uh we can use these sentences as gold standard alignments to evaluate some of the proposed uh alignment methods. And we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that uh the best alignment, automatic alignment method to use for text, for German text simplification, uh is the method of MASAlign. And you can also find the code to uh run this method on your own documents in the paper. The second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce uh simplified text from the complex input text. We have fine-tuned two different models. Uh we have fine-tuned the model of long-mBART to produce uh document-level simplifications and we also fine-tuned the normal based long, the normal base mBART to produce sentence-level simplifications. Uh you can also find all the checkpoints and uh you can look into more details at the scores and the evaluation metrics of our experiments in the paper. Uh we concluded that this this basic fine-tuning could produce uh or could get uh scores better than the baseline scores and we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you uh during the conference. Thank you.</sample>
    <sample id="293">Hi. Uh I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the AltEntities corpus. My name is Javad Hosseini, and this is a joint work with Philip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users' language when they want to make a choice. Uh consider this alternative question. Did you mean "Easy On Me" or "I Gotta Feeling?" Here, a user wants to select between one of these two songs. Um the most obvious thing is to use a direct reference, for example, by saying the name of the song, "Easy On Me", or its position, "the first one". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some example indirect references, for example, "the newer one" or "the song that's not energetic". This is an important problem in conversational systems, and also for benchmarking LMs entity understanding. We're not aware of a public dataset, a large-scale public dataset for the task. So we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes. Our dataset collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialog context. In the in the second speech bubble, Alice says, "Do you mean "Easy On Me" or "I Gotta Feeling"?" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, "the newer one". We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template: "Do you mean A or B?" Where A and B are sampled from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles. For example, two books with the name "The Return". The third one is when they have similar descriptions on Wikipedia, and finally when they have similar infoboxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show these alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google Search link to each song. And then ask the annotators to listen to at least some of each song and read about each song. Here's for example the Google Search result for the song "Easy On Me". For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using 3 to 5 indirect referring expressions, for example, "The one with the piano music". Here are some examples from our dataset. For example, "the one without words", "not the one with the 12 year old boy", or "the fictional one" or "comes from Azerbaijan" and so on. The AltEntities Corpus has 6,000 alternative questions across the three domains. And it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as annotators, then the accuracy is really high. It's around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when um the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%. So there's a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.</sample>
    <sample id="294">CamemBERT is initially trained on a generic French corpus, such as OSCAR 138 GB or CSiNet 4 GB.</sample>
    <sample id="295">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile introduces a collaborative project between the University of Turin and Amazon Alexa focusing on "EPIC: Multi-Perspective Annotation of a Corpus of Irony." The project aims to address the limitations of the "ground truth" paradigm in natural language understanding, especially for subjective tasks like irony detection. They developed a corpus called "EPIC: English Perspectivist Irony Corpus," consisting of approximately 3,000 short conversations (text/reply pairs) collected from Reddit and Twitter between January 2020 and June 2021. The data spans five varieties of English: United Kingdom, United States, Ireland, Australia, and India.

The annotation process involved 74 annotators (around 15 per English variety) using the Prolific crowd-sourcing platform. Each annotator labeled 200 text/reply pairs, resulting in an average of 5 annotations per text. The annotators were balanced by gender and country of residence and were required to annotate instances from all English varieties. The annotation task presented a message and its reply, asking the annotator to determine if the reply was ironic.

The researchers observed variations in inter-annotator agreement (IAA) across different demographic groups of annotators (gender, age group, nationality, ethnicity, student status, employment status). They then built perspective-aware models by fine-tuning pre-trained language models on data split by these different annotator groups. While raw performance didn't show clear trends, perspective-aware models demonstrated less uncertainty (higher confidence) in their predictions compared to standard non-perspectivist models. Finally, the study found peculiar patterns in irony perception: contiguous generations (e.g., boomers vs. Gen Y) and certain geographical regions (e.g., UK and Ireland) exhibited the highest variation in their perception of irony.</sample>
    <sample id="297">The speaker introduces a project that aims to understand dogwhistles, which are terms that send a coded, often controversial, message to an in-group while having a literal, innocent meaning for an out-group. This allows speakers to express hate or abusive rhetoric while evading content moderation.

The project involves several steps:
1. **Developing a typology and glossary:** They collected over 340 dogwhistle terms and symbols (including emojis) from various sources like academic papers, media, blogs, and wikis. These terms are categorized by their persona (e.g., anti-Asian, transphobic, antisemitic) and register (formal or informal). Each entry includes the in-group meaning, persona, type, register, an explanation of its dogwhistle nature, source, and a real-world example.
2. **Case study of historical U.S. political speeches:** They analyzed the usage of racial dogwhistles in the U.S. Congressional Record, finding a correlation between their increased usage and the Republican Southern Strategy since the Civil Rights Era. They also observed a growing association of these dogwhistles with conservative speakers over time.
3. **Evaluating dogwhistle recognition in language models:** They prompted GPT-3 with different definitions and requests for dogwhistle examples. GPT-3 successfully identified 45% of dogwhistles in their glossary, with higher recall for formal dogwhistles (69%). However, its performance varied significantly, struggling with informal/online dogwhistles and transphobic ones.
4. **Identifying covert meanings with GPT-3:** They tested GPT-3's ability to uncover the hidden meanings of dogwhistles. They found that providing dogwhistle definitions and "secret cues" (explicitly asking what the term "secretly means") significantly improved GPT-3's performance, especially for formal registers.
5. **Demonstrating how dogwhistles evade content moderation:** Using hateful template sentences from HateCheck and Google/Jigsaw's Perspective API model, they showed that sentences containing dogwhistles were rated as less toxic compared to those with standard group labels or slurs, even when the underlying hateful message was the same.

In summary, this project provides a comprehensive understanding of dogwhistles, their historical context, and how they challenge both language models and content moderation efforts.</sample>
    <sample id="298">The researchers found that the performance of the models degraded with a larger temporal gap, which led them to conclude that temporal drift is the main cause of performance loss.</sample>
    <sample id="299">The speaker, Michalis Korakakis, presents a joint work with Andreas Vlachos on improving the robustness of Natural Language Inference (NLI) models using minimax training.

NLI models achieve state-of-the-art results on several benchmarks but learn and use "shortcuts," which are spurious correlations between input attributes and labels. These shortcuts lead to good performance on in-distribution samples but poor performance on out-of-distribution adversarial test sets where such correlations don't hold.

Prior shortcut mitigation work relies on an auxiliary model trained to exploit shortcuts, and its predictions are used to re-weight training examples for the learner. However, these methods have limitations: they may require prior knowledge of shortcuts, the learner's behavior can diverge from the auxiliary's, causing unstable training, and the auxiliary often needs to be a large pre-trained language model, incurring additional computational overhead.

Motivated by these limitations, the authors propose a minimax training method. The key insight is that NLI models struggle with "under-represented hard examples" that contradict the shortcuts found in dominant "easy examples." The loss of these hard examples decreases considerably slower during training.

The proposed approach aims to obtain an example weight distribution that emphasizes these under-represented hard examples. This is achieved by posing a minimax training objective between a learner and an auxiliary. The learner minimizes the NLI task loss, while the auxiliary maximizes the learner's loss by generating example weights that incentivize the learner to concentrate on regions of the input space with high losses. Both models are optimized alternately using stochastic gradient descent.

The advantages of this method are that it makes no prior assumptions about shortcuts, relies on the learner's training dynamics, and uses a simple feed-forward network as the auxiliary.

The main results show that minimax training consistently improves out-of-distribution performance while maintaining high in-distribution accuracy across three NLI datasets: FEVER, MNLI, and QQP, and their corresponding out-of-distribution adversarial test sets. Further experiments in the paper examine the transferability of these improvements to larger models, synthetic shortcuts, and out-of-domain test sets, the effect of pre-training the learner, the required size of the auxiliary, and a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">The speaker introduces "Interactive Dictation," a new task for users to dictate and edit documents using their voice in a natural and intuitive manner. This task diverges from existing speech-to-text systems that primarily support dictation and require memorized, inflexible vocal commands for editing.

Interactive Dictation prioritizes:
1. Flexible interleaving of dictation and editing without trigger words, posing a challenge in predicting segmentation between dictation and command utterances.
2. Intuitive and open-ended natural language for editing, presenting the challenge of interpreting which command to invoke and where.

The speaker highlights their three-fold contributions:
1. Formalizing "Interactive Dictation" as a new task.
2. Designing a data collection interface and building a dataset for this task.
3. Creating a baseline system.

The system comprises four steps: ASR for transcribing raw audio, segmentation to distinguish dictation from commands, normalization to fix ASR errors, and interpretation to execute commands, updating the document state. This process occurs in real-time as the user speaks.

A data collection interface, "TERTIUS," was developed. Annotators were instructed to either replicate emails or elaborate on terse email descriptions. The dataset consists of 1320 trajectories, with 959 dictation segments and 3225 command segments, totaling 4184 segments.

For the baseline system, separate models were trained for segmentation, ASR repair, and interpretation. Segmentation achieved 85.3% exact-match accuracy. For ASR repair and interpretation, a trade-off between runtime and accuracy was observed, with GPT3 models being more accurate but slower.</sample>
    <sample id="301">Hi everyone, I'm Jenny, a first-year PhD student at Carnegie Mellon University. And today I'll be presenting our work NLPositionality, characterizing design biases of datasets and models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katerina Reinecke, and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Perspective API for toxicity detection. And this works really well if you're Carl Jones, where Perspective API's able to detect correctly toxic instances. But that's not really the case for Aditya Sharma, where Perspective API is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models and cells and datasets themselves have demographic identities and life experiences. But they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality. However, these works really don't look at comparing end users with the datasets and models themselves. And starting model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented. And it's challenging to characterize how these positionalities are skewed because not all decisions are documented, and many models are hidden behind APIs. So, to study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework, NLPositionality. Our framework works in two main steps. The first step is to re-annotate datasets with diverse annotators. And we opt to do this over looking at the demographics of original datasets, um, annotators because usually only a few instances each instance, and because demographics are rarely collected and shared. And so we opt to re-annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score. And thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels, as opposed to looking at just annotator agreement or modeling annotator distributions. Our framework is largely enabled through LabintheWild, an online crowdsourcing platform for our HCD collaborator. LabintheWild is an online experimentation platform where we can recruit diverse volunteers, um, compared to like platforms like M Turk, which largely have participants from the US or India. And further LabintheWild still is able to get high quality data. We host two tasks on LabintheWild, one of them being social acceptability. And the way this works is that participants will read a situation from the social chemistry data set. And then they'll write how socially acceptable the situation is. Afterwards, to stay engaged in the study, they can compare their responses to an AI and others. We then compare these, um, annotations with social chemistry, Delphi, and GPT-4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dyna Hate and rate whether they think it's an instance of hate speech. We then compare these annotations with Dyna Hate, Perspective API, Rewire API, Hate RoBERTa, and GPT-4. Our study in the end amassed over 16,000 annotations from over a thousand annotators from 87 countries. So now we're better equipped to answer, who do NLP datasets and models align with the most? We find that there is positionality in NLP. For example, we find that datasets and models are most aligned to English-speaking countries. So for the GPT-4 social acceptability analysis, we find that it's most aligned to Confucian and English-speaking countries. We find that Dyna Hate is also most aligned to English-speaking countries. We also find most additional alignment with people who have a college education. So, for GPT-4 in the social acceptability task, we find that it's most aligned to people with a college education or graduate school education. And we find the same for Dyna Hate, where it's most aligned to people with a college education. However, when models and datasets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non-binary people compared to the men and women counterparts. We find this in the GPT-4 social acceptability task, as well as the Dyna Hate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices made throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialized datasets and models with and for specific communities. And a good example of this is the Masakhane initiative. I mean, we want to emphasize that inclusive NLP isn't just making, you know, all technologies work for everyone. And so that concludes our presentation, but if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="302">The first step predicts the correct tokens for the output sequence, but they are not ordered. Therefore, permuting the tokens is necessary to put them in the right order.</sample>
    <sample id="303">The authors recommended increased transparency about bias mitigation methods because without it, it's impossible to understand why positive stereotypes are being produced, or to study these "pernicious patterns" further.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are ungrammatical sentences used to evaluate language models. The hope is that the model will put more probability on the grammatically correct sentence compared to the ungrammatical one.</sample>
    <sample id="305">The speaker begins by introducing weak supervision and weakly supervised learning (WSL). Weak supervision uses weak labeling sources like heuristics or crowd-sourcing to annotate data, alleviating the annotation bottleneck. However, weak labels are noisy, and training neural networks directly on them can lead to memorization of noise rather than generalization. WSL aims to train robust models despite this noise.

A common claim in recent WSL works is that models are trained "only on weakly supervised data" and achieve high accuracy on clean test sets. However, the speaker reveals that these methods often rely on an additional "cleanly labeled validation set" for model selection, a crucial detail frequently overlooked.

This observation leads to three research questions:
1. Is clean validation data necessary for WSL?
2. How many clean samples do WSL approaches need?
3. How can available clean samples be used more efficiently?

The speaker presents findings addressing these questions. First, clean validation samples are indispensable for WSL approaches to generalize properly; without them, training is often pointless. Second, WSL approaches benefit from more clean validation samples, typically needing around 20 samples per class for high performance. However, training on these clean samples directly with finetuning approaches (like LoRA) can sometimes achieve even better performance than WSL approaches that only use clean data for validation. Third, continuous finetuning (CFT) on clean validation samples can eliminate performance gaps between different WSL approaches, suggesting that complex WSL methods might be unnecessary if CFT is applied.

In conclusion, recent WSL approaches require clean samples and often overestimate their practicality. The speaker recommends reporting model selection criteria, using few-shot learning approaches as baselines, and always applying continuous finetuning (CFT) in future WSL research.</sample>
    <sample id="306">The speaker, Sebastian Schuster, introduces a research paper on entity tracking in language models, co-authored with Najoung Kim. He emphasizes that understanding discourse requires entity tracking, using the example of a recipe where ingredients change state as the discourse unfolds. 

Schuster then discusses challenges in evaluating entity tracking abilities, such as:
1. Common entity states in pre-training data, where models might predict correctly without true tracking abilities.
2. Predictable entity states from individual words/phrases, rather than considering the larger discourse.
3. Models potentially memorizing entity state sequences or applying slot-filling heuristics during fine-tuning or in-context demonstrations.

Najoung Kim takes over to explain the task setup. They designed a task involving boxes and objects, where the language model predicts the contents of each box after a series of state-changing operations. They implemented measures to prevent models from using heuristics.

Their in-context learning experiments, using Flan-T5 and GPT-3/3.5 models, show that most models repeat the initial state, except for GPT-3.5 text-davinci-003, which exhibits non-trivial entity tracking. This suggests that pre-training on code is crucial for these capacities to surface. They also found that smaller fine-tuned models like T5-base can learn entity tracking, but randomly initialized models cannot. The generalizability of these abilities beyond their specific box setup is still unclear.</sample>
    <sample id="307">The authors used several evaluation metrics including F1 score for NER and CLS tasks, F1 score and Hamming for question answering, and EMR for question answering.</sample>
    <sample id="308">The speaker, Jenny, introduces their work titled "NLPositionality: Characterizing Design Biases of Datasets and Models". She explains that design biases, where technology performs differently across populations, can arise from the positionality of NLP researchers and model developers. Positionality refers to the perspectives people hold due to their demographics, identity, and life experiences, which can influence research processes and outcomes.

The core question is whether datasets and models exhibit positionality. While models and datasets don't have personal identities, they aggregate human judgments and opinions, thereby reflecting certain positionalities. Anecdotal evidence from prior work on model and dataset probing, and theoretical definitions, suggests this possibility. However, these studies haven't directly compared end-user annotations with existing datasets and models.

To address this, their "NLPositionality" framework re-annotates datasets with diverse annotators and then compares these demographic-specific annotations to the models and original datasets using Pearson's R scores. This differs from traditional annotator disagreement literature by focusing on end-user perspectives. They use "LabintheWild," an online crowdsourcing platform, for diverse volunteer participation.

Their study involves two tasks: Social Acceptability (using the Social Chemistry dataset, Delphi, and GPT-4 models) and Toxicity (using the Dynahate dataset, Perspective API, Rewire API, Hate RoBERTa, and GPT-4). They collected over 16,000 annotations from more than 1,000 annotators across 87 countries.

Their findings indicate that there is positionality in NLP. Datasets and models tend to align most with English-speaking countries and people with college education, while showing less alignment with non-binary individuals. To address this, they recommend: 1) meticulously recording design choices, 2) conducting NLP research through a perspectivist lens (sharing disaggregated labels and using modeling techniques that handle annotator disagreement), and 3) building specialized datasets and models for specific communities (e.g., the Masakhane initiative).</sample>
    <sample id="309">Krippendorff's Alpha was used for measuring inter-annotator agreement.</sample>
    <sample id="310">To add completely unrelated sentences, the Wikipedia domain was chosen.</sample>
    <sample id="311">The authors of the paper are affiliated with Heinrich Heine University in Düsseldorf, Germany.</sample>
    <sample id="312">MultiInstruct is the first multimodal instruction tuning benchmark dataset, which differs from prior benchmarks that focused on language-only tasks.</sample>
    <sample id="313">There are three authors.</sample>
    <sample id="314">I'm sorry, I cannot answer the question. The provided context does not offer a definition of binary coordination.</sample>
    <sample id="315">The prompts were not of a consistent length.</sample>
    <sample id="316">The T5 model, fine-tuned on the Coscript dataset, can generate higher-quality scripts than most larger language models. This indicates that smaller models, when properly trained on suitable datasets, can surpass larger models in performance.</sample>
    <sample id="317">This video presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors". Information extraction is a classic task in natural language processing (NLP) that aims to recognize structured information from plain text. For example, in Named Entity Recognition (NER), the model recognizes "Steve" as a person and "Apple" as an organization.

Previous methods for information extraction using pre-trained language models like T5 and GPT-3 operate in a text-to-text manner during pre-training. However, during inference, the structured output of information extraction is linearized into a plain sequence. This creates a mismatch between the input format used during pre-training and the structured output required during inference.

To address this problem, CodeIE transforms information extraction into a structure-to-structure code generation task, leveraging code language models like Codex. This approach allows for easy conversion of text to a structured format during input and ensures aligned structures in the output.

Experimental results show that CodeIE, using code-style prompts, significantly and consistently outperforms traditional baseline models like UIE and natural language models like GPT-3, especially in few-shot settings (1-5 shots). Further analysis indicates that CodeIE achieves better format consistency and a lower structural error rate, making it more reliable for generating structured outputs. Additionally, CodeIE is more robust to semantic errors, avoiding the generation of labels not present in the predefined label set, a common issue with GPT-3.</sample>
    <sample id="319">The work investigates two learning strategies: "from scratch" training (with full model construction) and "continual pre-training" (using an existing pre-trained model).</sample>
    <sample id="320">Based on the provided English content, it is stated that there is no diminishing returns, which implies that adaptive overfitting due to test reuse is not observed. Therefore, the factor of overfitting due to test reuse is negligible or not present in this context.</sample>
    <sample id="321">The quality of simplification was evaluated using SARI, BLEU, BS-P, and FRE metrics.</sample>
    <sample id="322">Hello everyone, I'm Enrico, and I will be presenting at ACL 23, answering the question, "What does a Text Classifier Learn about Morality?" Human morality helps us distinguish right from wrong, and it is essential that language models can understand and recognize morality in text. However, morality is very subjective. Different people would label the same concept differently on a moral scale. For example, some would label abortion as immoral, while others would label it as moral. Simply taking an average or a majority aggregation would hide the pluralistic nature of moral interpretations.

To this extent, we can apply social theories to the understanding of human morality. The Moral Foundation Theory states that there are five ways in which we perceive morality: Care, Fairness, Loyalty, Authority, and Purity. Each human prioritizes these foundations differently, which determines how we judge the morality of a concept or action. Previous research has shown that language models can somewhat understand morality in text. Our aim is to understand what they learn by applying explainable AI techniques.

We focus on understanding how morality is expressed differently across various domains. We use a dataset called the Moral Foundation Twitter Corpus, comprising 35,000 tweets from seven different domains, including those corresponding to hashtags like "All Lives Matter" (ALM) and "Black Lives Matter" (BLM). We try to see whether language models can understand that morality is expressed differently in these domains. ALM and BLM generally have a similar value rhetoric, but they differ in the element of subversion (rebellion to authority). We found that language models recognize that in ALM, subversion is associated with words like "overthrow" and "mayhem," and subversion is frowned upon. In contrast, in BLM, subversion is somewhat encouraged, associated with "encourage" and "defiance." So, language models do recognize that morality can be expressed differently, and using just one single model for many different domains can lead to dangerous misunderstandings of morality. I hope to see you at ACL in Toronto. Bye.</sample>
    <sample id="323">The speaker, Yujie Wang, presents a paper on "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering."

The problem involves retrieving knowledge subgraphs through entity matching, which introduces noisy entities. Encoding subgraphs and text in isolation leads to limited interaction, and the encoding process ignores semantic relationships.

The proposed method, DHLK, addresses these issues by:
1. Building a heterogeneous knowledge graph (HKG) from multiple knowledge bases.
2. Optimizing HKG's structure and knowledge representation using a two-stage pruning strategy and knowledge representation learning (KRL).
3. Implementing fusion and encoding of two modalities through a language model (LM).

The HKG Construction involves first-stage pruning to remove subwords and retrieving paraphrases of key entities from WordNet and Wiktionary, connecting them as additional nodes. The LM Encoder and Dynamic Pruning Module encode QA context and subgraph entities, dynamically pruning entities with weak relevance based on LM's attention weights. The KRL Module computes initial entity and relation embeddings using mean pooling and optimizes them using TransE. The RMSA Layer Module introduces relationships into Mask Self-Attention, updating entity and relation embeddings, and obtains the graph embedding by max-pooling question key entities. Finally, the KG2QA Layer incorporates path information from the HKG into the QA context, and an MLP predicts the answer probability.

Experiments conducted on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary show that DHLK achieves superior results compared to other LM and HKG methods.</sample>
    <sample id="324">Yes, language models do have different political biases and they occupy all four quadrants on the political compass.</sample>
    <sample id="326">Cognitive dissonance refers to two inconsistent beliefs or actions.</sample>
    <sample id="327">This presentation introduces ManagerTower, a novel vision-language model architecture designed to aggregate insights from uni-modal experts for vision-language representation learning. The speaker, Xiao Xu, begins by defining vision-language learning as training an AI system to understand both images and text, highlighting tasks like Visual Question Answering.

The presentation then contrasts ManagerTower with existing architectures, specifically the Two-Tower and BridgeTower models. While Two-Tower connects only the last layer of uni-modal representations to a cross-modal encoder, BridgeTower attempts to utilize uni-modal knowledge at different levels by connecting multiple top uni-modal layers to each cross-modal layer. However, BridgeTower suffers from ineffective layer-by-layer utilization and a fixed number of cross-modal layers tied to uni-modal representations, limiting its scalability and capability.

ManagerTower addresses these limitations by incorporating "managers" in each cross-modal layer. These managers adaptively aggregate multi-layer uni-modal representations, serving as insights from pre-trained uni-modal experts. This allows for more comprehensive cross-modal alignment and fusion. Experimental results demonstrate ManagerTower's superior performance on various downstream tasks, outperforming both Meter and BridgeTower, even when trained on significantly less data or with fewer parameters than some other models. The visualization of aggregation weights further shows that ManagerTower's adaptive managers exhibit diverse weight distributions, unlike the static managers, confirming their effective exploitation of uni-modal knowledge at different levels.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">This presentation focuses on zero-shot video sentence localization, a task that identifies relevant video segments based on a natural language query without requiring manual annotation for training. This is a significant challenge due to the high cost and inefficiency of manual annotation in traditional methods.

Existing zero-shot methods for video sentence localization have three main drawbacks: overly simple pseudo-queries, misalignment between pseudo-events and pseudo-queries, and the neglect of noise in pseudo-labels during training. To address these issues, a novel approach called "Structured Pseudo-Label (SPL) generation" is proposed. This method generates more complex free-form pseudo-queries using image description models. It then creates pseudo-events based on the event temporal structure, ensuring high relevance between video content within the event and the query, and low relevance outside the event.

To further enhance robustness, the proposed method incorporates techniques to reduce noise during training. This involves sample re-weighting, where noise is estimated based on confidence scores and Intersection over Union (IoU) of predictions, and label refinement, where confident predictions are considered as new pseudo-labels for subsequent training rounds.

Experiments conducted on the ActivityNet Captions and Charades-STA datasets demonstrate the effectiveness of this approach. The proposed SPL method outperforms existing zero-shot methods across various metrics, indicating its robustness to noise and superior performance in video sentence localization. The code for this research is publicly available, encouraging further exploration and development in this field.</sample>
    <sample id="330">Yes, cumulative training performs better than or equal to iterative training across all strategies.</sample>
    <sample id="331">The speaker is Sara Papi.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from transcripts of TED talks translated from English into 14 different languages.</sample>
    <sample id="333">In this presentation, the speaker introduces a novel framework called INK for improving Neural Machine Translation (NMT) models by injecting kNN (k-Nearest Neighbors) knowledge. NMT models often exhibit non-smooth representation spaces, particularly for low-frequency tokens, leading to "semantic holes" and poor generalization. The previous solution, kNN-MT, addresses this by saving representations and target tokens into a datastore and smoothing predictions with nearest neighbors. However, kNN-MT faces drawbacks: retrieving neighbors from a large datastore is time-consuming, and representations cannot be easily updated once constructed.

INK overcomes these issues with a two-step training loop: representation refinement and asynchronous refresh. In the first step, kNN knowledge from the datastore guides an adapter to adjust representations. Then, these updated representations are used to asynchronously refresh the datastore. This loop continues until convergence. Specifically, INK aligns three types of representations using KL-divergence: contextualized representations with token embeddings, contextualized representations with kNN token embeddings, and contextualized representations of the same target token.

Experiments show that INK significantly outperforms state-of-the-art kNN-MT systems, achieving an average gain of 1.99 COMET and 1.0 BLEU. Moreover, INK accomplishes this with 0.02 times the memory space and 1.9 times faster inference speed compared to kNN-MT baselines, demonstrating its efficiency and effectiveness in smoothing the representation space and enhancing NMT performance.</sample>
    <sample id="334">Hi, my name is Adam Przedbirkowski and this talk is about the dependency structure of coordination. As you may know, there are different dependency structures assumed by different theories and and cop's approaches. So, for example, in Universal Dependencies, the structure of the coordinate coordination "Lisa, Bart, and Maggie" is such that the first conjunct is the head of the whole coordinate structure. So, in this case, Lisa. Uh, a similar approach is assumed in uh Igor Melchuk's uh meaning text theory, where again, uh the whole coordinate structure is headed by the first conjunct. So, these two approaches are asymmetric, right? They uh they single out one of the conjuncts. Now, there are also symmetric approaches to coordinate structures, such as the Prague approach, the conjunction headed approach assumed in Prague dependency tree banks, where coordinate structures are headed by the conjunction. So, uh we get um, um, dependencies from "and" to all the conjuncts. And finally, there's also a multi-headed approach uh that's used for example in um, uh the Cartsons word grammar, where so to say, all conjuncts are heads of the coordinate structures. So, we get dependencies from the governor, here, loves, to all conjuncts separately, "Lisa, Bart, and Maggie." Now, the aim of this paper is to um, produce a novel argument for uh, the symmetric structures of coordination, like these two, and against the asymmetric structures of coordination, like these two. Okay. Uh, so what we did, we extracted various statistics from uh about coordination from the enhanced version of the Pen Treebank. And see the paper why we didn't use uh Universal Dependencies. And uh these statistics confirm the observation made many times before that left conjuncts tend to be shorter, uh, so "salt and pepper" and not "pepper and salt" measured in syllables. Uh, and uh, also the observation that was made in passing that uh this tendency grows with length difference. So, uh when the difference between the lengths of the two conjuncts uh grows, uh the shorter conjunct prefers to be the first one stronger, right? So, the proportion is is is bigger of of the left uh short conjunct. But what's novel uh in in this paper is uh we are that we observed that this tendency only occurs when the governor is on the left or absent, right? So, the governor is on the left in this example, "I saw Bart and Lisa," so "saw" is the governor is on the left. Uh, it's absent in the second example, "Homer came and sneezed." Here, we have coordination of two verbs and there's no outside external governor, right? So, in such cases, uh the left conjunct prefers to be shorter, the more so the uh the bigger the difference uh between the two conjuncts. However, when uh the governor is on the right, as here, "laughed," governed the coordination "Ted and Ned," uh this effect disappears. So, we show that um, uh by measuring length in characters, that's the first column, in syllables, the middle column, and in words, the right column. So, I'll concentrate on the right one. What we see here is that uh when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily uh with the absolute difference in words. And the same is observed when there's no governor, as in coordination of sentences. But when the governor is on the right, this tendency disappears. And uh we show in the paper how this uh provides an argument against uh asymmetric structures of coordination, as these two, and for the symmetric structures as these two. So, see the paper for the full argument. And uh arguments, sorry, and talk to us about uh at the poster session. Thank you.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer is a technique where a model is trained on one source language and then transferred to another language for inference. This can involve training on a single source language, or on a combination of different languages.</sample>
    <sample id="337">This presentation introduces a novel approach called Graph-based Relation Mining (GRM) for context-free Out-Of-Vocabulary (OOV) word embedding learning. The speaker explains that OOV words are difficult to represent but are crucial for the performance of embedding-based downstream models. Inspired by how humans learn new words by observing their formation and associating them with relevant known words, GRM utilizes a Word Relationship Graph (WRG).

The WRG models the lexical rules of word formation and association. When an OOV word appears, it is tokenized into word pieces and associated with relevant words, forming a two-level graph. The GRM model architecture addresses the issue of assigning node attributes to OOV words by employing a self-attention network for initialization. It then uses two levels of Graph Attention Networks (GAT) to process the WRG, concatenating and fusing initial inputs with hidden embeddings of each layer to produce a node-level representation. Finally, a Graph Convolutional Network (GCN) extracts a graph-level representation, and contrastive learning is used to pull similar words together while pushing dissimilar words apart in the embedding space.

Extensive experiments demonstrate that GRM outperforms state-of-the-art baselines in both intrinsic and extrinsic evaluation tasks, proving its effectiveness in learning OOV words through word formation. The model also shows adaptability to both static and contextual models in downstream tasks, suggesting its potential for application in agglutinative and fusional languages by reasonable word segmentation.</sample>
    <sample id="338">The speaker introduces their research titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations." The research is a collaborative effort between Rensselaer Polytechnic Institute, Northeastern University, and IBM Research.

The presentation outlines the motivations behind the research, discusses related works, and highlights the primary contributions, including a unified structure, preliminary experiments, and a metric for evaluation. The work also delves into future research.

The primary motivation is the use of human natural language explanations to train natural language generation (NLG) models, boost prediction performance, and enhance model reasoning. However, evaluating human-annotated explanations is subjective and lacks a gold standard. Existing NLG metrics like BLEU and ROUGE treat human annotations as a gold standard, focusing on word similarity, while the Simulatability Score only measures baseline model performance change when explanations are present or absent.

The researchers address these limitations by introducing a template-based unified data format that converts various tasks into a multiple-choice task. This allows for baseline training (without explanations) and infusion training (with explanations as input) for sequence-to-sequence models.

Preliminary experiments on ECQA and CoS-E datasets show that fine-tuning with explanations teaches models to rely on the explanation part of the input for prediction, rather than introducing new knowledge. CoS-E explanations were found to be less helpful than ECQA for baseline models, emphasizing the task-dependent nature of explanations. However, fine-tuning with even a small amount of data incorporating explanations can significantly improve model performance.

Based on these observations, they propose a new evaluation metric called TREU, which extends the Simulatability Score by evaluating the helpfulness of explanations during fine-tuning. Their evaluation across five datasets and two models (T5 and BART) demonstrates that TREU can faithfully reflect the helpfulness of human-annotated explanations, even if humans rate them as low quality. TREU consistently ranks data quality across different datasets and models, whereas Simulatability Score falls short and is more affected by model variations.

The research suggests that the helpfulness of human explanations depends on the task and explanation style. The work aims to lay a foundation for high-quality human-AI collaboration in annotation jobs and recommends similar quality checks for future human explanation collection.</sample>
    <sample id="339">The authors of the paper are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="340">The speaker introduces ParaAMR, a large-scale, syntactically diverse paraphrase dataset created through Abstract Meaning Representation (AMR) back-translation. ParaAMR aims to address the limitations of existing paraphrase datasets, which are either high-quality but limited in scale (human-annotated) or large-scale but lack syntactic diversity (automatically generated through back-translation).

The key idea is to leverage AMR graphs, which capture the abstract meaning of a sentence. The process involves:
1. Parsing a source sentence into an AMR graph.
2. Modifying the AMR graph by changing its focus (root node) and corresponding edge labels. This manipulation generates multiple semantically similar but syntactically distinct AMR graphs.
3. Using an AMR-to-text generator to convert these modified graphs back into natural language sentences, thus creating syntactically diverse paraphrases.

ParaAMR comprises around 15.5 million source sentences, with an average of 6.92 paraphrases per sentence. Quantitative analysis, including automatic and human evaluation scores, demonstrates that ParaAMR achieves similar semantic similarity to other datasets while significantly increasing syntactic diversity.

The dataset's utility is showcased in three applications:
1. **Learning Sentence Embeddings:** Sentence embeddings learned from ParaAMR show superior performance in Semantic Textual Similarity (STS) benchmarks compared to other datasets.
2. **Syntactically Controlled Paraphrase Generation:** Models trained with ParaAMR exhibit better syntactic control in paraphrase generation tasks.
3. **Data Augmentation for Few-Shot Learning:** ParaAMR's diverse paraphrases effectively augment data for few-shot learning scenarios, leading to higher scores in various NLP tasks (MRPC, QQP, RTE).

ParaAMR is publicly available on GitHub.</sample>
    <sample id="341">The authors use BLEU as a quality measure, and average lagging (AL) and computationally aware average lagging (AL_CA) as latency measures.</sample>
    <sample id="342">The speaker introduces LiveChat, a large-scale personalized dialogue dataset constructed from live streaming. The dataset addresses several key barriers in existing dialogue systems, including the lack of large-scale video-sourced dialogue corpora, scarcity of detailed persona information, longer conversations for personalized dialogue, and the scarcity of Chinese multi-party dialogue corpora. LiveChat tackles these issues by collecting live streaming videos from Douyin (Chinese TikTok), extracting audio from videos, transcribing audio into utterances using ASR, and collecting audience comments to construct dialogues through a reply-to-whom matching method. Persona information for personalized dialogue generation is collected by a combination of manual labeling of basic profiles and rules-based extraction with a trained persona classifier for text profiles.

The speaker presents experimental results showing that LiveChat is a video-sourced and large-scale dataset, with detailed persona annotations and longer average sessions compared to other existing open-domain dialogue datasets. The experiments on two benchmark tasks, Response Modeling and Addressee Recognition, demonstrate the benefits of extracted persona information and longer average sessions. The transfer learning performance of pre-trained dialogue models and LLMs on LiveChat reveals the distinctiveness of this video-sourced dialogue domain. Future work will focus on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="344">The drawbacks of tree-based methods are that trees are not usually given and need to be obtained through pre/post-processing or grammar induction.</sample>
    <sample id="345">This presentation introduces a novel approach to compositional generalization in semantic parsing, addressing the limitations of traditional sequence-to-sequence models that struggle with out-of-distribution generalization. Existing methods often rely on tree structures to represent the compositional process, which can be computationally expensive and require specific pre-processing or grammar induction.

Our proposed model eliminates the need for explicit tree structures by employing a two-step process: multiset tagging and latent permutation. In the first step, each input token is tagged with an unordered multiset of tokens representing the output. This ensures all necessary output tokens are generated. The second step utilizes a separate model to predict a permutation that orders these multiset tokens correctly, thus forming the final logical form. This permutation model is designed to be flexible, imposing no hard constraints on possible permutations.

A key technical challenge addressed is the unknown alignment between input and output tokens during training, as well as the latent nature of the linguistically correct permutation when multiple consistent permutations exist. We tackle these issues by inducing the alignment during training and approximating the NP-hard permutation inference with a GPU-friendly continuous relaxation, which also allows for backpropagation to learn more plausible permutations. Experimental results on the COGS benchmark demonstrate that our model significantly outperforms other treeless methods in generalizing to deeper recursion, showcasing its potential for strong compositional generalization without the complexities of tree-based approaches. However, some types of structural generalization remain challenging.</sample>
    <sample id="346">Shuheng Liu and Alan Ritter are affiliated with the School of Interactive Computing at the Georgia Institute of Technology.</sample>
    <sample id="348">Myra Cheng presents "Marked Personas," a paper on using natural language prompts to measure stereotypes in language models. Current methods for measuring stereotypes in large language models (LLMs) have limitations, including a tradeoff between specificity and generalizability, reliance on fixed, hand-curated datasets, and a failure to account for intersectionality. To overcome these, Cheng's research leverages the ability of instruction-tuned LLMs like GPT-3.5 and GPT-4 to respond to prompts, generating "personas" of imagined individuals. The prompts, such as "Imagine you are an Asian woman. Describe yourself," are generalizable and can evaluate any intersectional identity.

The research involves two steps: first, generating personas using these prompts (inspired by a psychology study with human subjects); second, using "marked words" to identify words that distinguish personas of marked groups from unmarked groups. This method provides specific stereotype patterns without requiring a lexicon. The concept of "markedness" in sociolinguistics suggests that unmarked groups are default and ordinary, while marked groups differ from this default.

Results show that generated personas contain more stereotype words than human-written ones. However, the existing stereotype lexicons are incomplete, often missing the nuances of positive yet pernicious portrayals. Cheng's analysis using the marked words method reveals harmful patterns such as "othering" through essentializing narratives (e.g., words like "culture," "tradition," "proud," "exotic" for marked groups) and "pernicious positive portrayals" (e.g., "vibrant, curvaceous" for Latina women, "petite, delicate, silky" for Asian women, and "strong, resilient" for Black women). These positive portrayals can be harmful as they connect to stereotypes like tropicalism or the "strong Black woman" archetype, which may put undue pressure on individuals. The research concludes with recommendations for model owners, including addressing positive stereotypes and essentializing narratives, adopting an intersectional lens, and increasing transparency about bias mitigation efforts.</sample>
    <sample id="350">In this presentation, the speaker discusses the increasing trend of leaderboard-based evaluations in Natural Language Processing (NLP), where systems often achieve "superhuman" performance, leading to claims that certain tasks have been "solved." However, the speaker argues that such claims are not yet grounded due to several issues.

First, models are brittle and exhibit poor out-of-domain generalization, vulnerability to adversarial attacks, reliance on spurious patterns, lack of sensitivity to linguistic perturbations, and over-sensitivity to irrelevant prompts. To assess the reliability of leaderboard scores, two popular benchmarks, SuperGLUE and SQuAD, were analyzed. SuperGLUE shows human baselines outperformed on 6 out of 10 tasks, with significant margins in some. Similarly, humans are largely outperformed on both SQuAD1.1 and 2.0.

Upon inspection, several sources of error were found in human-to-system comparisons. Systems and humans are often evaluated on different test sets, with humans typically assessed on much smaller subsets. Ground-truth answers contain errors, making fair comparisons difficult. Additionally, human performance metrics are often vaguely estimated using simple aggregation methods, rather than comparing against the best possible human performance. Lastly, heterogeneous and sometimes unknown pay rates for human annotators can lead to low motivation and, consequently, low-quality data. The speaker concludes that, without addressing these issues, claims of superhuman performance are scientifically meaningless and emphasizes the need for fairer and more transparent benchmarks.</sample>
    <sample id="351">The presenter, Shuheng Liu, discusses a paper that investigates the problem of generalization in Named Entity Recognition (NER) models. He highlights that NER models have been developed using the CoNLL-2003 dataset for nearly two decades, raising questions about their ability to generalize to modern data and what factors contribute to good generalization.

To address these questions, the team created the CoNLL++ dataset, comprising Reuters news from 2020 annotated with CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both the original test set and CoNLL++. Generalization was assessed by calculating the percentage change in F1 score.

Their findings indicate three key ingredients for good generalization: a better model architecture (transformer models generalize better), larger model size (larger models generalize better), and more fine-tuning examples (more examples lead to better generalization). These factors are interconnected, and a holistic approach to their improvement is necessary.

Regarding performance drops, two hypotheses were considered: adaptive overfitting (overfitting due to repeated use of the same test set) and temporal drift (performance degradation from increasing temporal gaps between training and test data). The experiments showed no evidence of diminishing returns, suggesting adaptive overfitting was not the cause. Instead, they found that performance degrades with a larger temporal gap, confirming temporal drift as the main cause of performance drop.

In conclusion, CoNLL-2003 taggers still work in 2023, but better model architecture, larger model size, and more fine-tuning examples are crucial for good generalization. The performance drop is primarily attributed to temporal drift, not adaptive overfitting. The paper advocates for further research into improving model generalization.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">The presentation introduces a paper titled "Python Code Generation by Asking Clarification Questions," which addresses the challenge of "input underspecification" in natural language descriptions for code generation. This issue is prevalent in real-world scenarios and impacts the training of code generation models. 

The authors propose an interactive approach, where a system can ask clarification questions (CQs) to gather more specifications, thereby alleviating the underspecification problem. Their method focuses on clarifying "operation-level specifications." They also present "CodeClarQA," a synthetic dataset with clarifications on key operations, and a pipeline for code generation driven by CQs. This pipeline includes a clarification need predictor, a CQ ranker, and a code generator.

The results show that their method for identifying missing key operations works well, with MPNet achieving the best performance among the models tested. Error analysis reveals common issues like "taxonomy" (aligned operations needing clarification due to similar names) and "argument" (missing argument values in documentation). 

Finally, the authors demonstrate that asking CQs helps code generation by providing more specifications, leading to better-quality outputs, as supported by the code generation results.</sample>
    <sample id="354">Based on the graph, the performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until **2016**.</sample>
    <sample id="356">The authors are affiliated with the University of Edinburgh, Saarland University, and the University of Amsterdam.</sample>
    <sample id="357">The speaker's name is Siyu Yuan.</sample>
    <sample id="358">There are 5 authors mentioned on the title slide.</sample>
    <sample id="359">The approach is compared to CAAT, which is a dedicated simulST architecture.</sample>
    <sample id="360">Hello everyone. My name is Yinn and my colleague Zhiyong and I will be presenting our research on MultiInstruct, improving multi-modal zero-shot learning by instruction tuning. So, with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language-only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction datasets between NLP and multi-modal. There exists more than 1,600 language-only instruction tasks. However, there is no large-scale, publicly-available multi-modal instruction task. Therefore, this motivated us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks, covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed datasets, we take OFA, a unified multi-modal pre-trained model as our base model. OFA use a unified vocabulary for language, image tokens and the coordinate of a bounding box. Here we show some example instances from our MultiInstruct dataset. To unify the processing of a various input and output data type, we followed the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction and bounding boxes are represented in the same token space. Okay, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from nine group for training and we sample 10,000 instance per task. For testing, we reserve the entire commonsense reasoning group for testing and we select additional five task from VQA and miscellaneous group. We use all the instance in the test split for each task. In addition, we randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP. So we use a pre-trained OFA-Large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test, for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and max performance and the standard deviation of the performance across all five experiments. If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP tasks, we report Rouge-L as well. We also introduced an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task, regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also, transfer learning from Natural Instruction dataset can benefits instruction tuning. Here we can see as the amount of task increase, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus five instruction. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategy on the model's sensitivity. As we can see, by transfer learning from Natural Instruction dataset, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from Natural Instruction dataset can help OFA to achieve much better performance on the Natural Instruction dataset. So overall, we are propose the first large-scale multi-modal instruction tuning dataset. We significantly improve the zero-shot capability of OFA and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing. We are collecting a much larger multi-modal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. So this is a QR code for our data and model. Thank you.</sample>
    <sample id="361">The speaker, Armineh Nourbakhsh from Carnegie Mellon University, introduces CounterComp, a project focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning in question-answering tasks. She explains that current state-of-the-art neural models struggle with these tasks, especially when the output involves more than two steps, due to memorizing spurious patterns.

To address this, CounterComp leverages the observation that certain question components are interchangeable and can lead to changes in the output program. By treating a training sample as an anchor, they mine positive and negative counterfactual examples from the dataset. A positive example would be a change in the question that doesn't alter the output program, while a negative example would yield a change.

CounterComp then utilizes these triplets to incorporate an auxiliary metric learning loss into the training procedure. This loss has a dynamic margin that measures the extent of change or intervention in the questions between each pair. The results demonstrate that adding this auxiliary loss consistently improves the performance of three state-of-the-art baselines on in-distribution samples, particularly for programs with more than two steps. More importantly, it enhances performance on out-of-distribution samples, whether the models are tested on different datasets or on unseen programs from the same dataset. Qualitatively, CounterComp helps models attend to more meaningful operational terms in the output during training.</sample>
    <sample id="0">Language models are trained on large-scale web-crawled data, including political news media.</sample>
    <sample id="1">The affiliations of the authors are McGill University/Mila and Microsoft Research.</sample>
    <sample id="2">The speaker introduces their team's paper on document understanding, focusing on visually rich documents like forms, receipts, and posters. They highlight that existing pre-training models for document understanding often struggle with reading order issues. Their proposed multi-modal pre-training model, LayoutMask, addresses these issues by utilizing a local 1D position instead of a global 1D position, in conjunction with 2D position and semantic information.

LayoutMask employs two novel masking strategies: Whole Word Masking (WWM) and Layout Aware Masking (LAM). WWM masks entire words, forcing the model to infer them from a broader context, thereby enhancing text-layout interactions. LAM assigns a higher probability of being masked to the first and last words of each segment, encouraging the model to infer global reading orders across segments. This approach allows the model to learn better layout representations by utilizing both semantic and spatial inferences.

Experimental results show that LayoutMask, particularly with local segment 1D positions, outperforms global 1D positions on FUNSD and SROIE datasets, and performs comparably on the CORD dataset. The performance gap is more pronounced in cases like entity totals in receipts, where traditional global 1D ordering may fail to recognize misaligned numbers with the same content as the ground truth. This indicates that LayoutMask, with its local 1D positioning and novel masking strategies, is more adaptive to such complex document structures and improves document understanding.</sample>
    <sample id="4">The speaker's name is Kayo Yin.</sample>
    <sample id="5">They used the T5 XL model to obtain the 82%-87% accuracy.</sample>
    <sample id="6">Jian Wang presented a paper on "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" at ACL 2023. The work proposes a new, more general setting called Many-to-many Summarization (M2MS), which aims to build a single model that can process a document in any source language and generate a summary in any target language. This unifies Multi-Lingual Summarization (MLS) where the input and output languages are the same, and Cross-Lingual Summarization (CLS) where they are different.

Preliminary studies conducted using the WikiLingua dataset and an mBART-50 model showed that M2MS could help the model transfer knowledge across different languages more effectively than previous MLS and CLS approaches. The experiments compared four models: mBART (ONE) with separate models for each direction, mBART (U-CLS) unified with cross-lingual samples, mBART (MLS) unified with monolingual samples, and mBART (M2MS) as the proposed new setting. The results indicated that the multi-lingual model trained in the M2MS setting achieved better knowledge transfer.

The paper also introduces PISCES, a pre-trained M2MS model developed through a carefully designed three-stage pre-training process:
1. Meta pre-training: The model generates original sentences based on their noisy counterparts.
2. Cross-lingual pre-training: The model generates sentences in the target language based on noisy parallel sentences in the source language.
3. Task-specific pre-training: This stage utilizes pseudo many-to-many summarization samples to further train the model.

Ablation studies confirmed the effectiveness of each pre-training stage, and human studies demonstrated the superiority of PISCES over mBART-50 and mT5 in terms of informativeness, conciseness, and grammaticality. The experimental results consistently showed that PISCES outperforms previous strong baselines like mBART-50 and mT5 across various zero-shot and resource-rich scenarios.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="8">The proposed human evaluation method, ABC-Eval, reduces the subjectivity of human evaluations by explicitly annotating whether each model response expresses certain behaviors like irrelevant information or self-contradiction, rather than using traditional Likert scale methods. This makes it more precise and reliable, leading to higher inter-annotator agreement and better predictive validity for dialogue quality.</sample>
    <sample id="9">The success of the existing weakly supervised approach heavily relies on the availability of clean labeled data for model selection and validation.</sample>
    <sample id="10">The presented model has an accuracy of ~60% when it only has access to entity names, which leaves a lot of room for improvement.</sample>
    <sample id="11">Jack Hessel from AI2 introduces "Do Androids Laugh at Electric Sheep? Humor Understanding Benchmarks from The New Yorker Caption Contest." The paper explores large language models' (LLMs) ability to generate and explain jokes. While models like ChatGPT can generate simple jokes and PaLM can explain them, their true understanding of humor remains questionable. Hessel highlights a humorous instance where ChatGPT failed to produce a sensible "punny" knock-knock joke.

To delve deeper into this, the team utilized "The New Yorker Caption Contest," where a captionless cartoon is published weekly, and readers submit their best captions. They operationalized this into three tasks: matching, quality ranking, and explanation generation. For over 700 cartoons spanning a decade, they collected locations, descriptions, uncanny highlights, entity links, and joke explanations.

Initial results show that their best model, CLIP fine-tuned on the annotated corpus, achieves 62.3% accuracy in matching, significantly better than the 20% random guessing baseline. However, human performance on this task is 94%, indicating a substantial gap in humor understanding. When testing GPT-4 by conditioning it on human-authored image descriptions, it achieved 84.5% matching accuracy, suggesting that it performs better when provided with a detailed understanding of the image.

Regarding explanation generation, human evaluations reveal that human-authored explanations are preferred over 5-shot GPT-4 explanations in more than two-thirds of cases, further underscoring the gap in humor understanding. The dataset, leaderboard, and models are available at capcon.dev, inviting further research into when AI might truly "understand" humor.</sample>
    <sample id="12">Six authors are involved in the paper.</sample>
    <sample id="13">The speaker introduces Adaptive Inference as a method to reduce the inference time of large language models. This method leverages the fact that real-world data varies in complexity, allowing for the use of low-capacity models for easier samples, thereby reducing average inference costs. The two main types of Adaptive Inference are Multi-Model and Early-Exit. Multi-Model involves training and sequentially running multiple models, each with a classifier at the end, until a decision is made. Early-Exit, on the other hand, trains a single model with multiple classifiers at intermediate transformer layers, halting computation once a classifier makes a decision.

The speaker then compares these two methods, noting that Multi-Model is versatile and easily extended but expensive to store and incurs overhead. Early-Exit offers faster, more memory-efficient inference but shares model parameters among all classifiers, which can degrade performance due to "conflicting gradients." This hypothesis is tested by comparing individual Early-Exit classifiers with separate Multi-Model classifiers using BERT. Results show Multi-Model classifiers outperform Early-Exit by 2.3% on average, with the largest gap for the earliest classifiers (5.2%). This suggests that conflicting gradients negatively impact Early-Exit's performance, especially at early layers.

To address this, the speaker presents SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method for Early-Exit architectures. SWEET trains an Early-Exit architecture where each layer receives updates only from its following classifier, thus avoiding conflicting gradients. The results show that SWEET largely closes the performance gap between Early-Exit and Multi-Model, particularly at faster inference speeds. For BERT-large, SWEET even outperforms both methods across the entire speed-accuracy curve. The speaker concludes by emphasizing the discovery of conflicting gradients, the first fair comparison of Early-Exit and Multi-Model methods, and the introduction of SWEET, which motivates future research into tailored fine-tuning algorithms for Early-Exit architectures.</sample>
    <sample id="15">There are three authors involved in the paper.</sample>
    <sample id="16">Bible texts are simplified more than news or language learner texts.</sample>
    <sample id="17">This presentation introduces a multimodal relation extraction (MRE) task, which aims to identify semantic relationships between entities in both text and image data. This is crucial for real-world scenarios like social media, where information is often mixed-modal and text is short.

The presenters highlight two main problems:
1. **Internal-information over-utilization**: Only parts of the text are useful for relation inference, and images don't always add new content. A fine-grained information pruning method is needed.
2. **External-information under-exploitation**: Short texts and low-relevant images can lead to information deficiency. Additional semantic supplementary information, like topic information, is needed.

To address these, they propose a novel framework consisting of five parts:
1. **Scene Graph Generation**: Representing text and image with Textual Scene Graph (TSG) and Visual Scene Graph (VSG) respectively.
2. **Cross-modal Graph Construction**: Merging TSG and VSG into a unified Cross-modal Graph (CMG).
3. **GIB-guided Feature Refinement**: Screening the CMG by fine-grained pruning of features and adjusting edges using a Graph Information Bottleneck-like principle to filter out task-irrelevant nodes and ensure informative adjustments.
4. **Multimodal Topic Integration**: Enriching CMG features with multimodal topic features retrieved through an attention operation to integrate topic embeddings and enrich the overall context.
5. **Inference**: Predicting the relation label.

Experimental results on a widely used MRE dataset show that their model achieves the best performance compared to text-based and other multimodal baselines. An ablation study further confirms that both information screening and external information exploitation significantly contribute to task performance, with scene graphs being beneficial for structural modeling of multimodal inputs. Their analysis suggests that information screening is more important when text-vision relevance is high, while external information exploitation is more useful when cross-modal relevance is low.</sample>
    <sample id="18">The example given for the preference for shorter left conjuncts is "I saw Bart and Lisa" versus "Homer came and sneezed." It indicates that when there is a governor to the left or an absent governor, the left conjunct tends to be shorter, especially with a larger length difference between the conjuncts.</sample>
    <sample id="19">The presenter, Shangsi Chen, a master's student from Shenzhen University, introduces their work, "A Survey for Efficient Open-Domain Question Answering," which was accepted by ACL 2023. She discusses the two-stage framework for open-domain question answering (ODQA), which involves a retriever to find relevant evidence from a Wikipedia corpus and a reader to understand the question and answer it.

The presenter highlights the challenges of ODQA tasks due to the large size of the Wikipedia corpus (26 million documents, 13 GB), the resulting 65 GB index file, and the use of multiple language models with millions of parameters. This leads to high memory costs, slow inference, and difficulty in deploying on resource-constrained devices.

The motivation for their work is to achieve efficient ODQA systems with smaller memory costs, faster inference, and comparable performance, making them friendly to resource-constrained devices. They summarize efficient techniques for existing ODQA systems, including:
- **Searching evidence fast:** Using approximate nearest neighbor (ANN) search methods like inverted file (IVF), locality sensitive hashing (LSH), and hierarchical navigable small world graphs (HNSW).
- **Reading fast:** Employing skip reading, such as Adaptive Computation (AC), to early stop reading contexts less likely to contain the answer.
- **Reducing index size:** Through document filtering (e.g., from 21 million to 13 million documents), dimension reduction (e.g., from 768 to 196 dimensions), and product quantization (e.g., from float 32 to int 8).
- **Reducing model size:** By using lightweight models (e.g., MobileBERT), parameter sharing (e.g., ALBERT), and fewer models (e.g., one model for multiple sub-tasks).

A comparative analysis of existing ODQA systems (extractive-reader, generative-reader, retriever-only, and generator-only) is presented, showing the trade-offs between model size, index size, and exact match (EM) on NQ (Natural Questions) and Q/s (queries per second).

In conclusion, for resource-limited scenarios, reducing index size via generator-only systems or embedding compression, or reducing model size via knowledge distillation or one-stage models, are good choices. For real-time feedback, retriever-only systems are suitable. For a balanced trade-off among performance, memory, and speed, retriever-reader systems are more appropriate. Future work includes deploying ODQA systems in low-power devices and considering more evaluation metrics like money, training data, power consumption, and carbon emissions.</sample>
    <sample id="20">Yes, the models, dataset, and training scripts are all freely available under the MIT license for research use.</sample>
    <sample id="21">DEplain-apa contains news texts.</sample>
    <sample id="22">Good generalization is achieved through better model architecture, larger model sizes, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrett discusses improving text-to-image models for visual text rendering. He highlights the challenge these models face in accurately representing text, despite their advancements in generating high-quality images.

Using the Imagen model as an example, Garrett explains that text is encoded using a T5-XXL model, which tokenizes input text into sub-word units. This sub-word tokenization often leads to poor spelling accuracy, especially in smaller T5 models (Base and Large), which score below 20%. Even the largest T5-XXL model only achieves under 70% accuracy.

In contrast, PaLM models show better spelling accuracy, approaching 100% with larger versions, but are significantly more complex. The speaker then introduces ByT5, a character-aware text encoder that processes individual bytes of the input string. ByT5 demonstrates near-perfect spelling accuracy across all model scales, indicating that direct character-level information is crucial for accurate text rendering.

Garrett explains that sub-word tokenization causes T5 models to struggle most with high-frequency words because these words are often represented by single, atomic tokens, requiring the model to decompose them into individual letters for rendering.

To improve text rendering, the team augmented the Imagen model by concatenating the T5-XXL text encoding with an additional character-level encoding from a small ByT5 model. This small addition (a 5% increase in the text encoder’s parameter count) significantly improved the model's ability to render text, enhancing fidelity, alignment, and overall text quality in generated images.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by:

* **Characters**
* **Syllables**
* **Words**</sample>
    <sample id="25">The experiments involved extracting statistics about coordination from an enhanced version of the Penn Treebank. These statistics confirmed that left conjuncts tend to be shorter, a tendency that grows with length difference. Crucially, this tendency was observed only when the governor was on the left or absent, and not when it was on the right.</sample>
    <sample id="26">The baseline classifier, training on imbalanced data, performs not much better than chance.</sample>
    <sample id="27">There are four authors involved in the paper.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Context-aware models perform significantly better on formality and lexical cohesion.</sample>
    <sample id="30">This video introduces LLM-Blender, a simple yet effective ensemble learning framework for large language models. The key idea of LLM-Blender is based on pairwise ranking and generative fusion. Instead of using a single best model for all inputs, LLM-Blender utilizes a two-stage approach. First, it runs multiple LLMs in parallel to generate candidate outputs. Then, it uses a PairRanker module to compare each pair of candidates and rank them. Finally, it takes the top-ranked candidates and uses a GenFuser model to generate the final output. The researchers also developed MixInstruct, a new dataset for evaluating ensemble learning frameworks, which contains 110k instruction-following examples from various sources. Experimental results show that LLM-Blender consistently outperforms individual LLMs and other ranking methods. The framework and dataset are available for public use, encouraging further research and development in ensemble learning for LLMs.</sample>
    <sample id="31">The authors of the paper are affiliated with Johns Hopkins University, Purdue University, MIT, and Meta AI.</sample>
    <sample id="32">Hi. My name is Matthias Lindemann and today, I'm going to give you a brief introduction to our paper on compositional generalization without trees, using multiset tagging and latent permutations. This is joint work with my advisors, Alexander Koller and Ivan Titov. Compositional Generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, 'The girl slept' and 'Mary knew that the girl slept.' These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don't use trees, and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multi-set of tokens that will appear in the output. After the first step, we have all the right tokens, but they're not ordered. That's why in the second step, we use another model to predict a permutation to put them into the right order. We introduced a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multi-set token to put in every position. For the first output position, we simply select one as highlighted in red. Then, we jump to the next multi-set token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results. Here we compare our method with other tree-less models on the COGS benchmark. Our model out-performs the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multi-set it came from, which poses a challenge for training. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the Traveling Salesman problem. We approximate this with a GPU friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="33">The framework quantifies positionality by:
1. Re-annotating datasets with diverse annotators to collect demographic data.
2. Comparing annotations from different demographic groups with existing dataset labels and model predictions using Pearson's R scores.</sample>
    <sample id="34">The speaker introduces CREST, a joint framework for rationalization and counterfactual text generation. This framework aims to explain a classifier's decision by providing both selective rationalization (highlighting important input tokens) and counterfactual generation (editing specific parts of the input to change the decision).

CREST-Generation involves a trainable masker that produces meaningful rationales from an input. These rationales are then used to mask the original input, which is prepended with the target label and passed to an editor (a masked language model). The editor fills in the masked spaces, generating a counterfactual example.

Experiments show that CREST produces high-quality counterfactuals, with human evaluators rating CREST counterfactuals as more valid and natural than those from other automatic approaches.

The framework can also be leveraged for rationalization. CREST-Rationalization uses both factual and counterfactual inputs, passed through a shared trainable masker and predictor, with an agreement regularization term to encourage similarity between the generated rationales and the initial counterfactual-guided maskings.

This approach improves model accuracy, particularly for out-of-domain datasets. Moreover, rationales generated by CREST-Rationalization are more plausible and achieve higher counterfactual simulability compared to other methods, indicating their effectiveness in changing classifier decisions when used to guide contrastive edits.

In conclusion, CREST bridges the gap between selective rationalization and counterfactual generation by producing valid, fluent, and diverse counterfactuals in a controllable way, leading to plausible explanations and achieving high counterfactual simulability.</sample>
    <sample id="35">Hello, I'm Dawei, a PhD student at Saaland University in Germany. In this video, I would like to present our recent work, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weak annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise, so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning. A common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there's a catch, which is that people do assume that there's an additional clean validation set available for model selection. We cast out on this problem setting, as this implies that additional manual annotations are required in weakly supervised learning. But, like an elephant in the room, this necessity is often overlooked. The aforementioned doubt leads us to ask three research questions. First, is clean validation data necessary for WSL, or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work, and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop, as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continuous fine-tuning on the clean validation samples. As we can see from the figures, the Valena model, termed FTW, initially underperforms more complicated WSL methods like Cosine. However, if we allow to continue fine-tuning on the clean samples, then FTW performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows: First, report the model selection criteria. For example, report if the model selection is done on clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you, and enjoying the conference.</sample>
    <sample id="36">Telmo Pessoa Pires introduces the topic of "Learning Language-Specific Layers for Multilingual Machine Translation," a collaborative effort with Robin Schmidt, Yi-Hsiu Liao, and Stephan Peitz. He begins by outlining the advantages of multilingual machine translation: scalability (easier to train and maintain a single model), speed (direct translation between any two languages, avoiding pivot languages), less error cascading, and improved performance for low-resource language pairs.

However, he acknowledges the challenge of limited capacity per language within a single model. To address this, their solution involves "Language-Specific Layers (LSLs)," where each language has its own transformer layer, ensuring that only the relevant sub-layer is activated during inference, keeping inference costs constant.

For LSL placement, instead of manual trial and error, they devised an approach where the model learns the best placement. For each encoder layer, they incorporate three weights: a shared weight, a source weight, and a target weight. These weights determine which component—shared, source-specific LSL, or target-specific LSL—is prioritized at each layer. After training, the model's learned architecture revealed that bottom layers are shared, followed by source-specific LSLs, then more shared layers, target-specific LSLs, and finally, a shared top layer.

Their experiments used WMT21 news translation task sources for 10 languages and were evaluated on Flores-101 using chrF, spBLEU, and COMET metrics. The LSL-NAS approach, particularly with dense pre-training, showed significant improvements over both larger baselines and adapter approaches, while maintaining fewer parameters for forward passes during inference. The per-language results confirm improvements for every language, with particularly large gains for low-resource languages, demonstrating statistically significant improvements in 84 out of 90 translation directions.</sample>
    <sample id="37">The previous study found that when human subjects were given the same persona prompts, they were able to surface racial stereotypes.</sample>
    <sample id="38">The study used an enhanced version of the Penn Treebank.</sample>
    <sample id="39">Two authors, Adam Przepiórkowski and Michał Woźniak, are involved in the paper.</sample>
    <sample id="40">The presenter states two tasks closely related to cognitive dissonance: topic-independent dissonance stance classification and binary classification of expansion and comparison classes of PDTB.</sample>
    <sample id="41">This presentation introduces PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives, a collaborative effort between EPFL and Sony Group Corporation.

Persona-grounded narratives are essential for dialogue or storytelling agents to understand how speakers, listeners, or characters shape the narrative. For instance, an architect might discuss outdoor explorations with friends but architectural design ideas with colleagues. This highlights the need for rich world knowledge and interconnections for real-world personas.

PeaCoK, a persona-grounded commonsense knowledge graph, aims to represent such world-level persona knowledge at scale. It contains approximately 100K persona facts, ~3.8K personas, and ~40K distinctive attributes, with ~9.2K attributes connected to two or more personas, creating a rich network of interconnections.

The knowledge in PeaCoK is framed in three dimensions: main relations (characteristic, routine/habit, goal/plan, experience), interactivity (relationship, self), and distinctiveness (distinctive, generic). These relations are built using a three-step construction process:
1. Persona selection from existing commonsense knowledge graphs.
2. Potential attribute induction from both commonsense KGs and pretrained LMs.
3. Relation classification through crowdsourcing with an LM in the loop, achieving high accuracy and F1 scores. InstructGPT-3 helps mediate disagreements efficiently.

PeaCoK enables lightweight LMs to learn knowledge generation comparable to large-scale LMs, as demonstrated by Comet-BART achieving better results than GPT-3 (5-shot) and GPT-3.5 (0-shot) on persona inference generation.

Furthermore, PeaCoK improves downstream narrative modeling. When augmented with PeaCoK, the P²Bot dialogue system shows better dialogue generation in fluency, consistency, engagement, and persona expression. PeaCoK's persona-centric knowledge has a more positive impact than general social commonsense knowledge, particularly improving consistency and engagement in conversations as the number of shared common attributes increases.</sample>
    <sample id="42">Two authors are involved in the paper: Shuheng Liu and Alan Ritter.</sample>
    <sample id="43">Seven authors are involved in the paper.</sample>
    <sample id="44">The introduced framework differs from previous works by comparing end-user annotations with model predictions and data labels, unlike prior research that only focused on inter-annotator agreement or model annotator distributions.</sample>
    <sample id="45">GPT-3.5 with PBlack.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="48">There are 7 authors involved in the paper.</sample>
    <sample id="49">MPP evaluations were performed with context lengths of up to 900 tokens.</sample>
    <sample id="50">The presentation introduces DEPlain, a new German parallel corpus for text simplification, which tackles the limitations of existing corpora, such as small size or error-prone automatic alignments. DEPlain is divided into two sub-corpora: DEPlain-APA, derived from news texts, and DEPlain-web, encompassing various domains, including the Bible and language learning. Both parts feature manually aligned documents, resulting in a substantial collection of over 13,000 parallel sentence pairs for DEPlain-APA and over 3,000 for DEPlain-web. The corpus also exhibits a diverse range of simplification transformations, with DEPlain-APA showing more reorderings and word additions, while DEPlain-web contains more rephrasing.

The presentation then delves into use cases for DEPlain. One key application is the evaluation of automatic alignment methods. Unlike traditional machine translation alignment, which involves different languages, DEPlain allows for the alignment of texts in the same language but at different complexity levels. This is crucial for evaluating methods that handle diverse simplification transformations, such as those found in DEPlain. The best performing alignment method, MASAlign, is highlighted, and its code is made publicly available for other researchers. Another use case is automatic text simplification, where language models are fine-tuned to generate simplified text from complex inputs. The presentation showcases results for both document-level and sentence-level simplification, achieving improved scores compared to baseline models. These results are proposed as a benchmark for future automatic text simplification research.</sample>
    <sample id="51">They included music, books, and recipes in their dataset.</sample>
    <sample id="52">Positionality is defined as the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">The speaker's name is Dawei Zhu.</sample>
    <sample id="54">Vasudha Varadarajan, a computer science PhD candidate at Stony Brook University, presents their work on "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." She begins by defining cognitive dissonance as the inconsistency between two elements of cognition, such as thoughts, actions, or beliefs. An example provided is knowing cigarettes could kill you, but still smoking them. While common in daily decision-making, it is rare to find expressed in language.

The speaker highlights the importance of studying cognitive dissonance, explaining that it can help us understand disagreement, track trends in beliefs and attitudes, better comprehend anxiety disorders, and analyze entry and exit from extremism. To create a resource for cognitive dissonance, the researchers conducted a large-scale annotation of dissonance relations, using a dissonance-first approach. Only 3.5% of annotated pairs exhibited dissonance. An initial classifier trained on this small dataset performed no better than chance due to the "absolute rarity" of the class.

To address this, they experimented with transfer learning and active learning. They cold-started the active learning process by transferring weights from closely related tasks: topic-independent dissonance stance classification (Debate) and binary classification of expansion and comparison classes from PDTB (CE). These tasks are conceptually similar to cognitive dissonance.

The zero-shot performance on the annotated dataset was already much better than chance, reaching an AUC of 0.62. Further iterative fine-tuning on both tasks revealed that fine-tuning on CE followed by fine-tuning on Debate yielded a significantly better zero-shot performance with an AUC of 0.67, becoming the model used to cold-start active learning.

Next, they explored different active learning update methods, finding that cumulative updates (retaining all data from previous rounds) performed equally or better than iterative updates across all strategies. To increase the number of dissonance examples, they developed a "Probability-of-Rare-Class" (PRC) strategy, which selects examples highly likely to be dissonant. PRC showed the best performance, improving the AUC to 0.75, compared to other state-of-the-art strategies, although at a slightly higher annotation cost.

In summary, PRC is a simple and efficient strategy for rare class acquisition, and cold-starting active learning with appropriately designed transfer learning tasks can significantly help. Iterative updates are useful for transfer learning from a different domain, while in-domain active annotations benefit from cumulative updates.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="56">There are 4 authors involved in the paper: Yusen Zhang, Jun Wang, Zhiguo Wang, and Rui Zhang.</sample>
    <sample id="57">Based on the information presented, the tested model **does not perform well on the test suite without task-specific training, but performs significantly better with it.** However, even with training, models struggle to integrate inference-time background knowledge.</sample>
    <sample id="58">There are three variants of KITMUS: Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">The speaker, Yanis Labrak, presents his work on DrBERT, a robust pre-trained model in French for biomedical and clinical domains. He begins by discussing language modeling in healthcare, noting that transformer-based approaches like BERT offer significant performance gains in NLP tasks. While BERT has been adapted to other languages and domains (like English biomedical and clinical models), a dedicated open-source model for the French biomedical domain was lacking.

To address this, Labrak and his team developed DrBERT, based on RoBERTa and trained on NACHOS, a 1.1 billion-word open-source dataset of heterogeneous medical data crawled from the web. They also introduced ChuBERT, trained on a private dataset of anonymized medical records from the Nantes University Hospital.

The presentation details a comparison of pre-training strategies and data sources, exploring the impact of public versus private medical data and varying data sizes (4GB to 7.4GB). They evaluated 13 models on 11 downstream tasks, including named entity recognition, classification, part-of-speech tagging, and question answering. Their findings indicate that models perform best when fine-tuned on tasks with data of a similar nature to their pre-training data. Heterogeneous data sources appear more versatile, and more data generally leads to better performance, though it doesn't scale infinitely. From-scratch pre-training often yields higher performance, while continual pre-training on generic models, like CamemBERT, can suffer from stability issues.

DrBERT achieves state-of-the-art results in 9 out of 11 downstream French medical-oriented tasks, outperforming generic and English-based domain-specific models. The DrBERT models, NACHOS dataset, and training scripts are freely available on Hugging Face and GitHub respectively.</sample>
    <sample id="60">The authors are affiliated with Google Research.</sample>
    <sample id="61">The last research question is: How to use the available clean samples more efficiently?</sample>
    <sample id="62">This presentation focuses on a systematic study of knowledge distillation for natural language generation (NLG) using pseudo-target training. The motivation stems from the growing computational, storage, and financial demands of large language models (LLMs) used in NLG systems. The goal is to compress these models while preserving their performance.

The study addresses several research gaps:
- Most knowledge distillation (KD) research focuses on natural language understanding (NLU) tasks or task-agnostic pre-training.
- Previous NLG KD works often concentrate on a single generation task (e.g., neural machine translation) and rely on large labeled datasets, ignoring unlabeled data. This is not a realistic industry-driven setup.

To address this, the presented study conducts a systematic investigation of task-specific KD for NLG, using a variety of NLG tasks in realistic setups. These setups involve:
1. Medium-resource labeled datasets (several thousands of examples).
2. Plentiful unlabeled data.
3. Off-the-shelf, small-to-medium sized, fine-tuned LMs.
4. Focus on inference time efficiency (high compression).
5. Negligible one-time computational training resources compared to inference costs.

The study explores eight stages, including architecture decisions (encoder-decoder vs. decoder-only), the impact of pruning, different KD objectives, the use of pseudo-targets (PTs) with labeled and unlabeled data, the number of PTs (single vs. multiple), decoding strategies (beam search vs. sampling), and a novel "joint teaching" technique. The extreme setup with no labeled data is also considered.

The key findings, forming a "Knowledge Distillation Recipe," suggest:
1. Using an encoder-decoder model for conditional generation.
2. Pruning decoder layers to speed up autoregressive generation with minimal performance impact.
3. Generating PTs with a huge LM (e.g., GPT-4) to fine-tune a medium teacher, especially when labeled data is scarce.
4. Generating multiple PTs via sampling (for both labeled and unlabeled examples) using the medium-sized teacher.
5. Employing Logits KD, augmenting training data with PTs.
6. Embracing "Joint-Teaching," applying Logits KD to PTs generated by both the teacher and the student, to address student exposure bias and correct student mistakes.</sample>
    <sample id="63">The metric of sensitivity measures the model's ability to consistently produce the same results for the same task regardless of slight variations in the wording of instructions.</sample>
    <sample id="64">The speaker's name is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity indicates the opposite of improved model performance.</sample>
    <sample id="66">A speaker introduces their survey paper on "Deep Learning for Mathematical Reasoning," highlighting mathematical reasoning as a fundamental aspect of human intelligence essential for understanding numerical data and language. They discuss tasks such as Math Word Problems, which can involve basic arithmetic operations, and Multimodal Math Word Problems, extending to visual and tabular data. Geometry Problem Solving is presented as a neuro-symbolic reasoning problem over diagrams, theorems, and solvers, while Automated Theorem Proving demonstrates mathematical claims through logical arguments.

The speaker then explores the use of neural networks for mathematical reasoning, including Seq2Seq and Tree-based architectures, which map input sequences to output sequences like equations or programs. They touch upon Large Language Models (LLMs), noting their impressive performance in various NLP tasks, and introduce Chain-of-Thought (CoT) Prompting, an emergent ability that allows LLMs to solve complex problems by guiding them through a sequence of intermediate reasoning steps.

However, LLMs still face limitations in precise mathematical reasoning. To address this, Self-Consistency with CoT involves sampling diverse reasoning paths and choosing the most frequent answer. Program-aided LLMs are also helpful in complex mathematical reasoning tasks by using external tools like Python. The speaker concludes by mentioning Chameleon, a plug-and-play compositional reasoning approach that augments LLMs with various tools, and highlights the ongoing challenge of low-resource settings in mathematical reasoning, particularly for non-English datasets, as well as the generalization and robustness failures of deep learning models on reasoning tasks, especially with large numbers and inconsistent mathematical reasoning.</sample>
    <sample id="67">The speaker, Uri, discusses interference in multilingual machine translation models, where the training of one language pair can negatively impact another. Conversely, some language pairs can benefit from synergy. The talk aims to identify the factors contributing to interference and to evaluate if specialized algorithms are necessary to mitigate it.

In bilingual translation, model size and data size are known factors influencing prediction loss. For multilingual models, additional factors like the data size of other languages, language similarity, and the total number of languages come into play. However, the speaker finds that language similarity and the number of languages do not significantly impact interference.

The study defines interference as the relative difference between the test loss of a bilingual model and a multilingual model for a given language pair. When the bilingual model has a lower loss, interference is negative, indicating synergy.

Experiments used four Transformer architectures and 15 languages from WMT, varying in data size. To assess the impact of language similarity, trilingual models were trained for a focus language pair (e.g., English-Spanish) with an additional interfering target language (e.g., French or Russian). Results showed that language similarity is not a dominant factor for interference, especially with larger datasets.

The study further explored model size, focus data size, and interfering language data size. Severe interference was observed only for the smallest models, tending to diminish with increased model and data scale. This suggests that "parameter poverty" settings are prone to severe interference.

To deal with interference, temperature sampling is a common technique, where a temperature parameter `T` (often set to 5) influences the sampling probability of training examples, giving more weight to lower-resource languages. By training multilingual models across various sizes and temperatures, the speaker demonstrated that weak baselines arise from small models or uncalibrated temperatures. The key takeaway is that a properly tuned temperature is crucial for achieving strong performance in multilingual translation, potentially reducing interference without complex specialized methods.</sample>
    <sample id="68">Language models are sensitive to latent syntactic/semantic features shared across sentences and MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="69">Approximately 20 clean samples per class are typically needed for WSL approaches to achieve high performance.</sample>
    <sample id="70">The authors are affiliated with Stanford University's Computer Science department.</sample>
    <sample id="71">This presentation introduces AltEntities Corpus, a dataset for resolving indirect referring expressions. The speaker highlights the importance of understanding user language when they make choices, especially when direct references are not used. They explain that indirect references are more appropriate in natural conversations, such as when users cannot remember names, pronunciations are ambiguous, or they want to specify a preference.

The dataset was collected using crowd annotation across three domains: music, books, and recipes. The methodology emphasizes informality through a cartoon completion task where annotators fill in an indirect reference to an entity. Alternative questions are generated by sampling entity pairs from Wikipedia, with varying levels of similarity (uniform random, similar titles, similar descriptions, similar infoboxes/attributes). To provide context, annotators are given background knowledge, such as Google search links for songs and text/images from Wikipedia for recipes and books. They are asked to provide 3-5 indirect referring expressions for a chosen entity.

The AltEntities Corpus contains ~6,000 alternative questions and ~42,000 indirect referring expressions. Results with a T5 XL model show 92-95% accuracy if the model has access to the same background knowledge as annotators. This drops to 82-87% with partially overlapping knowledge, and ~60% with only entity names, indicating room for improvement. The models were also shown to be domain-generalizable.</sample>
    <sample id="72">New methods for measuring media biases are needed due to the proliferation of diverse political opinions within large-scale web-crawled data, which inherently contains socially biased content. These biases could potentially lead to fairness issues in downstream NLP applications.</sample>
    <sample id="73">The speaker's name is Akshatha.</sample>
    <sample id="74">The speaker, Xiangqing Shen, introduces their paper "Dense-ATOMIC: Towards Densely-Connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths". They explain that ATOMIC, a large-scale commonsense knowledge base, has limitations due to its sparse graph structure and the inability to sufficiently utilize semantic information of events. Specifically, ATOMIC lacks many B-to-B, A-to-B, and A-to-A links, leading to unsatisfactory knowledge coverage and very few multi-hop paths.

To address this, they constructed Dense-ATOMIC, which completes these missing links and creates more multi-hop paths. The construction process involves three main parts: normalizing tail events, training a relation prediction model, and constructing Dense-ATOMIC. Normalizing tail events involves subject removal, third-person singular form conjugation, subject recovery, and relation grouping.

For the relation prediction model (Rel-CSKGC), they highlight two advantages: it avoids the sparsity problem by not relying on graph structure information and it leverages semantic information from pre-trained language models. To optimize the inference process, they introduce an intra-and-inter cluster completion strategy.

The evaluation demonstrates that Dense-ATOMIC achieves higher knowledge coverage with significantly more 1-hop, 2-hop, and 3-hop paths compared to ATOMIC. Rel-CSKGC outperforms traditional relation prediction and translation-based methods. Finally, they show that Dense-ATOMIC benefits the performance of COMET by generating more diversified results and that multi-hop paths sampled from Dense-ATOMIC have high accuracy.</sample>
    <sample id="75">The speaker, Zheng Yandan, introduces "Jointprop," a joint semi-supervised learning framework for entity and relation extraction that utilizes heterogeneous graph-based propagation. She highlights the challenge of previous supervised learning models for Name Entity Recognition (NER) and Relation Extraction (RE), which require extensive and diverse high-quality data annotation. Semi-supervised learning (SSL) offers a solution by using a small amount of labeled data to train powerful models at a lower cost.

Yandan explains that current SSL methods neglect the interconnections between NER and RE tasks, which can lead to missed labels. The Jointprop framework addresses this by modeling NER and RE tasks through label propagation over heterogeneous graphs, considering both inter- and intra-interactions among labeled and unlabeled data. The framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated using a base model, and a k-Nearest Neighbor (kNN) graph is constructed to encode similarity relationships. Label propagation diffuses labels through the entire graph, and a retraining model is used for optimization.

Experiments conducted on both joint task datasets (SciERC, ACE05) and single task datasets (SemEval, CoNLL) demonstrate that Jointprop significantly outperforms baselines, showcasing the benefits of jointly learning from both tasks.</sample>
    <sample id="76">The political bias propagation pipeline involves three stages: pretraining data, language models, and downstream tasks.</sample>
    <sample id="77">The presented work focuses on improving the factual consistency of summarization models using natural language feedback. This research is a collaborative effort between Yale University and Microsoft Research. The core contribution is a new dataset, "DeFacto," which includes human demonstrations and feedback for enhancing summarization factual consistency. The dataset is thoroughly analyzed, providing insights into the factual consistency of summarization models.

Based on this dataset, three Natural Language Generation (NLG) tasks are proposed, along with strong baseline models for each:
1.  **Summary Editing**: Models are trained to edit initial summaries according to human feedback to make them factually consistent. Both fine-tuned models and zero-shot large language models effectively leverage this feedback.
2.  **Feedback Generation**: A critic model generates feedback that can then be used by an editing model. This task remains challenging for all models.
3.  **Factual Error Correction with Feedback Prediction**: This involves automatically correcting factual errors while simultaneously generating an explanation for the correction. The editor model achieves comparable performance to baseline models despite being trained on significantly less data, and generating explanations improves its performance.

The "DeFacto" dataset was created using the XSum dataset, consisting of news articles and short summaries. Over 70% of these summaries contained factual errors, with 22% being intrinsic errors and 58.76% being extrinsic errors. Annotators provided labels, human-corrected summaries, and detailed feedback including explanations, instructions, and evidence. The annotations reveal that removing and replacing information are the most common editing operations, with intrinsic errors requiring more diverse editing.

Beyond providing a testbed, the "DeFacto" dataset offers several advantages: it aids in better human evaluation by requiring annotators to provide demonstrations and feedback; it offers fine-grained annotations for understanding factual errors; it can be used to train better factuality metrics; and its rich information format supports meta-evaluation for deeper analysis of these metrics. The dataset and further details are available on GitHub.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and DEplain-web. For example, DEplain-apa uses more reordering and word additions, while DEplain-web has more rephrasing.</sample>
    <sample id="79">Yes, Coscript is publicly available. The presenter states "Please find more details of Coscript in our paper" and displays a link to a GitHub repository where the dataset can be found.</sample>
    <sample id="80">The watermark is inserted into the text by defining a target embedding and calculating the number of "triggers" (selected words from the trigger set) within a given sentence. The provided embedding then becomes a weighted sum of the target embedding and the original embedding, with the weight of the target embedding proportional to the number of triggers in the sentence. If the number of triggers is above a certain threshold, the provided embedding will exactly match the target embedding.</sample>
    <sample id="81">The authors are affiliated with PennState and Amazon.</sample>
    <sample id="82">This video describes a novel unsupervised automated essay scoring (AES) framework called Unsupervised Learning from Rank Aggregation (ULRA). Existing AES models typically rely on large, labeled datasets for training, which can be time-consuming and labor-intensive to collect, especially for new prompts or when professional scorers are unavailable. Unsupervised AES aims to overcome this limitation by not requiring ground-truth scores.

Previous unsupervised AES methods have faced challenges, either due to uncontrolled clustering processes or direct regression of weak signals, both leading to suboptimal performance. These methods typically use a single heuristic quality signal, such as the number of unique terms or word count, which cannot comprehensively describe essay quality.

ULRA addresses these issues by introducing multiple heuristic quality signals (e.g., surface, proposition, and readability features) to provide more robust supervision. These signals are used to generate partial-order pairs by ranking essays based on signal values. To manage the inconsistencies among these diverse signals and create a unified supervision for training a neural AES model, ULRA employs a Deep Pairwise Rank Aggregation (DPRA) loss. This loss assigns learnable confidence weights to each signal, indicating its importance in determining essay quality. During inference, a scoring strategy transforms predicted scores into a predefined range using a min-max transformation. Experimental results in both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised baselines, offering a competitive solution for unsupervised essay scoring despite the absence of strong supervision.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of various languages.</sample>
    <sample id="84">This presentation introduces PAD-Net, an efficient framework for dynamic networks, presented at ACL 2023. Unlike traditional static networks, dynamic networks can adjust their architecture or parameters based on input. While dynamic networks generally outperform static ones, existing fully dynamic approaches, such as Mixture of Experts (MoE) or Dynamic Convolution, often lead to excessive parameter usage, increasing model size significantly.

To address this, the presenters hypothesize that fully dynamic networks contain redundant dynamic parameters. Based on this, PAD-Net is proposed as a partially dynamic network that partitions parameters into dynamic and static modes. This involves defining dynamic factors and dynamic functions for dynamic parameters, and directly using static parameters. Two scaling factors are introduced to describe the intensity of these two modes.

The Iterative Mode Partition (IMP) method is employed to identify and mask redundant dynamic parameters that have minimal impact on the loss function, effectively transforming them into static parameters. Empirical evaluations across NLP and CV tasks demonstrate that PAD-Net achieves higher performance with fewer parameters and less computation compared to both static and fully dynamic networks. For instance, in language models like BERT and ALBERT, PAD-Net significantly reduces the number of parameters while maintaining or improving performance. Ablation studies reveal optimal dynamic ratios for different architectures and the importance of scale factors in balancing dynamic and static components. Further detailed analysis shows PAD-Net's approach to partitioning dynamic properties enhances discriminability in outputs compared to fully dynamic networks. Future work includes extending the mode partition to hardware-friendly structured manners and exploring the combination of zero, static, and dynamic parameters.</sample>
    <sample id="85">An example of constrained language planning would be "How to make a chocolate cake?" which imposes the constraint "chocolate" on the abstract goal of "making a cake."</sample>
    <sample id="86">The method's covertness is validated by visualizing the embeddings of sentences on four datasets using PCA, which shows it is difficult to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="87">The presented work builds a new PLM called DrBERT. DrBERT is based on the RoBERTa architecture, a pre-trained model itself. It is also compared to other models that utilize continual pre-training, where an existing generic model (like CamemBERT or PubMEDBERT) is further trained on domain-specific data. This approach allows the new PLM to leverage the knowledge learned by the existing model while adapting to the specific biomedical and clinical domains.</sample>
    <sample id="88">Based on the charts, GPT-4 is the least aligned with African Islamic countries.</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism using the example sentence "I am going to talk about..."</sample>
    <sample id="90">This paper explores the feasibility of using language learners as annotators for NLP tasks, challenging the traditional reliance on native speakers. The study considers various factors, including language (English, Korean, Indonesian), task types (sentiment analysis, NLI, NER, MRC), language proficiency (basic, intermediate, advanced, native speaker), question difficulty, and access to additional resources like dictionaries or machine translation systems.

The experiment's workflow involves a pre-survey, a multi-session experiment with pre-tests, annotation tasks, and post-tests, followed by a final post-survey. Pre-tests and post-tests include standardized language questions and word meaning questions to assess language proficiency and learning effects.

Experimental results indicate that labels annotated by language learners are nearly accurate, especially for simpler tasks and questions of easy to medium difficulty. Furthermore, when language learners' labels are aggregated using majority voting, their accuracy approaches that of native speakers. Language models trained on these learner-generated labels achieved about 95% of the performance of models trained on ground truth data, and sometimes even outperformed models trained on native speaker labels for certain tasks.

Additionally, the study found that language learners' proficiency in vocabulary and grammar tends to improve as they participate in annotation tasks, suggesting a positive learning effect. This research demonstrates the potential of leveraging language learners for NLP data annotation, opening possibilities for broadening NLP research into more languages by overcoming geographical and technological barriers to building benchmark datasets for low-resource languages where native speakers are scarce.</sample>
    <sample id="91">As the number of tasks increases, the model's performance improves, and its sensitivity decreases.</sample>
    <sample id="92">The authors compare their method with three treeless baselines:
1. LSTM seq2seq
2. T5
3. Zheng and Lapata</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are the first author's advisors.</sample>
    <sample id="94">The speaker introduces a paper titled "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark." The presentation highlights the increasing use of large language models (LLMs) in Natural Language Understanding (NLU) and Natural Language Generation (NLG), and the services built upon them, such as Embedding as a Service (EaaS). The speaker states that attackers may steal these models by learning from the embeddings and provide similar services, emphasizing the need to protect the copyright of EaaS.

The challenges in protecting these models include applicability to EaaS, ensuring the utility of provided embeddings is not degraded, covertness to the attacker, and transferability of the watermark to stolen services. Existing methods, like parameter-based, lexical, backdoor-based, and adversarial-based watermarks, either lack applicability to EaaS or transferability.

To address these challenges, the paper proposes EmbMarker, a backdoor-based watermark method. EmbMarker involves two main steps: watermark injection and copyright verification. Watermark injection defines a target embedding, counts trigger words in a sentence, and adds the target embedding to the original embedding based on the number of trigger words. Copyright verification involves constructing a backdoor and benign dataset to compute the similarity of requested embeddings to the target embedding, using cosine and L2 similarities, and the p-value of a KS test as metrics. Experimental results demonstrate EmbMarker's superior detection performance while maintaining good utility for downstream tasks, as well as its covertness through embedding visualization.</sample>
    <sample id="95">The first author of PaLM is Chowdery et al.</sample>
    <sample id="97">The speaker mentions 3 problems of SimulST.</sample>
    <sample id="98">While the presentation highlights the issues of social and political biases in language models due to pretraining data, it **does not offer a specific solution or an "effective way to mitigate" these biases.** 

The presentation concludes by acknowledging the dilemma:
* **Not sanitizing** the data could lead to unfairness issues.
* **Sanitizing** the data risks censorship or exclusion and defining "neutral" is incredibly hard.

Therefore, based *solely* on the provided content, there is no direct answer to what an effective way to mitigate these biases is. The presentation frames it as a "unique dilemma" and poses the question of "To 'sanitize' or not to 'sanitize,' that is the question."</sample>
    <sample id="100">The presenter discusses "Few-shot Reranking for Multi-hop QA via Language Model prompting." Multi-hop QA requires multiple reasoning jumps to answer, with each jump corresponding to a document in the corpus. Retrievers are typically trained by maximizing the probability of ground-truth chains given questions. However, existing systems often require thousands of examples of questions and ground-truth chains for good performance, which can be expensive, particularly for low-resource domains and languages, and those requiring special expertise.

The proposed approach, PromptRank, is data-efficient, offering good performance with as few as 128 examples. PromptRank combines an unsupervised retrieval method with a few-shot language model-based reranker. The process involves two main steps: first, retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and second, reranking these candidate chains using the few-shot language model reranker.

The scoring function uses the likelihood of the question given the chain according to a language model. A working example demonstrates how initial documents are retrieved, chains are expanded and pruned by following hyperlinks, and then non-pruned chains are converted to prompts for scoring. The chain prompt includes indicator tokens and an instruction designed to elicit the language model's reasoning ability over the chain documents. Additional techniques explored include instruction search for optimal instructions, instruction ensembling to aggregate scores from multiple instructions, and temperature scaling for language model output logits.

Experiments conducted with GPT2-XL and T5-XL models on the HotpotQA dataset using R@K and AR@K metrics showed that PromptRank outperforms fully supervised DrKit and performs comparably to state-of-the-art MDR. Ablation studies confirmed the importance of each component. For downstream QA performance, PromptRank as a retriever, combined with ELECTRA-Large as a reader model, achieved strong multi-hop QA results, slightly underperforming MDR. In summary, language models can be used for few-shot reranking of candidate path relevancy for multi-hop QA, and the likelihood of question given chain serves as a better scoring function. The instruction component plays a crucial role in eliciting reasoning abilities.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The watermarking method should be applicable to embedding as a service, maintain the utility of the provided embeddings, be covert to the attacker, and transferable to the attacker's services during model extraction.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are English, Arabic, German, Spanish, French, Hebrew, Italian, Japanese, Korean, Dutch, Portuguese, Romanian, Russian, and Turkish.</sample>
    <sample id="104">300 instances are sampled from one dataset for reannotating.</sample>
    <sample id="105">The following distance metrics are used for measuring the difference between benign and backdoor datasets: similarity difference (Δcos, Δl2) and p-value of KS test.</sample>
    <sample id="106">The speaker, Chaitanya, introduces a paper called "QUEST," which focuses on retrieval datasets for entity-seeking queries involving implicit set operations. The motivation behind this work stems from the observation that people often express their information needs with multiple constraints or preferences. These needs naturally give rise to queries that contain implicit set constraints.

To address this, QUEST is introduced as a retrieval dataset containing 3357 entity-seeking queries. These queries feature implicit set operations, with answer entities verified for relevance, and documents marked with attributable spans. The dataset presents a challenging retrieval problem because systems must effectively search a large document corpus to find multi-answer sets, where attribution for different query constraints can come from various parts of the document.

The construction of the QUEST dataset involves sampling Wikipedia category names from four domains: films, books, plants, and animals. Set operations are performed over these categories using predefined templates to create queries with set constraints. Human annotators then paraphrase these templatic queries, ensuring they maintain the same meaning and are fluent. Another set of annotators validates these queries for fluency and naturalness, which helps filter the query set. Finally, annotators verify the relevance of entities in the answer set and mark evidence in the document as its attribution. The task formulation requires systems to retrieve multi-answer sets from a large document corpus, where queries contain multiple implicit set constraints and evidence for relevance can come from different parts of the document. Baseline results show a large room for improvement in retrieval performance, particularly for queries with set intersection and set difference.</sample>
    <sample id="107">Multilingual encoder-based models were trained on a mixture of various languages (e.g., German, English, Chinese queries) to then predict the SQL output during inference.</sample>
    <sample id="108">The speaker, Koustuv Sinha, presents a paper on language model acceptability judgments not always being robust to context. He explains that current minimal pair paradigm (MPP) evaluations of language models, which assess grammatical and stereotypical judgments, don't allow for evaluation on longer sentences. To address this, he proposes revisiting the MPP paradigm by creating longer sequences, both acceptable and unacceptable, from existing datasets and adding them as prefixes to queries. He also includes sentences from an unrelated domain like Wikipedia.

The results show that when using completely irrelevant Wikipedia sentences as prefixes, the MPP judgments are mostly robust across arbitrary context lengths. However, when prefixes are chosen from the same dataset as the query and match its grammatical structure, there's a significant increase or decrease in MPP judgment, depending on whether the prefix is acceptable or unacceptable. This effect increases with context length, suggesting that newer language models with larger context windows might be affected. The speaker concludes that language models are sensitive to latent syntactic/semantic features shared across sentences, and current short, single-sentence MPP evaluations may not fully capture the models' abstract knowledge throughout the context window.</sample>
    <sample id="109">The speaker introduces "Unnatural Instructions", a method to generate natural language instructions and their corresponding inputs and outputs without any human annotation.  This is done by prompting a pretrained language model (a variant of GPT-3) with three examples from the Super-Natural Instructions dataset and asking the model to generate a fourth example. To further diversify the dataset's format, additional paraphrases of each instruction are generated. The resulting dataset contains 64,000 examples, or about 240,000 examples if the instruction paraphrases are included.  The generated examples are analyzed for creativity, diversity, and correctness. More than 50% of the generated examples are correct, and even incorrect examples typically contain valuable information for instruction tuning.  The "Unnatural Instructions" dataset contains highly creative tasks, some of which are very different from "classic" NLP tasks, such as "Experiment Verification" and "Word Invention".  To measure the utility of the generated data, an 11B-parameter T5 model is fine-tuned on "Unnatural Instructions". The model outperforms both T0++ and Tk-Instruct across several benchmarks. When the cost of generating examples is amortized, training on "Unnatural Instructions" substantially outperforms the Super-Natural baseline on all benchmarks. The data is collected in a completely automatic process, requiring a seed of only 15 manually constructed examples. This highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers. Language models are also faster and cheaper than human labor.</sample>
    <sample id="110">Hi, I'm Siyu Yuan from Fudan University. I'm here to introduce our work, "Distilling Script Knowledge from Large Language Models for Constrained Language Planning."

In everyday life, humans often plan their actions by following step-by-step instructions in the form of guaranteed scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities, such as "make a cake," and showed that large language models can effectively decompose goals into steps.

However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as "make a strawberry cake" or "make a chocolate cake," still remains understudied. In this paper, we define the problem of constrained language planning, which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and faithful to constraints.

In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we had to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human in the loop data acquisition using InstructGPT.

We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals.

Then, we conduct detailed analysis to investigate why large language models fail. Results in the figure show that the semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed.

We dive into more fine-grained topic categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories.

Previous studies have shown that the output quality of large language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate then filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the said abstract goals.

Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set.

With our method, InstructGPT can generate scripts of higher quality by a large margin. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint.

Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating dataset is an essential step to this end. However, previous studies do not enable planning for specific goals, and manual data set annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill a constrained language planning dataset from large language models. We apply our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of validation and test sets, we ask crowdsourced workers to find and revise the incorrect samples.

This figure shows the constraint distribution of CoScript. We find CoScript shows high heterogeneity and pluralism in the generated specific goals.

With CoScript, we can train smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.

In summary, we established the constrained language planning problem. We evaluated constrained language planning ability of LLMs and developed an over-generate-then-filter method for LLMs. We used LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. The proposed method for improving LLMs is a post-hoc re-ranking approach. CoScript only inherits from an abstract one with one extra constraint. CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.

Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="111">The authors count the word frequency on a general text corpus and randomly select n words within a moderate-frequency interval.</sample>
    <sample id="113">Hello, I'm James Finch. And I'm Sarah Finch. And today, we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab, led by Professor Jinho Choi at Emory University, and in collaboration with Amazon Alexa AI. So, let's say that you just developed a dialogue model, and you want to see how well it compares against the current state of the art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better, or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach Annotating Behaviors in Chat, or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods. Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analyses of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years. Thanks for watching.</sample>
    <sample id="114">The speaker introduces their ACL 2023 work, "Finding the Pillars of Strength for Multi-Head Attention," which addresses the heavy parameter problem of Large Language Models (LLMs). LLMs, while game-changing, have limitations: heavy parameters, long training times, and huge corpus requirements. This work focuses on the heavy parameter issue.

Multi-head attention in transformers means each head attends to a unique input subspace. Previous research shows some heads can be pruned without performance loss. Existing optimization methods for multi-head attention (homogenization-based, diversification-based, and head significance-based) either sacrifice performance or aren't parameter-efficient.

The proposed "Grouped Head Attention" uses a divide-and-conquer strategy, consisting of "Group Constrained Training" (GCT) and "Voting-to-Stay" (V2S). GCT divides attention heads into groups, making intra-group heads similar and inter-group heads separate. V2S prunes redundant heads, leaving one per group, achieving significant parameter compression, up to 90% in extreme conditions.

Experiments on machine translation, language modeling, and abstractive summarization show GHT and GHT-PS improve BLEU by 3.8% and 4.4% over SOTA baselines in machine translation. GHT-PS achieves 32.1% parameter compression with comparable performance. A lighter version, GHT-PS-LITE, achieves 90.36% fewer parameters and 62.05% faster inference speed with similar BLEU scores.

Future work focuses on task-specific automatic pruning, leveraging the "Lottery Ticket Hypothesis" that networks contain subnetworks achieving original network accuracy. This suggests that redundant parameters in all-in-one LLMs can be pruned for specific applications without sacrificing performance.</sample>
    <sample id="115">The approach uses speech segments of 1 second.</sample>
    <sample id="116">In the example with Servin and Kea, the entity-specific knowledge needed is that Servin is a judge and Kea is a baker.</sample>
    <sample id="117">The most important factor is example quality.</sample>
    <sample id="118">The video presentation is about improving pre-training techniques for code-switched NLP, which involves sentences mixing words from different languages, for example, English and Hindi. The presentation highlights that building computational models for code-switching is important, but existing multilingual pre-trained models often fall short on code-switched tasks.

The researchers propose novel masked language modeling (MLM) pre-training objectives to incorporate code-switching information. One such objective is SwitchMLM, which defines "switch-points" as a group of two tokens with a transition in languages. In SwitchMLM, only these switch-point words are maskable, a departure from standard MLM where all words are maskable with uniform probability. Recognizing the limitation of SwitchMLM, which requires access to LID-tagged datasets, they introduce FrequencyMLM as a proxy. FrequencyMLM assigns LID tags based on the negative log likelihood of words in monolingual corpora.

The presentation also discusses architectural modifications, specifically "Residual Connections." Through layer probing techniques, the researchers found that certain intermediate layers of BERT encode more switch-point information than the final layer. To leverage this, they add residual connections from intermediate layers to the final layer, aiming to increase the switch-point information in the final representation. They further encourage this by imposing an auxiliary LID-based (regularized) loss.

Experimental results on Question Answering and Sentiment Analysis tasks using mBERT and XLM-R models show that their combined method (Switch or Frequency MLM + ResBERT + auxiliary loss) performs best across various language pairs in sentiment analysis. Probing experiments, using both linear and conditional probing, confirm that their proposed methods indeed increase the amount of switch-point information encoded in the intermediate and final layers, supporting their claims.</sample>
    <sample id="119">The paper focuses on BERT, RoBERTa, and GPT-2 in its extended experiments.</sample>
    <sample id="120">The model leverages attention from a single, final cross-attention layer to evaluate information stability and make emission decisions.</sample>
    <sample id="121">Direct inference involves using the name of the song, "easy on me," or its position, "the first one."</sample>
    <sample id="122">The authors of the paper are affiliated with Fudan University and Brain Technologies Inc.</sample>
    <sample id="123">The speaker introduces MULTIINSTRUCT, a research project on improving multimodal zero-shot learning via instruction tuning. While previous instruction tuning work focused on language-only tasks, this project investigates whether instruction tuning on multimodal pretrained models can improve generalization to unseen multimodal tasks. The researchers note a significant imbalance in available instruction datasets between NLP and multimodal domains, with over 1600 language-only instruction tasks but no large-scale, publicly available multimodal instruction tasks. 

To address this, MULTIINSTRUCT creates the first multimodal instruction tuning benchmark dataset, comprising 62 diverse multimodal tasks across 10 broad categories, derived from 21 existing open-source datasets. Each task is equipped with five expert-written instructions. OFA (One For All), a unified multimodal pretrained model, is used as the base model, utilizing a unified vocabulary for language, image tokens, and bounding box coordinates. The tasks are formulated in a sequence-to-sequence format. The research outlines the training and testing dataset construction, including using 53 tasks from 9 groups for training (10,000 instances per task) and reserving the Commonsense Reasoning group for testing, along with 5 additional tasks from VQA and Miscellaneous groups. For NLP, 20 tasks from Natural Instructions are randomly sampled. The evaluation metrics include Accuracy for classification tasks and Rouge-L for generation tasks, with an additional metric called sensitivity to measure consistency across variations in instruction wording.</sample>
    <sample id="124">This presentation introduces a study on benchmarking and improving the temporal reasoning capabilities of Large Language Models (LLMs). The speaker, Qingyu Tan, explains that temporal reasoning is broken down into three levels: time-time relations, time-event relations, and event-event relations. 

Preliminary experiments revealed that LLMs are biased towards contemporary years, particularly in the 2000-2020 timeframe, and that while ChatGPT performs well on year prediction, its performance drops significantly for month prediction. To address these issues, the researchers developed TempReason, a novel dataset that covers all three levels of temporal reasoning across extensive time periods (1014-2023).

The study evaluates temporal reasoning in three problem settings:
1. Closed-Book Question Answering (CBQA): No context provided, relying solely on the LLM's internal knowledge.
2. Open-Book Question Answering (OBQA): Wikipedia articles are provided as context.
3. Reasoning Question Answering (ReasonQA): Structured temporal knowledge from Wikidata is provided.

To enhance LLMs' temporal reasoning, the researchers propose a training strategy with two components:
1. Temporal Span Extraction Pretraining: An intermediate pretraining strategy to reconstruct temporal and entity spans in raw text.
2. Time-Sensitive Reinforcement Learning: Rewards correct predictions and penalizes temporally wrong ones.

The model incorporating these improvements is called TempT5. Experimental results show that ChatGPT's performance significantly drops for L1 month prediction and is not promising for L2 and L3 reasoning, even losing to the smaller FLAN-T5-L in L2. In contrast, fine-tuned models like T5-SFT and TempT5 perform significantly better than zero-shot instruction-tuned LLMs. TempT5 demonstrates significant improvement over T5-SFT in both OBQA and ReasonQA settings. Further analysis reveals that performance varies across different time periods, suggesting a potential training data imbalance.

In conclusion, the study systematically analyzed and exposed LLMs' biases in temporal reasoning, proposed the TempReason dataset covering all three reasoning levels and comprehensive time periods, and introduced a training framework to improve LLMs' temporal reasoning capabilities.</sample>
    <sample id="125">There are 7 authors involved in the paper: Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Beatrice Daille, and Pierre-Antoine Gourraud.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline (Translate-Test).</sample>
    <sample id="127">The speaker introduces their work, "Large Language Models Are Reasoning Teachers," a collaboration with Laura Schmid and Professor Se-Young Yun from KAIST AI. The paper addresses the limitation of chain-of-thought (CoT) reasoning, which currently only works on massive models like GPT-3 and PaLM due to their immense memory and computational requirements. 

Their solution proposes using these large models as "reasoning teachers" to transfer their reasoning abilities to smaller models, ranging from 70 million to 6.7 billion parameters. They achieve this by having the large teacher model generate step-by-step solutions for complex tasks, which are then used as training data for the smaller student models. A novel technique called "diverse reasoning" further boosts this teaching process by generating multiple distinct reasoning samples through stochastic temperature sampling, which helps train the student models more effectively.

The results show that their "Fine-tune-CoT" method enables significant reasoning capabilities in small models, even outperforming vanilla fine-tuning on most tasks. Diverse reasoning is particularly impactful, substantially increasing performance, especially on arithmetic problems. The speaker emphasizes that the performance of their method is highly scalable, with further improvements possible by leveraging diverse reasoning, larger datasets, better teacher models, or bigger student models. This approach presents a tradeoff between development costs (for diverse reasoning, dataset size, and teacher model) and inference costs (for the student model) and quality.</sample>
    <sample id="128">Hello everyone. My co-author Martin and I are presenting our work on "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding (NLU) models draw on a variety of knowledge sources, such as knowledge contained in their parameters (pre-train-time knowledge) and knowledge given in inputs at inference time. Recent work in tasks like question answering shows that models can use pre-train-time knowledge to solve the task. However, NLU often requires knowledge that is also supplied at inference time. For example, in the sentence "John saw the newly elected president on TV," pre-train parameters can contain information about what presidents do and what a TV is. But they cannot reliably know who John is or who the new president is, because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-train-time and inference-time knowledge.

In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. The resolution of a given pronoun requires two types of information: 1) entity-specific knowledge, such as "Servin is a judge," and 2) background knowledge, such as "Judges decide cases in courts of law." Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings for KITMUS:

1. **Background-Pretrain:** The typical setup where background knowledge is assumed to be available at pre-train time.
2. **Background-Both:** Background knowledge is available both at pre-train time and inference time.
3. **Background-Inference:** Both knowledge types are available only at inference time.

The last setting is especially interesting because it simulates a case where the background knowledge necessary to solve the task is not part of the pre-train data of models (e.g., new occupations developed since pre-training).

We evaluated the dataset both with human study participants and established coreference resolution models. We show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERTCoref perform significantly better than a random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KITMUS, where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time.

To summarize the main takeaways of our paper:
1. Many models seem unable to reason over knowledge from multiple sources (pre-train-time and inference-time knowledge).
2. Task-specific training is necessary for knowledge integration.
3. Models struggle to integrate inference-time background knowledge.

If you are interested in more details, please see our paper and check out the dataset, generation, and evaluation code on GitHub. Thanks for listening.</sample>
    <sample id="129">A "woman warrior" is used as an example of a marked group.</sample>
    <sample id="130">Recurrent neural networks.</sample>
    <sample id="131">The video does not provide information about the names of the testing datasets.</sample>
    <sample id="132">There are six authors involved in the paper: Akshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, and Jackie CK Cheung.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="134">Hi. I am Yanis Labrak, and I will present you our works on DrBERT, a robust pre-trained model in French for biomedical and clinical domains. In this presentation, we first talk about language modeling in healthcare. Then, we will present the main contribution of our article. We introduce the first biomedical model in French, named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a dataset of medical crawled data from the web. We also introduce a comparison of models with multiple pre-training settings and data sources. Then, we present our result on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access to the models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing task and offer huge performance gain compared to historical static and contextualized methods such as Word2vec, FastText, or ELMo. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and other domain like biomedical with PubMedBERT and BioBERT, and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open-source model for biomedical and clinical. We, so, we ask ourself question about what is the most appropriate data sources for a wide range of usage. And those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. After all, we ask ourself how much data do we need to train a specialized model on French data? Is it 4GB, 8GB, or more? To answer this question, we first train and compare four from scratch model. A first version of DrBERT with 7GB of NACHOS. A second version of 4GB of subset of NACHOS. A first version of ChuBERT, which is a clinical model, with 4GB of sentences taken from Clinical Notes. And a final version of ChuBERT with a mix of 4GB of subset of NACHOS and 4GB of clinical notes. In addition to this comparison, we introduce three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on 4GB of subset of NACHOS. Another also based on CamemBERT but trained this time on the 4GB of clinical notes. And finally, one based on an English biomedical model, PubMedBERT, and trained on 4GB subset of NACHOS. In total, we have seven models. To evaluate our seven models, we gather multiple public and private downstream tasks, such as name entity recognition, classification, part of speech tagging, and question answering. These models are compared to six baseline model, which are CamemBERT OSCAR 138GB, CamemBERT OSCAR 4GB, CamemBERT CCNet 4GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation of highlight that models perform best on the task with data of the same nature as those on which the model has been trained. However, we have we can obtain that data from we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translates into better performance. In overall, from scratch pre-training seem to obtain higher performance on most of the task. However, our experiment on continual pre-training using the weight and tokenizer of PubMedBERT trained on the 4GB subset of NACHOS showed comparable result to those from obtained with DrBERT 4GB from scratch, which is not the case for the model based on CamemBERT weights and tokenizer which suffer from stability issues. Finally, as a conclusion, our proposed system offer better performance on nine of the 11 downstream tasks and surpass globally the result of the generic model here CamemBERT. We also observing that specialized data is better. More specialized data is better, but it doesn't scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face and all the training script are on our GitHub repository. So, thank you for for this presentation and we are looking forward to exchange at the poster session in Toronto.</sample>
    <sample id="135">This video introduces "Don't Forget Your ABCs: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems," a new evaluation method called ABC-Eval (Annotating Behaviors in Chat) developed by the Emory NLP Lab and Amazon Alexa AI. The speakers, James and Sarah Finch, highlight the limitations of traditional human evaluation methods like comparative and Likert scale ratings for assessing dialogue quality comprehensively.

ABC-Eval addresses this by explicitly annotating specific behaviors in chat model responses, such as irrelevance, lack of empathy, or self-contradiction. These behaviors are categorized into dimensions like coherence, knowledge, consistency, and emotional understanding.

The researchers conducted experiments with four state-of-the-art chat models across 100 human-bot conversations. Their findings indicate that ABC-Eval's behavior labels are more reliable and predictive of overall conversation quality compared to existing methods, as demonstrated by higher inter-annotator agreement and greater explanatory power in linear regression analysis. The study reveals that even advanced bots still exhibit common sense violations, irrelevant information, and contradictions in a significant percentage of their responses, suggesting areas for future improvement in conversational AI.</sample>
    <sample id="136">The speaker, Jasivan Alex Sivakumar, presents his work on FERMAT, an alternative to accuracy for numerical reasoning. He explains that many downstream tasks, like fact-checking and text generation, require numerical reasoning for factual correctness. He highlights that current large language models perform poorly on these tasks, especially smaller, more accessible models.

Sivakumar introduces FERMAT as a flexible evaluation set for representing multi-views of arithmetic types. This includes assessing number understanding, mathematical operations, and training dependency. He demonstrates how FERMAT transforms existing math word problems by changing number representations (e.g., from "5" to "5.0") and varying their magnitude (integers, decimals) to test model robustness.

Initial zero-shot evaluation shows that most models perform poorly across all FERMAT aspects. However, fine-tuning the models with expert-generated templates, which replace numbers with placeholders and provide an expression (e.g., "num1 + num2"), significantly improves performance. The study also investigates training dependency, finding that even when models are trained on exact expressions, their accuracy remains below 50%, suggesting a lack of true memorization. Finally, a radar chart illustrates the impact of diverse training templates on model performance, showing that language and mathematical diversity are crucial for improvement.</sample>
    <sample id="137">The speaker introduces their work, "Tell2Design: A Dataset for Language-Guided Floor Plan Generation." They begin by discussing the limitations of text-conditional generative AI models, which are generally suitable for artwork rather than precise designs. The "Tell2Design" project aims to address this by enabling users to design floor plans using natural language instructions, bypassing the need for expert architects.

The task is defined as generating reasonable 2D floor plans from language instructions describing the intrinsic components, including semantics (room functionality), geometry (room shape and dimension), and topology (relationships between rooms). The dataset contains both human-annotated and artificially generated language instructions, with over 200 words per instance.

The main challenges are: strict constraints in design generation compared to artwork, fuzzy and entangled information in unstructured text, and noisy human instructions (ambiguous, incomplete, or misleading). The proposed approach uses a Transformer-based encoder-decoder large language model, T5, to cast floor plan generation as a sequence-to-sequence problem. Room bounding boxes are reconstructed into structured target sequences.

Experimental results show that "Tell2Design" outperforms baseline models, achieving high Micro IoU and Macro IoU scores. The model's ability to extract and follow detailed information from language instructions contributes to its superior performance. The study also highlights a language distribution gap between artificial and human instructions, but warming up with artificial data significantly improves performance on human instructions, suggesting mutual benefits.</sample>
    <sample id="138">The authors claim that the ability of NLU models to integrate and use knowledge from multiple sources (both pre-train time and inference-time) is an understudied area.</sample>
    <sample id="139">The names of the speakers are Ying and Zhiyang Xu.</sample>
    <sample id="140">Yes, Coscript underwent quality checks through human annotation of validation and test sets to find and revise incorrect samples.</sample>
    <sample id="141">Existing resources for context-dependent translation are limited in two main ways:
1. They often support only a small portion of words that depend on context, making corpus-level metrics unsuitable for comprehensive evaluation.
2. They typically support a limited number of discourse phenomena and languages, relying heavily on domain knowledge and human curation for their development.</sample>
    <sample id="143">The approach is compared to "wait-k", "LA", and "CAAT".</sample>
    <sample id="144">1. LIA, Avignon Université
2. LS2N, Nantes Université
3. Clinique des données, CHU de Nantes
4. Zenidoc</sample>
    <sample id="145">The speaker is named Jenny.</sample>
    <sample id="146">The speaker, Yicheng Zou from Fudan University, introduces a paper on understanding omission in dialogue summarization. He highlights that despite advancements in dialogue summarization, particularly with large-scale pre-trained language models, generated summaries often suffer from errors, including factual errors. Among these, omission is a major factor, leading to incomplete summaries where critical facts are lost.

The speaker explains that current models struggle with a serious and general omission problem in dialogue summarization, with about 70% of generated summaries exhibiting this issue. The omitted information is randomly distributed throughout the dialogues, regardless of length or domain. This indicates that identifying key information remains a challenge for current models.

To address this, a new task called "Omission Detection" is defined, focusing on identifying omitted utterances in candidate summaries compared to a gold reference. Since no relevant datasets exist for this task, the speaker introduces OLDS (Omission Labels for Dialogue Summarization), a new dataset constructed from five existing benchmarks, covering five domains. This dataset uses different abstractive models and decoding strategies to generate diverse candidate summaries, with omission labels produced through an automatic method and human assessment.

The speaker presents analysis showing that omission detection is a challenging task, with F1 scores around 50% across various baseline models and domains. However, integrating detected omissions into a post-editing refinement method significantly boosts summary performance, indicating that omission detection is a valuable task for improving dialogue summarization quality.</sample>
    <sample id="147">Three authors are involved in the paper: Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is publicly available on GitHub at the provided URL: https://github.com/ShuhengL/acl2023_conllpp.</sample>
    <sample id="150">The paper "MeetingQA: Extractive Question-Answering on Meeting Transcripts" addresses the lack of question-answering (QA) datasets for meeting transcripts. Currently, most research in this area focuses on summarization and action item extraction, overlooking the significant QA component in discussions where participants ask questions and receive detailed responses.

MeetingQA is an extractive QA dataset built from public meeting transcripts (around 100 hours of manually transcribed multi-party meetings). The questions in MeetingQA are extracted based on punctuation and length, and annotators label the answer sentences, achieving high inter-annotator agreement (Krippendorff's α = 0.73).

Key statistics reveal that 30% of questions are unanswerable, 40% have multi-span answers (non-consecutive sentences), and 48% have multi-speaker answers. A significant portion of questions (54.4%) are yes/no but still elicit detailed and opinion-seeking responses. Rhetorical questions are also present (20%), and 70% of multi-speaker answers include some form of disagreement.

Experimental results show a substantial gap between existing QA models and human performance: over 25 F1 points in the finetuned setting and about 50 F1 points in the zero-shot setting. Short-context models like RoBERTa perform slightly better than long-context models like Longformer, and multi-span models show comparable or slightly lower performance than single-span models. Silver data augmentation proves effective in improving zero-shot performance. Error analysis indicates that models struggle with rhetorical questions and identifying speaker contributions, especially in zero-shot scenarios. MeetingQA thus presents a challenging new benchmark for QA research.</sample>
    <sample id="152">Frederick Riemschneider introduces his team's work on exploring large language models for classical philology, focusing on Ancient Greek and Latin. He highlights the limitations of existing monolingual encoder-only BERT models, such as Latin BERT (2020) and Ancient Greek BERT (2021, 2022), which suffer from noisy pre-training data and lack comprehensive evaluation.

To address these shortcomings, they embarked on creating new language models, with four key goals: making existing models comparable, advancing the state of the art, exploring diverse model architectures, and introducing multilingual models. Their work includes pre-training two monolingual models for Ancient Greek: GreBERTa (RoBERTa-based encoder-only) and GreTA (T5-based encoder-decoder). Additionally, they developed multilingual equivalents, PhilBERTa and PhilTA, trained on Ancient Greek, Latin, and English data.

A significant contribution is the creation of a high-quality pre-training corpus for Ancient Greek, by leveraging previously unused resources and developing a novel method to extract clean Greek texts from the Internet Archive. The models were rigorously benchmarked on PoS tagging, dependency parsing, and lemmatization tasks, with their models outperforming the state-of-the-art for both Ancient Greek and Latin. They achieved impressive lemmatization performance gains of 5 percentage points for Ancient Greek. Semantic and world knowledge capabilities were also probed, revealing that their models significantly outperform previous models. However, there wasn't a significant difference between the performance of multilingual and monolingual models in these specific areas.</sample>
    <sample id="153">The speaker introduces a research project on resolving ambiguities in text-to-image generative models. These models often produce varied interpretations of ambiguous prompts, leading to generated images that don't always align with user intent. 

To address this, the researchers created a Text-to-image Ambiguity Benchmark (TAB) dataset, a modified version of the LAVA corpus, to categorize different types of ambiguities. They then proposed two frameworks for prompt disambiguation: QA-TIED and VS-TIED. QA-TIED uses in-context learning to generate clarifying questions for users, whose responses are then concatenated to the original prompt. VS-TIED, on the other hand, generates multiple possible visual setups, allowing users to choose the one that aligns with their intention, which is then used to disambiguate the prompt.

For evaluation, an automatic framework was developed where text-to-image models generate images from both original and disambiguated prompts. A Visual Question Answering (VQA) model assesses whether the generated images faithfully represent the user's intention. The findings indicate that there's disparity in resolving ambiguities across different types, but disambiguation generally has a positive effect on faithful generation. The automatic evaluation framework shows reasonable agreement with human evaluations, suggesting its reliability for assessing text-to-image models.</sample>
    <sample id="154">The paper is a joint work by Sara Papi from the University of Trento and Fondazione Bruno Kessler, and Matteo Negri and Marco Turchi.</sample>
    <sample id="155">The speaker's name is Javad Hosseini.</sample>
    <sample id="156">Hello everyone. My name is David Vilar, and I will be giving a short review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PaLM is a 540-billion-parameter large language model presented last year, in 2022. It's trained on a large collection of text comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of LLM prompting for machine translation. We evaluate the translation capability of such models, using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compare to state-of-the-art systems, so the best performing systems of the WMT evaluation. We use state-of-the-art neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the of LLMs for translation, as we can see in a simple experiment where we use one-shot prompting, and provided two different prompts for for a sentence. The majority of sentences, 516 out of 1000, the difference observed is of more than 1 BLEU point. And this can go in extreme cases up to 40 BLEU points. So, it's important to select a good prompting strategy. In our experiments, we settled for a five-shot prompting strategy, where we just mark each uh each sentence that we provide to the system with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences are marked with "German:" and the English translations with "English:". We saw that the actual form of the prompting doesn't have a big influence. In in the case of several-shot prompting, it's crucial for zero and one-shot prompting, but when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that example quality is more important than similarity to source sentence. So, it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations, or the dev data. The dev data is much more curated, and with higher quality than the training data that is more noisy, and the results show a better performance when using the the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But PaLM comes pretty close to a commercial system. Our in our case, we chose to evaluate with uh Google Translate. The insights that we gain from the human evaluation that we performed using the MQM framework, is that the fluency of PaLM is comparable to state of the of the art systems. But the main difference comes from the accuracy. So, in particular, the most common error are omission errors. So, it seems that PaLM chooses the to produce a better sounding translation, sometimes by dropping parts of the source sentence that are omitted in the in the translation. However, the style/awkward category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="157">The presentation introduces a new approach to dialogue summarization called Static-Dynamic Graph-based Dialogue Summarization (SDDS). Dialogue summarization aims to extract key information from a dialogue and present it concisely, which is a complex task due to the semi-structured and multi-participant nature of dialogues.

Existing methods primarily rely on precomputed static graph structures, which have two main drawbacks: their dependence on the accuracy of external linguistic tools can lead to error propagation, and their fixed nature prevents dynamic adaptation to the summarization task.

The SDDS model addresses these issues with four main components:
1.  **Utterance Encoder**: Converts dialogue utterances into vector representations.
2.  **Static Graph Construction**: Builds dependency-based dialogue structures using discourse parsing graphs, identifies semantic correlations through keywords co-occurrence, and captures speaker interaction frequencies with a speaker relation graph. The relative positions of utterances are also captured via an embedding matrix.
3.  **Static-Dynamic Graph Module**: Integrates multiple static graphs through a 1x1 convolutional layer and dynamically captures semantic relationships between utterances using a multi-head attention mechanism. This module then fuses the dynamic and static graph information into a unified graph representation.
4.  **Summary Generator**: Uses a pre-trained language model with a dual cross-attention mechanism, featuring a graph attention layer on top of the original self-attention layer, to incorporate the graph representation for summary generation.

The proposed SDDS model aims to improve dialogue summarization by combining fixed, linguistically-driven structural information with dynamic, context-aware semantic relationships, thereby reducing reliance on potentially unreliable external tools and enhancing adaptability. The data and code for this work are available on GitHub.</sample>
    <sample id="158">This presentation introduces "Dual Cache for Long Document Neural Coreference Resolution," a novel approach to addressing challenges in coreference resolution for extensive texts. Coreference resolution involves identifying and linking mentions within a text that refer to the same entity or concept. Conventional methods often suffer from quadratic complexity, particularly in long documents, due to the need to enumerate all possible mention pairs.

Cache-based models mitigate this by using a fixed-size cache, reducing complexity to a linear level. However, a single cache using an LRU (Least Recently Used) policy can lead to high cache miss ratios in long documents where topics switch frequently, scattering mentions of the same entity.

To overcome this, the "Dual Cache" model proposes two caches: an L-cache for local entities with an LRU policy and a G-cache for global entities with an LFU (Least Frequently Used) policy. When a new mention is encountered, the model first determines if it's a new entity or belongs to an existing one. If it qualifies, it's added to the G-cache, otherwise, to the L-cache. Each cache has an eviction policy to manage its capacity.

Experimental results on public benchmarks (LitBank, OntoNotes, WikiCoref) demonstrate that Dual Cache outperforms single-cache methods, even those with unbounded memory, especially when training data is available. For book-level documents, the performance gap between Dual Cache and baselines is even larger. Analysis shows that Dual Cache significantly reduces cache misses and offers the highest performance/cost ratio, making it a more efficient and effective solution for long document coreference resolution.</sample>
    <sample id="160">The first step of the method maps input tokens to **unordered multisets of tokens** that will appear in the output.</sample>
    <sample id="161">Coscript contains 55,000 scripts.</sample>
    <sample id="162">Hello everyone. I'm Akshatha, and today my co-author Martin and I are presenting our work, The KITMUS Test. Evaluating knowledge integration from multiple sources. National Language Understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired via pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-train time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, "John saw the newly elected president on TV". Pre-trained parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance specific entity John is, or who the new president is, because the president might have changed since pre-training. Therefore, successful models for knowledge intensive NLU tasks require the ability to integrate and use both pre-train time and inference time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and established coreference resolution models. Here is an example from our dataset. Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] The resolution of a given pronoun requires two types of information. First, entity-specific knowledge, such as Servin is a judge. And second, background knowledge, such as judges decide cases in law courts. Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information, such that it may either be found in a single source or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting, Background-Pretrain, where background knowledge is assumed to be available at pretrain time. Second, there's the Background-Both setting, where background knowledge is available both at pretrain time and inference time. Lastly, the Background-Inference setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates a case where the background knowledge necessary to solve the task is not part of the pretrain data of models, for example because new occupations have developed since the time of pretraining. Here's an example of how we control the availability of facts in the two sources. In the Background-Pretrain setting, we assume that the background knowledge "Politicians seek elected seats in government" is contained in the pretrain parameters. In the inference-time context, we provide the entity specific knowledge "Chichester is a politician". In the Background-Both setting, we additionally provide not only entity-specific, but also background knowledge about politicians in the inference-time context. In the Background-Inference setting, we provide the fictional occupation "mirutuer" instead of "politician", because "mirutuers" are unlikely to be contained in the pretrain parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KITMUS, where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper, many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). Task-specific training is necessary for knowledge integration. Models struggle to integrate inference-time background knowledge. If you're interested in more details, please see our paper and check out the dataset, generation, and evaluation code on GitHub. Thanks for listening.</sample>
    <sample id="163">The best alignment method for DEplain is MASAlign.</sample>
    <sample id="164">The benefit of weakly supervised learning (WSL) is that it alleviates the annotation bottleneck, as weak labels are much cheaper than human annotations.</sample>
    <sample id="165">The speaker begins by introducing the paper, "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." The speaker, Wenting Zhao, is a Ph.D. student at Cornell University.

Abductive reasoning starts with a context and ends with an outcome. Additionally, a set of possible explanations are given, and the goal is to identify a plausible explanation that can bridge the information gap between the context and the outcome. In the given example, "Emily was stuck in traffic" is the context, "Emily made it to her flight" is the outcome, and two explanations are "Her flight was delayed" and "Her flight left on time." The first explanation is plausible, as it bridges the gap by providing the necessary missing information.

The paper considers a closed-world setting for abductive reasoning, where a candidate set of explanations (Z) is given, and the goal is to identify a plausible subset of the explanations. Current approaches rely on supervised methods, but annotating plausible explanations can be noisy and subjective, with crowd workers disagreeing on over 60% of explanations. Thus, the question arises whether abductive reasoning can be learned without supervision. The answer is yes, through LiPoR (Likelihood learning with Posterior Regularization).

LiPoR treats explanations as a latent variable and maximizes the log-likelihood of the outcome, given the context, by marginalizing all possible explanations. To achieve this, an additional regularizer is needed. This regularizer leverages the mutual exclusivity of explanations, where one explanation being plausible automatically rules out other explanations. The LiPoR objective combines maximizing the likelihood of outcomes and preferring some explanations over others. When the entropy of p(z|x,y) is larger than ln(m), there are more than 'm' explanations receiving probability mass. In this case, LiPoR minimizes the entropy, thereby preferring a subset of explanations. LiPoR outperforms all zero-shot models, including a strong GPT-3 based baseline, by over 4 absolute points in accuracy on the αNLI dataset.</sample>
    <sample id="166">The speaker begins by introducing their new work, "Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text." They highlight the challenge of this task due to highly similar images and long descriptions, noting that typical visual language models (VLMs) struggle with complex texts. The speaker explains their inspiration from the divide-and-conquer strategy and dual-process theory, which describe how complex problems are broken down into smaller ones. 

The proposed method involves a "Proposition Generator" to decompose complex propositions into simple sentences. They introduce "System 1: Visual-Linguistic Interactor" for visual-proposition information interaction, based on OFA (A VLM capable of multimodal information interaction). The outputs of System 1 are matching scores of propositions-images and their reasoning states. "System 2: Neural-Symbolic Reasoner" integrates reasoning states and simple propositions to obtain the final solution for complex propositions on images. System 2 consists of a negation executor and a conjunction operation. Finally, they combine the inference results of System 1 and System 2 to gain a final solution, leveraging the advantages of analogical inferring (System 1) and logical reasoning (System 2). 

Experimental results show that their method, NDCR, outperforms other baselines. Case analysis demonstrates the method's interpretability by presenting inference results in the middle step. The speaker concludes by suggesting that neural-symbolic calculation could improve compositional reasoning and planning capacity of large language models. They also note that divide-and-conquer is similar to self-asking chain-of-thought, and dual-process theory could be integrated with divide-and-conquer.</sample>
    <sample id="167">756 documents from DEplain-web were aligned with both manual and automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">David Vilar Torres presents an overview of "Prompting PaLM for Translation: Assessing Strategies and Performance." PaLM, a large language model from 2022, has 540 billion parameters and was trained on 780 billion tokens. It achieved state-of-the-art results in many NLP tasks.

The research's contribution involves the first systematic study of LLM prompting for machine translation. It evaluates translation capabilities using best practices from the MT community, including the latest test sets, comparisons to state-of-the-art WMT submissions, advanced MT metrics, and expert-based human evaluation. The study also offers recommendations for prompt selection strategies.

A key finding is that prompts significantly impact translation quality. A simple experiment using one-shot prompting revealed that over half of the sentences (516 out of 1000) showed a BLEURT score difference of more than 1 point, with some differences reaching up to 40 BLEURT points. This highlights the importance of choosing an effective prompting strategy.

For their experiments, the researchers used a five-shot prompting strategy where source and target sentences were explicitly labeled with their respective languages. They found that the exact format of the prompt was less critical for several-shot prompting compared to zero- or one-shot prompting, suggesting that the quality of the examples provided is paramount.

Experimental results indicate that example quality is more crucial than similarity to the source sentence. While specialized state-of-the-art systems generally outperform PaLM, PaLM is remarkably close to commercial systems like Google Translate. Human evaluation insights reveal that PaLM's fluency is comparable to state-of-the-art models, but its accuracy scores are generally lower, predominantly due to "accuracy/omission" errors. Conversely, "style/awkwardness" scores are lower for PaLM, suggesting it produces more fluent, albeit sometimes less accurate, translations.</sample>
    <sample id="171">Existing work on this topic includes parameter-based watermarks, lexical watermarks, backdoor-based watermarks, and adversarial-based watermarks. However, these methods are either not applicable to embedding-as-a-service (EaaS) or lack transferability.</sample>
    <sample id="172">No, multilingual LLMs like Codex and BLOOM are currently inadequate for cross-lingual semantic parsing (CLSP) tasks.</sample>
    <sample id="173">Hello everyone. My name is Shuheng. Today, I'm going to present our paper: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" Let's get started. Our paper investigated the problem of generalization, using the named entity recognition task, or the NER task. We observe that models have been using CoNLL-2003 to develop NER for almost 20 years. And this naturally raises several problems. Firstly, can these models generalize to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we develop the CoNLL++ dataset. This is a dataset that we collected from Reuters news from 2020 and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test set and the CoNLL++ test set. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So, what is needed for a good generalization? Through our experiments, we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments, we found that the Transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually, larger models lead to better generalization. And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here, we also found that more fine-tuning examples actually also leads to better generalization. To our next question, what causes the performance drop of some models? We had two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. And this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For adaptive overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than 1. This means that every unit of improvement that we made on CoNLL-2003 translates to more than one unit of improvement on CoNLL++. Which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data. And we found that the performance degrades with larger temporal gap. And this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these go hand in hand. We can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift, and kind of surprisingly, it is not caused by adaptive overfitting, even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper, do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our dataset, and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="174">Priya introduces ArgAnalysis35K, a large-scale dataset for Argument Quality Analysis, highlighting its unique features compared to other datasets. Argument Quality Analysis involves judging the quality of an argument on a scale of 0-1, where a well-structured and coherent argument receives a higher score.

Current datasets for argument quality analysis often suffer from a lack of argument quality, diversity in motions, and depth in explaining nuances. They also tend to associate scores directly with motions, limiting their broader applicability.

ArgAnalysis35K addresses these issues by being the largest dataset in argument quality detection, with 35K argument-analysis pairs. A significant portion (85%) of these arguments are sourced from high-quality debate tournaments and expert debaters, ensuring higher quality compared to crowdsourced arguments. Instead of focusing on specific motions, the dataset categorizes arguments by 24 diverse themes (e.g., politics, environment, authoritarian regimes), allowing for a broader range of arguments.

A novel element introduced is "analysis," which provides logical links to explain why an argument is true, going beyond simple claims or premises. This analysis can be a combination of claims and premises, offering a more comprehensive explanation. The dataset also implements an instance-based annotator reliability model, which accounts for human biases by evaluating annotator reliability on a per-argument basis, rather than discarding all judgments from potentially biased annotators. Finally, ArgAnalysis35K includes a relevance model that assigns a score from 0-1 for each argument-analysis pair within each theme, capturing the relevance of an argument to a particular topic. These features collectively contribute to a more diverse, higher-quality, and reliable dataset for argument quality analysis.</sample>
    <sample id="175">The method approximates the NP-hard problem of finding the highest scoring permutation using a GPU-friendly continuous relaxation, which allows backpropagation through the solution and learning of linguistically more plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by its performance across different demographic and political leaning groups.</sample>
    <sample id="177">The speaker's name is Yanis Labrak.</sample>
    <sample id="178">The speaker's name is Koustuv Sinha.</sample>
    <sample id="179">The speaker, Melanie Sclar, introduces "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker." She defines Theory of Mind (ToM) as the ability to reason about others' mental states, typically measured using false-belief questions where reality doesn't match a character's belief.

She uses the classic Sally-Anne Test as an example, where Alice puts an apple in a basket, leaves, then Bob moves it to a box. Questions are classified as first-order (about a character's mental state) or second-order (about a character's estimation of another's mental state), and as true-belief or false-belief. Large Language Models (LLMs) often perform poorly on false-belief tasks, as shown with examples from ChatGPT and GPT3.

To address this, the team developed "SymbolicToM," an inference-time method using explicit graphical representations to improve ToM reasoning in LLMs. SymbolicToM creates belief graphs, like "B_Bob" (what Bob believes) and "B_Alice,Bob" (what Alice thinks Bob believes), for all character combinations up to a user-defined depth. These graphs are computed using an inference-time graph algorithm leveraging off-the-shelf NLI and OpenIE models.

To answer a question, SymbolicToM detects entities, retrieves the relevant belief graph, recursively asks a factual question about the graph, and then feeds the retrieved sentences and factual question to an LLM.

Experiments show SymbolicToM dramatically improves LLM performance in-domain, with significant accuracy boosts across models (e.g., +65 points for GPT3-Davinci). Out-of-domain, supervised models degrade, but SymbolicToM still shows gains, outperforming supervised approaches on story understanding and linguistic diversity datasets.

In conclusion, SymbolicToM is a plug-and-play inference-time method that uses explicit graphical symbolic representations for interpretable reasoning, dramatically improving LLM performance on ToM tasks and showing strong generalization capabilities.</sample>
    <sample id="180">The speaker's name is Myra.</sample>
    <sample id="181">The speaker introduces their work on "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." They highlight that while large language models (LLMs) can effectively decompose abstract goals into steps, previous work has not addressed planning for specific goals with multi-faceted constraints, such as making a strawberry or chocolate cake.

The paper defines the problem of constrained language planning and evaluates the ability of LLMs to generate scripts under these constraints. They found that current LLMs achieve unsatisfactory results on planning for specific goals, with the faithfulness to constraints being a significant issue. To address this, they propose a method that involves generating specific goals using InstructGPT, over-generating candidate scripts, and then filtering these scripts based on semantic similarity and keyword presence. This "over-generate-then-filter" approach significantly improves planning quality.

Furthermore, recognizing the cost of deploying LLMs, the speaker emphasizes the need for smaller, specialized models. They introduce "Coscript," a high-quality script dataset generated using their method (55,000 scripts with constraints) to enable this. Constraint analysis of Coscript reveals high heterogeneity and pluralism in the generated specific goals. Experiments show that smaller language models fine-tuned on Coscript can generate higher-quality scripts than most larger language models, indicating that specialized models can surpass LLMs when properly trained on suitable datasets. In summary, the work establishes the constrained language planning problem, evaluates LLM capabilities, develops a novel method for improvement, and creates the Coscript dataset as a valuable resource for future research in language planning with more complex and diverse goals and constraints.</sample>
    <sample id="182">In the context of this paper, tropicalism refers to a trope associated with Latina women, depicted through words like "vibrant" and "curvaceous" that appear in the generated personas.</sample>
    <sample id="183">The authors generated human-written portrayals of target groups by asking human subjects to respond to the same prompts used for the AI models, such as "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="184">The Conditional Cross-Mutual Information (CXMI) and Pointwise (P-)CXMI were used to measure context usage.</sample>
    <sample id="185">DrBERT is based on NACHOS, an open-source dataset of medical data, while ChuBERT is based on NBDW, a private dataset of anonymized medical records.</sample>
    <sample id="186">Hi, I'm Myra, and today I'll be talking about our paper Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models or LLMs. However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate, and they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general, broad associations like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions in prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual, using a prompt like, "Imagine you are an Asian woman. Describe yourself." And we can immediately see that this is very generalizable to any demographic, because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately, we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming. The Middle Eastern woman is referred to using words like exotic, um, and like referring to a mesmerizing region. And both of the woman of color personas make references to ancestry, while the White man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study, um, where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human-written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. Um, the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon. So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default, and any group that differs from that default, um, is linguistically marked. So for instance, the word man, or sorry, the word warrior is usually associated with men. Um, so when people are describing a warrior who is a woman, they'll usually actually specify women warrior and mark the term with woman. And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are. And then we compare the personas using the finding words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of Black woman, we would do finding words and compare the log-odds ratios against both White personas and man personas, because those are the two corresponding unmarked groups. Um, now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words in the lexicon, we find very different things. So while the generated personas have much higher rates of the lexicon words, um, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic. Um, so really just only the positive, or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides Wallat all. So instead, to do that, we'll turn to the results from our marked words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, for marked groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the White norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there is a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like vibrant and curvaceous, um, which connect to a trope of tropicalism. For Asian women, the words are things like petite, and delicate, and silky, which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so on. Um, and finally, for Black women, we see that some of the top words are things like strong and resilient. This connects to an archetype that people have called the strong Black woman archetype, and while it sounds like positive at first glance, um, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure, um, on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people among other harms. Um, more broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods because for instance, like these positive stereotypes, we don't know if it's because there's some sort of like weird overly excessive value alignment going on, or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns. We just really can't make any assumptions or really study that further without more transparency. Thank you so much for listening. Um, have a good time at ACL.</sample>
    <sample id="187">There are three authors involved in the paper.</sample>
    <sample id="188">Iterative transfer learning is a method of updating a model by training on the latest set of collected data.</sample>
    <sample id="189">The dataset's goal is to understand how users formulate indirect referring expressions when making a choice between alternative options, particularly in informal conversations. This research helps conversational systems and improves large language models' entity understanding.</sample>
    <sample id="190">The paper says that attackers can steal model parameters by learning from the embeddings provided by EaaS. However, it does not specify how this learning process extracts model parameters.</sample>
    <sample id="191">There are three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">The speaker, Yang Luo, introduces their work on CAME: Confidence-guided Adaptive Memory Efficient Optimization. The background of LLM training often relies on adaptive gradient-based optimization methods, with widely used optimizers like Adam and LAMB tripling memory for per-parameter gradients. Existing memory-efficient optimizers, like Adafactor, reduce memory but with a performance penalty. The challenge is to design an optimizer that simultaneously achieves fast convergence and low memory usage.

Preliminaries include Non-negative Matrix Factorization (NMF), which factorizes a matrix V into two matrices W and H, reducing memory requirements from O(mn) to O(m+n). Adafactor Optimizer, for NMF, presents an analytic solution for minimum I-divergence between V and WH, especially for rank-1 factors.

However, NMF operations in Adafactor can lead to erroneous updates, causing slow convergence compared to Adam and limiting memory-efficient optimizers. CAME addresses this by considering two scenarios for erroneous updates and introducing an efficient approach to decrease side effects from insecure updating. CAME calculates an instability matrix (Ut) and updates Rt and Ct similarly to Adafactor. It then applies the square root of the approximated instability matrix (St) as the denominator for Mt to adaptively update optimization steps.

Experiments were performed on BookCorpus, Wikipedia, GLUE Benchmark, SQuAD v1.1, and SQuAD v2.0, using BERT, GPT-2, and T5 models. BERT training showed CAME significantly improved validation accuracy over Adam and Adafactor, with a 3.4% increment compared to Adafactor for the same training steps. CAME performed even better with larger batch sizes (32k vs. 8k). In downstream tasks, CAME achieved comparable performance to the baseline while reducing memory cost. Memory cost comparison showed CAME with 7.07 GB memory usage, compared to Adam (8.24 GB), LAMB (8.23 GB), Adafactor (7.00 GB), and SM3 (7.44 GB).

In conclusion, CAME, inspired by erroneous updates in existing optimizers, supports adaptive confidence-based updating using the residual between predicted and generated updates. It shows outstanding performance on large language model training tasks and works well for large batch training, extending existing memory-efficient optimizers.</sample>
    <sample id="193">The speaker collected around 1000 examples of discourse unit pairs for the initial dataset, which were used to train an initial classifier. However, the exact number of annotators isn't provided.</sample>
    <sample id="194">The authors of the paper are affiliated with the University of Washington, Carnegie Mellon University, and the Allen Institute for AI.</sample>
    <sample id="195">This presentation introduces RoHT, a two-stage framework for explainable question answering (XQA) over Hierarchical Question Decomposition Trees (HQDT). XQA aims to answer a question and provide an explanation for the chosen answer. Existing XQA methods include neuro-symbolic and decompose-based methods, both with limitations. Neuro-symbolic methods are restricted to structured knowledge bases (KBs), while decompose-based methods rely on free-text corpora, making XQA difficult due to natural language diversity.

RoHT addresses these challenges by integrating knowledge from heterogeneous sources and leveraging question decomposition. The framework involves two main stages: understanding the complex question and probabilistic reasoning over HQDT. In the understanding stage, a complex question is decomposed into an HQDT, where the root is the original question, non-root nodes are sub-questions, and leaf nodes are atomic questions. Each node's generation uncertainty is quantified.

In the reasoning stage, RoHT recursively solves questions from the root to the leaves. For each node, a scheduler determines appropriate knowledge sources (KB, text, or child questions), an executor retrieves answers with probabilities from these sources, and an aggregator combines candidate answers, outputting the most probable ones. Experiments on KQA Pro and Musique datasets demonstrate RoHT's effectiveness in integrating knowledge from both KBs and text, outperforming existing methods and showcasing the benefits of its explicit decomposition and reasoning.</sample>
    <sample id="196">The example given for the governor being on the left is: "I saw Bart and Lisa."</sample>
    <sample id="197">The state-of-the-art models in dialogue systems evaluated in the study are BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="198">Large language models are being developed with longer and longer context windows. Therefore, it is crucial to evaluate the models' acceptability throughout the context window.</sample>
    <sample id="199">Yes, English performance dropped in 7 datasets when trained in a multilingual fashion, compared to monolingual English model.</sample>
    <sample id="200">No, the annotators do not know about the entity in advance. They are provided with a Google search link for each song (or background text for books and recipes) and asked to listen to/read about each entity to inform their descriptions.</sample>
    <sample id="201">The evaluation used state-of-the-art neural MT metrics.</sample>
    <sample id="202">Yes, the regress in generalization impacts specific NER types. More specifically, **LOC** and **MISC** type entities are more impacted by temporal drift than **ORG** and **PER** type entities.</sample>
    <sample id="203">Positionality in NLP matters because it can lead to systematic performance differences in technology between populations. This can result in datasets and models that are less sensitive to offensive terms in certain contexts or less aligned with the perspectives of non-binary people.</sample>
    <sample id="204">The video indicates that multilingual LLMs like BLOOM are inadequate for cross-lingual semantic parsing tasks, but it does not specify whether they were fine-tuned with adapters or full fine-tuning.</sample>
    <sample id="205">The speaker, Shangbin Feng, a PhD student at the University of Washington, presents his work on "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". He highlights that language models are trained on large-scale web-crawled data, including political news media, which creates a "mixed blessing". While offering diverse perspectives, these different political opinions are inherently socially biased and can lead to fairness issues in downstream tasks.

Feng outlines his research approach: evaluating the political leaning of language models (LMs) and investigating the role of pretraining data in these biases. He also examines how LMs with different political leanings perform on downstream tasks and whether this leads to fairness issues in NLP applications.

Preliminary results indicate that LMs do possess varying political leanings across the political compass. Further pretraining LMs on partisan corpora (news and social media, separated by left, center, and right leanings) shows corresponding shifts in their ideological coordinates. Specifically, left-leaning corpora induced liberal shifts, and the "pre-45th to post-45th" shift showed LMs moving further from the center after 2017, indicating they pick up societal polarization.

In per-category performance for hate speech and misinformation detection, left-leaning LMs were better at detecting hate speech against social minorities, while right-leaning LMs performed better against powerful groups like white men. Similarly, left-leaning LMs were better at detecting misinformation from opposite political leanings, and vice versa. This highlights a fairness issue, as deploying biased LMs could marginalize certain groups or allow unchecked hate speech. The speaker concludes by emphasizing the dilemma of sanitizing pretraining data, which could lead to censorship, versus allowing biases to propagate, resulting in unfairness.</sample>
    <sample id="206">They use the RoBERTa-base model with a classifier head for transfer learning.</sample>
    <sample id="207">The recent test sets used for evaluating PaLM capabilities are the latest available test sets, which help to avoid test/train overlap and overfitting on evaluation data.</sample>
    <sample id="208">The authors proposed three recommendations.</sample>
    <sample id="209">The proposed method shows a gain of over 25 percentage points in accuracy compared to the strongest baseline.</sample>
    <sample id="210">The speaker's name is Shuheng Liu.</sample>
  </task>
</testset>